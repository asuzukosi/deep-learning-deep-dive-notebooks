{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization is adjusted the learned parameters iteratively to improve performance of the model\n",
    "# Here will be getting a deeper understanding into how optimization works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First handle retreiving the data\n",
    "train = FashionMNIST(root=\"data\", train=True, transform=ToTensor(), download=True)\n",
    "test = FashionMNIST(root=\"data\", train=False, transform=ToTensor(), download=True)\n",
    "\n",
    "train_data = DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(dataset=test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "class FashionMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.sequential(x)\n",
    "        logits_prob = self.softmax(logits)\n",
    "        return logits_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTClassifier(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fmnist_model = FashionMNISTClassifier().to(\"mps\")\n",
    "print(fmnist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters are adjustable parameters that let you control the optimization process\n",
    "# ie, hyperparameter allow you adjust the learning environment\n",
    "\n",
    "# number of epochs\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimization loop\n",
    "# each iteration through the entire optimization loop is called an epoch\n",
    "\n",
    "# Each epoch consists of the training loop and the validation loop\n",
    "# The training loop iterates over the training dataset and tries to converge at the optimal parameters\n",
    "# The validation/test loop iterates over the test dataset to check if the model performance is improving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: loss function is the degree of dissimilirity between our\n",
    "# predicted output and the actual output, you can see this as the test case for the \n",
    "# code our model is trying to write. It is the loss function we are trying to \n",
    "# minimize during the optimization\n",
    "\n",
    "# The most common loss functions are MSE for regression, Negative Log likelihood (NLL) for classificaiton,\n",
    "# CrossEntropyLoss combines the LogSoftmax adn the NLLLoss (negative log likelihood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization is the process of adjusting the model parameters ot reduce the model error.\n",
    "# The optimization algorithm defines how this process is performed. Stochastic gradient descent is the most popular approach\n",
    "# Different optimization algorithms work better on differnt problems\n",
    "\n",
    "# we initialize the optimzer be passing it our model paramters\n",
    "optimizer = torch.optim.SGD(fmnist_model.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the training loop optimization happens in 2 steps\n",
    "\n",
    "# We call the zero_grad method on the optimizer to set the gradients of all \n",
    "# the tensors to be optimized, this is because backward propagation is culmunative, \n",
    "# and the gradients from the previous training step will be added to the gradients\n",
    "# of the current training step\n",
    "\n",
    "\n",
    "# Back propagate the predicted loss with loss.backward() and find the gradient of all the loss with respect to all the parameters\n",
    "\n",
    "# Once we have our gradient we can now perform optimizer.step() to apply the optimization technique to adjust the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the training process\n",
    "\n",
    "def train(dataloader:DataLoader, model:nn.Module, optimizer, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(\"mps\"), y.to(\"mps\")\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1)*len(X)\n",
    "            print(f\"The Loss is : {loss} => [{current}/{size}]\")\n",
    "\n",
    "\n",
    "def test(dataloader:DataLoader, model:nn.Module, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(\"mps\"), y.to(\"mps\")\n",
    "            pred =  model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss/=num_batches\n",
    "    correct /=size\n",
    "    correct *= 100\n",
    "    print(f\"Test Error | Accuracy : {correct} % Avg loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def iterate(epochs, loss_fn, optimizer, model, training_data, testing_data):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t} ...............................\")\n",
    "        train(training_data, model, optimizer, loss_fn)\n",
    "        test(testing_data, model, loss_fn)\n",
    "    print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ...............................\n",
      "The Loss is : 1.701212763786316 => [64/60000]\n",
      "The Loss is : 1.6316320896148682 => [6464/60000]\n",
      "The Loss is : 1.6615177392959595 => [12864/60000]\n",
      "The Loss is : 1.6755821704864502 => [19264/60000]\n",
      "The Loss is : 1.6724430322647095 => [25664/60000]\n",
      "The Loss is : 1.6242613792419434 => [32064/60000]\n",
      "The Loss is : 1.702429175376892 => [38464/60000]\n",
      "The Loss is : 1.6630449295043945 => [44864/60000]\n",
      "The Loss is : 1.6485706567764282 => [51264/60000]\n",
      "The Loss is : 1.6234943866729736 => [57664/60000]\n",
      "Test Error | Accuracy : 81.57 % Avg loss: 1.6490007130203732\n",
      "Epoch 1 ...............................\n",
      "The Loss is : 1.639133334159851 => [64/60000]\n",
      "The Loss is : 1.6495221853256226 => [6464/60000]\n",
      "The Loss is : 1.6556569337844849 => [12864/60000]\n",
      "The Loss is : 1.6231305599212646 => [19264/60000]\n",
      "The Loss is : 1.6643898487091064 => [25664/60000]\n",
      "The Loss is : 1.6299853324890137 => [32064/60000]\n",
      "The Loss is : 1.6796536445617676 => [38464/60000]\n",
      "The Loss is : 1.608451008796692 => [44864/60000]\n",
      "The Loss is : 1.6166523694992065 => [51264/60000]\n",
      "The Loss is : 1.6787128448486328 => [57664/60000]\n",
      "Test Error | Accuracy : 81.15 % Avg loss: 1.6521223388659727\n",
      "Epoch 2 ...............................\n",
      "The Loss is : 1.6609328985214233 => [64/60000]\n",
      "The Loss is : 1.6266849040985107 => [6464/60000]\n",
      "The Loss is : 1.740022897720337 => [12864/60000]\n",
      "The Loss is : 1.6912832260131836 => [19264/60000]\n",
      "The Loss is : 1.6303176879882812 => [25664/60000]\n",
      "The Loss is : 1.6376502513885498 => [32064/60000]\n",
      "The Loss is : 1.6704411506652832 => [38464/60000]\n",
      "The Loss is : 1.622338056564331 => [44864/60000]\n",
      "The Loss is : 1.5781936645507812 => [51264/60000]\n",
      "The Loss is : 1.6055225133895874 => [57664/60000]\n",
      "Test Error | Accuracy : 81.58 % Avg loss: 1.6489549556355567\n",
      "Epoch 3 ...............................\n",
      "The Loss is : 1.5689258575439453 => [64/60000]\n",
      "The Loss is : 1.697676658630371 => [6464/60000]\n",
      "The Loss is : 1.644659399986267 => [12864/60000]\n",
      "The Loss is : 1.6550052165985107 => [19264/60000]\n",
      "The Loss is : 1.639129638671875 => [25664/60000]\n",
      "The Loss is : 1.583879828453064 => [32064/60000]\n",
      "The Loss is : 1.6564290523529053 => [38464/60000]\n",
      "The Loss is : 1.6454615592956543 => [44864/60000]\n",
      "The Loss is : 1.6657036542892456 => [51264/60000]\n",
      "The Loss is : 1.6991140842437744 => [57664/60000]\n",
      "Test Error | Accuracy : 81.58999999999999 % Avg loss: 1.6480692852834227\n",
      "Epoch 4 ...............................\n",
      "The Loss is : 1.6276254653930664 => [64/60000]\n",
      "The Loss is : 1.64776611328125 => [6464/60000]\n",
      "The Loss is : 1.5762474536895752 => [12864/60000]\n",
      "The Loss is : 1.6458674669265747 => [19264/60000]\n",
      "The Loss is : 1.6612279415130615 => [25664/60000]\n",
      "The Loss is : 1.6645169258117676 => [32064/60000]\n",
      "The Loss is : 1.6705231666564941 => [38464/60000]\n",
      "The Loss is : 1.7392523288726807 => [44864/60000]\n",
      "The Loss is : 1.6689059734344482 => [51264/60000]\n",
      "The Loss is : 1.6200730800628662 => [57664/60000]\n",
      "Test Error | Accuracy : 81.58 % Avg loss: 1.64779592851165\n",
      "Epoch 5 ...............................\n",
      "The Loss is : 1.6236920356750488 => [64/60000]\n",
      "The Loss is : 1.6949372291564941 => [6464/60000]\n",
      "The Loss is : 1.6150847673416138 => [12864/60000]\n",
      "The Loss is : 1.6358520984649658 => [19264/60000]\n",
      "The Loss is : 1.5957281589508057 => [25664/60000]\n",
      "The Loss is : 1.6769921779632568 => [32064/60000]\n",
      "The Loss is : 1.5963603258132935 => [38464/60000]\n",
      "The Loss is : 1.6240664720535278 => [44864/60000]\n",
      "The Loss is : 1.6269127130508423 => [51264/60000]\n",
      "The Loss is : 1.6288385391235352 => [57664/60000]\n",
      "Test Error | Accuracy : 81.58999999999999 % Avg loss: 1.6475006433049584\n",
      "Epoch 6 ...............................\n",
      "The Loss is : 1.6170886754989624 => [64/60000]\n",
      "The Loss is : 1.5624747276306152 => [6464/60000]\n",
      "The Loss is : 1.6501688957214355 => [12864/60000]\n",
      "The Loss is : 1.6516183614730835 => [19264/60000]\n",
      "The Loss is : 1.621532917022705 => [25664/60000]\n",
      "The Loss is : 1.5995702743530273 => [32064/60000]\n",
      "The Loss is : 1.5872623920440674 => [38464/60000]\n",
      "The Loss is : 1.710694432258606 => [44864/60000]\n",
      "The Loss is : 1.6677227020263672 => [51264/60000]\n",
      "The Loss is : 1.6221567392349243 => [57664/60000]\n",
      "Test Error | Accuracy : 81.49 % Avg loss: 1.6485712019501217\n",
      "Epoch 7 ...............................\n",
      "The Loss is : 1.6713533401489258 => [64/60000]\n",
      "The Loss is : 1.5524189472198486 => [6464/60000]\n",
      "The Loss is : 1.7399351596832275 => [12864/60000]\n",
      "The Loss is : 1.6490744352340698 => [19264/60000]\n",
      "The Loss is : 1.6127681732177734 => [25664/60000]\n",
      "The Loss is : 1.639608383178711 => [32064/60000]\n",
      "The Loss is : 1.731378436088562 => [38464/60000]\n",
      "The Loss is : 1.6266320943832397 => [44864/60000]\n",
      "The Loss is : 1.6794161796569824 => [51264/60000]\n",
      "The Loss is : 1.6189944744110107 => [57664/60000]\n",
      "Test Error | Accuracy : 81.54 % Avg loss: 1.648373897667903\n",
      "Epoch 8 ...............................\n",
      "The Loss is : 1.575981855392456 => [64/60000]\n",
      "The Loss is : 1.5914628505706787 => [6464/60000]\n",
      "The Loss is : 1.6453168392181396 => [12864/60000]\n",
      "The Loss is : 1.5439971685409546 => [19264/60000]\n",
      "The Loss is : 1.7101750373840332 => [25664/60000]\n",
      "The Loss is : 1.6701760292053223 => [32064/60000]\n",
      "The Loss is : 1.642484426498413 => [38464/60000]\n",
      "The Loss is : 1.6197497844696045 => [44864/60000]\n",
      "The Loss is : 1.558847188949585 => [51264/60000]\n",
      "The Loss is : 1.5968594551086426 => [57664/60000]\n",
      "Test Error | Accuracy : 81.6 % Avg loss: 1.6467024125870626\n",
      "Epoch 9 ...............................\n",
      "The Loss is : 1.6282113790512085 => [64/60000]\n",
      "The Loss is : 1.6695512533187866 => [6464/60000]\n",
      "The Loss is : 1.594512701034546 => [12864/60000]\n",
      "The Loss is : 1.5991506576538086 => [19264/60000]\n",
      "The Loss is : 1.5487134456634521 => [25664/60000]\n",
      "The Loss is : 1.610602855682373 => [32064/60000]\n",
      "The Loss is : 1.5829367637634277 => [38464/60000]\n",
      "The Loss is : 1.6979694366455078 => [44864/60000]\n",
      "The Loss is : 1.6585084199905396 => [51264/60000]\n",
      "The Loss is : 1.6715755462646484 => [57664/60000]\n",
      "Test Error | Accuracy : 81.6 % Avg loss: 1.6493216274650233\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "iterate(10, loss_fn, optimizer, fmnist_model, train_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
