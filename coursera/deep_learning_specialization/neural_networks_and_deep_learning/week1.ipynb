{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning applied to supervised learning\n",
    "# Taking input X and mapping to output Y\n",
    "\n",
    "# Supervised learning with neural networks\n",
    "# Neural networks work for both structured and structured data\n",
    "# Neural networks can also work in both supervised and unsupervised learning\n",
    "# In summary, Neural networks are universal approximators and can be used to solve any given problem, provided with the right features and the right architecture. \n",
    "\n",
    "\n",
    "\n",
    "# Scale drives deep learning: more data, larger networks and more computing.\n",
    "# Hand engineenering features would produce better performance for smaller amounts of data\n",
    "\n",
    "# We have now learnt how to build more sophisticated algorithms that can improve the performance of neural networks\n",
    "# e.g. switching from a sigmoid function to a ReLU function, due to the vanishing gradient problem and learning is very slow.\n",
    "# Gradient descent is an optimization algorithm\n",
    "\n",
    "# Speed of iteration can help improve the experimentation of various architectures in neural networks.\n",
    "\n",
    "# What you'll learn in the first few weeks\n",
    "# Building and getting to work with neural networks.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
