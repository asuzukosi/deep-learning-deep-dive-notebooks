{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nueral networks from scratch book.\n",
    "# Introduction to neural networks\n",
    "\n",
    "# What are neural networks?\n",
    "# Neural netwworks are the foundational component of building deep learning algorithms. \n",
    "# Neural networks are fundamental building block units for neural networks.\n",
    "# Artificial neurons are used to construct layers (Which can be represented as matrices)\n",
    "# Deep learning is the concept of having several hidden layers. The hidden layers are the ones\n",
    "# that the neural network controls. Most neural networks in use are a form of deep learning\n",
    "\n",
    "# Artificial Inelligence -> Machine learning -> Neural networks -> Deep learning\n",
    "\n",
    "# Supervised learning is learning with pre-established labels. Unsupervised learning is learning \n",
    "# from data without explicit labels. \n",
    "\n",
    "# A measure about the information of we are trying to predict is called the feature. A group of features\n",
    "# is called the feature set (which is represented as a vector). The values of a feature set can be regarded\n",
    "# as a sample. Samples are fed into neural network models to train them to fit desired output\n",
    "# from these inputs and predict based on them at inference time. \n",
    "\n",
    "# Labels are teh outputs that occured during each sample. so each sample is labeled with the actual outcome of the\n",
    "# sample based on the feature set. Labels can also be referred to as targets or ground truths.\n",
    "\n",
    "# Reinforcment learning is a form of unsupervised learning\n",
    "# Generative learning is also a form of unsupervised learning\n",
    "# So essentially, I'm specializing in forms of unsupervised learning. \n",
    "# Neural networks were conceived in the 1940s but figuring out how to train them\n",
    "# was a big issue till the invention of backpropagation in the 1960s by Geoff Hinton et al.\n",
    "# Neural networks became an area of attention when they started winning competitions in 2010\n",
    "\n",
    "\n",
    "# What is a neural network?\n",
    "# Artificial neural networks are inspired by the organic brain which has its own\n",
    "# form of neural networks. It is not a perfect comparison, but in the brain there are \n",
    "# neurons, activations adn a lot of interconnectivity (with layers?)\n",
    "\n",
    "# Artificial neural networks are syntactically similar to biological neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInput layer 10\\nH1 = 16\\nH2 = 16\\nH3 = 16\\nO = 2\\n\\n    M       V \\nL1(10*16) + 16 = 160 + 16\\nL2(16*16) + 16 = 256 + 16\\nL3(16*16) + 16 = 256 + 16\\nL4(16*2) + 2 = 32 + 2\\n\\t\\t\\n\\t\\t704 + 50 = 754 total params\\n\\n\\nIn the neural network above we have 4 affine matrices and 4 vectors\\nThe vector transformations can be seen as mini-functions that are combined\\nTo build the complex function. The output vectors of each of the transformation can \\nBe seen as some sort of intermediary vector representation of the data.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A single neuron itself is useless(can be used in a linear regression problem where the data can be fit into a straingt line)\n",
    "# But connecting multiple neurons together can build powerful capabilities. \n",
    "\n",
    "\"\"\"\n",
    "Input layer 10\n",
    "H1 = 16\n",
    "H2 = 16\n",
    "H3 = 16\n",
    "O = 2\n",
    "\n",
    "    M       V \n",
    "L1(10*16) + 16 = 160 + 16\n",
    "L2(16*16) + 16 = 256 + 16\n",
    "L3(16*16) + 16 = 256 + 16\n",
    "L4(16*2) + 2 = 32 + 2\n",
    "\t\t\n",
    "\t\t704 + 50 = 754 total params\n",
    "\n",
    "\n",
    "In the neural network above we have 4 affine matrices and 4 vectors\n",
    "The vector transformations can be seen as mini-functions that are combined\n",
    "To build the complex function. The output vectors of each of the transformation can \n",
    "Be seen as some sort of intermediary vector representation of the data.\n",
    "\"\"\"\n",
    "\n",
    "# We understand 'how' neural networks work but we dont undertand 'why' they work. I feel Its one of thos big mathematical mysteries.\n",
    "# Dense layers/ Linear layere the most common type of neural network layer, where every neuron in a layer is fully connected to all the\n",
    "# neurons of the previous layer and the next layer. This means that the output of a neuron in a particular layer becomes the input to a neural \n",
    "# network in the next layer. Each neuron has a weight connected to it, which are the trainable parameters. \n",
    "\n",
    "# Once al input flow through the connected weights, they are summed up and added to the bias term, then an activation function is applied on the result\n",
    "# to apply non linearity to the output of the neuron. \n",
    "\n",
    "# The purpose of the bias is to offset the input positively or negatively\n",
    "# The concept of weights and biases can be thought of as knobs that we can tune to fit our model to the data.\n",
    "# In mordern neural networks, we can have thousands, millions or even billions of parameters. \n",
    "\n",
    "# The parameters are tuned by the optimizer durin training. \n",
    "\n",
    "# Weights change the magnitude and direction of the neuron while biases shift theh value eiter up or down along the output plane. \n",
    "# Weigtts and bias help affect the output neuron but they do so in slightly different ways.\n",
    "\n",
    "# Scale and normalize your data (preprocessing before passing it into the network)\n",
    "\n",
    "# For classification, you can use one nueron to represent each output class. For binary classification,\n",
    "# you could implement it in a way where there is only a single output nueron, but you can use some heuristics to \n",
    "# determine the output. \n",
    "\n",
    "# What makes nueral networks look challenging is the math involved adn how scary it can sometimes look.\n",
    "\n",
    "# A typical neural network has thousands of 'adjustable' parameters (weights and biases).\n",
    "# In this wasy neural networks work as vast functions with an enormous amount of parameters. \n",
    "# Finding the right comdination of inputs and outputs to solve a specific problem is the why we train our model with optimizers\n",
    "# One major issue in supervised machine learning is the problem of overfitting. When the model simply trains to fit in the training\n",
    "# data then the model does not really 'understand' anything, it is just trying to cram/memorize the results for a given input. \n",
    "\n",
    "# Thus we use in-sample data to train our model, and outsample data to test it. \n",
    "# The idea of our model trying to learn underlying distributions about our data is known as generalization. \n",
    "\n",
    "# We calculate how wrong our neural networks are through an algorithm called the loss function. and we slowly \n",
    "# adjust our parameters based on the derivative of the loss function over many iterations till the loss function is minimized. \n",
    "# This can be rephrased as an optimization problem, this is why we use optimization alogrithms to train our neural network\n",
    "# because we are trying to optimize our parameters to have the lowest possible loss function. \n",
    "# But the goal is to enable the network generalize on previously unseen data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# Coding a simple neuron\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# A single neuron\n",
    "# This hypothetical neuronn has 3 inputs\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "# Our network will have weights initialized randomly (should they really be random)\n",
    "# and it should have a bias starting from 0. \n",
    "\n",
    "# Each input needs a weight associated with it. \n",
    "# Inputs are the data we pass into the network to get the desired output. \n",
    "# The values for the weights and biases are what get trained by the neural network.\n",
    "# And these are the values that make the model actually work (or not work).\n",
    "\n",
    "# We will simulate weights for now assigining a single weight to each input\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "# For a single neuron only 1 bias value is needed. ie for every set of inputs and weights, there is a single bias value for their dot product.\n",
    "# For every set of neuron outputs there is an equivalent set of bias values.\n",
    "bias = 2\n",
    "\n",
    "\n",
    "# The product of each input and its equivalent weight and the result of that operation added to the bias gives you the output of that \n",
    "# neuron. \n",
    "\n",
    "output  = (inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3]) + bias\n",
    "\n",
    "# The output of this simulated neuron is :\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A layer of Neurons\n",
    "# Neural networks usually have layers that consist of more than one neuron output. \n",
    "# Layers are simply a group of neurons stacked on top of each other. \n",
    "# Each layer takes a single set of inputs for all the neurons in that layer.\n",
    "# The layers output is equivalent to the number of neurons in that layer. \n",
    "\n",
    "# Each neuron in a layer has its own bias term\n",
    "# Lets experiment with a neural network with 4 inputs and 3 neurons (i.e. 3 outputs for that layer)\n",
    "\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output1  = (inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3]) + bias\n",
    "output2  = (inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3]) + bias2\n",
    "output3  = (inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3]) + bias3\n",
    "\n",
    "\n",
    "outputs = [\n",
    "    output, \n",
    "    output2,\n",
    "    output3,\n",
    "]\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# In the example above, we have 3 sets of weights adn 3 biases which match to the number \n",
    "# of output neurons in our layer. Each neuron is connected to the same set of inputs. \n",
    "# The difference is in the seperate weights and biases. \n",
    "\n",
    "# This is called a fully connected neural network. where every neuron in a current layer\n",
    "# is connected to every output of the previous layer.\n",
    "\n",
    "# Fully connected neural networks are popular but they are not a requiremetn\n",
    "all_weights = [weights, weights2, weights3]\n",
    "biases = [bias, bias2, bias3]\n",
    "\n",
    "layer_outputs = []\n",
    "# This will loop to the number of outputs in the layer\n",
    "for neuron_weights, bias in zip(all_weights, biases):\n",
    "    output = 0\n",
    "    # This loops to the number of inputs in the layer\n",
    "    for input, weight in zip(inputs, neuron_weights):\n",
    "        output += input * weight\n",
    "    output += bias\n",
    "    layer_outputs.append(output)\n",
    "    \n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip() allows us to loop thruogh two iterables of the same size simultaneously.\n",
    "# We can tell we have 3 neurons because we have 3 different sets of weights and biases. \n",
    "\n",
    "# Now lets start thinking of these objects in a more efficient way: Tenors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors, Arrays and Vectors\n",
    "# Tensors are closely-related to arrays. \n",
    "# A tenor is an object similar to an array, or matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single list {1 sample}\n",
    "l = [1, 5, 6, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of list, can be seen as a table of data, or a matrix\n",
    "lol = [[1, 5, 6, 2], \n",
    "       [3, 2, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list of lists of lists, it can also be called a tensor.\n",
    "# This can also be viewed as a representation of an image of size (3, 2, 4).\n",
    "# 3 Channels for RGB, and 2 rows by 4 columns.\n",
    "\n",
    "lolol = [[[1, 5, 6, 2], \n",
    "       [3, 2, 1, 3]],\n",
    "        \n",
    "       [[5, 2, 1, 2], \n",
    "       [6, 4, 8, 4]],\n",
    "       \n",
    "       [[2, 8, 5, 3], \n",
    "       [1, 1, 9, 4]],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_imbalanced_list_of_lists = [[4, 2, 3],\n",
    "                               [5, 1]]\n",
    "# This is not a 'homogeneous' list and can not be used in machine learning.\n",
    "# A list of lists is homogenous if all the dimensions of the lists within it are the same.\n",
    "# The first dimension is the number of sublists the second dimension is the number of items within each sublist. \n",
    "# (4,3) = (rows, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix, a martix is a rectagular array, it has 2 dimensions, rows and colums.\n",
    "# It can also be referred to as a 2d array. \n",
    "# We can have infinitely dimensional matrices\n",
    "\n",
    "list_matrix_arra = [[4, 2],  # this can be viewed as a list of items in a table, with two columns per item\n",
    "                    [5, 1], \n",
    "                    [8, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D matrices, commonly used to represent images\n",
    "\n",
    "lolol = [[[1, 5, 6, 2], \n",
    "       [3, 2, 1, 3]],\n",
    "        \n",
    "       [[5, 2, 1, 2], \n",
    "       [6, 4, 8, 4]],\n",
    "       \n",
    "       [[2, 8, 5, 3], \n",
    "       [1, 1, 9, 4]],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 5, 6, 2], [3, 2, 1, 3]],\n",
       " [[5, 2, 1, 2], [6, 4, 8, 4]],\n",
       " [[2, 8, 5, 3], [1, 1, 9, 4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets analyze the 3d matrix\n",
    "lolol[0], lolol[1], lolol[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the lolol matrix is (3, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that brings us back to the question, what is a tensor. \n",
    "# Pages of computer science have been written on the debate of tensors vs arrays.\n",
    "\n",
    "# A tensor object is an object that can be represented by an array.\n",
    "# We will tenssors as arrays in the context of deep leanring\n",
    "# Tensors are the foundational representations we use to build concepts in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors are not just arrays, but they are represented as just arrays in code\n",
    "# An array is an ordered homogenous container for working with numbers.\n",
    "# Arrays are the main datastructure of the numpy package. \n",
    "# A linear array is also called a 1-dimensional array which can also be refered to as a vector.\n",
    "# Arrays can be used to build representations of matrices in code. by simply\n",
    "# creating a 2-dimensional array. \n",
    "# The items in the matrix can be accessed by using a tuple of indices as the key\n",
    "\n",
    "# A vector in mathematics is what we call a list in python, or a 1-dimensional array in numpy\n",
    "# We can then build matrices as an array of arrays.\n",
    "# We also look at the vectors algebraically as a set of numbers within a bracket. \n",
    "\n",
    "# For the context of deep learnign we do not look at vectors from the perspective of \n",
    "# physicists who look at vectors as an array in space characterized by magnitude and direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4],\n",
       "       [7, 7, 7]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product and vector additions\n",
    "sample = np.array([[1, 2, 3], # These can be viewed as the neuron outputs for each input\n",
    "          [4, 5, 6]])\n",
    "\n",
    "adding = np.array([3, 2, 1]) # This can be seen as the bias\n",
    "\n",
    "sample + adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do vectors multiply?\n",
    "# Multiplication one of the most important operations we will perform on a vector. \n",
    "# We can achieve the same implementation in python by multiplying each index against the other and summing \n",
    "# the results in two equally sized lists, this is known as the dot product. \n",
    "# Because of the number of variable interconnections made we can model complex non linear relationships\n",
    "# with activation functions, which allow us to compound the results of a set of operations above each other \n",
    "# and achieve non linearity in or function. \n",
    "\n",
    "# When multiplying vectors you can either do a dot product or a cross product.\n",
    "# A cross product will produce a vector, while a dot product will produce a scalar value (single value/number)\n",
    "\n",
    "# vec_a . vec_b = sum[i=0, n](vec_a[i]*vec_b[i])\n",
    "\n",
    "# where n is the number of items in the vector, for dot product to work, both vectors have to be of the same size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]\n",
    "\n",
    "dot_product = a[0]*b[0] + a[1]*b[1] + a[2]*b[2]\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context of deep learning, we can call a the input and b the weights\n",
    "# suddenly it becomes apparent the the neural network output operation was simply \n",
    "# a dot product operation. \n",
    "\n",
    "# plain python does not allow us to perform dot products natively,\n",
    "# So we would be using the numpy package. \n",
    "# Numpy also allows us perform other operations that are very helpful to \n",
    "# deep learinng such as vector addition.\n",
    "# This the element-wise addition of vectors, although the two vectors must be\n",
    "# of the same size. \n",
    "# vec_a + vec_b  = [vec_a[0] + vec_b[1], vec_a[2] + vec_b[2], ...., vec_a[n]+vec_b[n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a single neuron with numpy\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = np.dot(inputs, weights) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builing a Layer of neurons with numpy\n",
    "# We would be calculating the output of a layer with 3 neurons.\n",
    "# This means that the weights should be a matrix. This is becuase as we can see \n",
    "# in the cell above, a single neuron is an array/vector. So given our knowledge of \n",
    "# linear algebra, we can say that a set of vectos sis a matrix, therefore a set of\n",
    "# neurons (which are vectors) is a matrix. To sum it up a layer is a matrix, keep that\n",
    "# mental image in mind for future reference. \n",
    "\n",
    "\n",
    "# Numpy treats matrices as a list of vectors. It perform the dot product of \n",
    "# each vector individually one after the other. \n",
    "# returning a list of the result of the dot product for each vector. \n",
    "\n",
    "# We would simply add the biases directly as they are matrices of the same shape with the output of the dot product \n",
    "# of the weight matrix and the input. So we would simply add the correspoding biases to each of them.\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weight_matrix = [[0.2, 0.8, -0.5, 1],\n",
    "                 [0.5, -0.91, 0.26, -0.5],\n",
    "                 [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(weight_matrix, inputs) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proces of matrix vector multiplication and then addition with another vector\n",
    "# is the simplest and common way of represiting input.weights + bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with a batch of data.\n",
    "# So far we have only been working on one sample observation.\n",
    "# But in deep learning we usually deal with large sets of samples (datasets).\n",
    "# It would be more efficient if we could perform these matrix operations simultaneously\n",
    "# on a whole set of data in one pass. \n",
    "\n",
    "\n",
    "# The challenges in this occur when we are tryrin to perform a matrix operation on the data\n",
    "# Becuase of the way matrix is formatted, which is (row, col) We would not be able to \n",
    "# perform the matrix operations directly as we have previously done. This is becauese the \n",
    "# input shape of the first matrixe in the matrix multiplication must be the same with the output of \n",
    "# second matrix in the matrix multiplication. Now because our shape has changed to have (num_instances, num_featurs)\n",
    "# from (num_featurs, ) We would not be able to perform matrix multiplication directly the way we did previously.\n",
    "# because the shapes simply don't match anymore. \n",
    "# But given the benefits of working with batches of data, such as efficiency and parallilization. \n",
    "# Also, trianing with batches help in achieving generalization faster and better. Since you would not be \n",
    "# making adjustments based on a single input, but over ta batch of inputs.\n",
    "\n",
    "\n",
    "# At this point, we are conceptually thinking of a matrix of inputs and a matrix of weights.\n",
    "# In this case, we would need to think of something a bit more complex than a dot product, we would have to think of matrix products,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix product\n",
    "# A matrix product is an operation of two matrices. \n",
    "\n",
    "# Matrix product requires one set of vectors to be arranged horizontally and the other vertically\n",
    "# But in our case both inputs and weights are aligned horizontally. \n",
    "\n",
    "# This size of the second dimension of the first matrix must align with the size of the dimension of the first dimension on the \n",
    "# second matrix. \n",
    "\n",
    "# Column vectors are vectors of a column of a matrix\n",
    "# Row vectors are vectors of a row of a matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3]]), (1, 3))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposition of the matrix product. \n",
    "# a.bT <- matrix product of a and b transpose\n",
    "\n",
    "\n",
    "# Transposition is the rotation of a matrix along the diagonal axis. \n",
    "# This happens in such a way that the rows become columns and the columns become rows.\n",
    "\n",
    "# A row vector is a vector where its first dimension equals 1 i.e (1, 8)\n",
    "# this represents one row and 8 columns. \n",
    "\n",
    "#  A column vector is a matrix with the second dimension equal to 1 i.e (8, 1)\n",
    "row_vector = np.array([[1, 2, 3]])\n",
    "row_vector, row_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_vector.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [2],\n",
       "        [3]]),\n",
       " (3, 1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_vector = np.array([[1], [2], [3]])\n",
    "column_vector, column_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_vector.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(row_vector, column_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(column_vector, row_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 8.9  , -1.81 ,  0.2  ],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A layer fo Neurons with a batch of data.\n",
    "# Looking back at the inputs and weights, we need to perform transposition on the second argument in the matrix multiplication.\n",
    "# The second arguement in our case will be the weight matrix. It will turn the row vector of \n",
    "# neurons into column vectors. \n",
    "\n",
    "# We were able to perform this operation without any issues before because our weights were in a matrix\n",
    "# while our input was in a vector (a single sample).\n",
    "\n",
    "\n",
    "# We plan to represent the samples as a set of row vectors and then the weights as a set of column vectors. \n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason we transpose the weights rather than the inputs is because we \n",
    "# want our outputs to be sample related and no neuron related as we pass it further through the \n",
    "# network, as the next layer will be expecting a batch of inputs which would be expected to be a stack of \n",
    "# row vectors. \n",
    "\n",
    "\n",
    "# This is why we pass in a list of samples even if there is only one sample, and it returns a list of predictions\n",
    "# even though there is only a single prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
