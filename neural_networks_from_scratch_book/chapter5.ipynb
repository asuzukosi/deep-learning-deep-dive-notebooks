{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "# Now we know how to perform a forward pass, how do we determine how wrong our network is\n",
    "\n",
    "# Calculating network error with loss function.\n",
    "# Our goal is to train/teach/optimize our model over time. \n",
    "\n",
    "# In order to do this, we need to tweak the parameters. But how do we decide what \n",
    "# parameters to tweak and how much to modify them?\n",
    "\n",
    "# In order to do this, we need to calculate how much error a model has, \n",
    "# the function for performing this operation is called a loss function also referred to as a cost function\n",
    "\n",
    "# We ideally want the Loss(the result of the loss function) to be 0.\n",
    "\n",
    "# The reason we don't use the bare argmax of the prediction is because what we are\n",
    "# actually trying to find is the confidence of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    L = - sum[j=1, j=num_classes](y*log(y_pred))\\n    # since the y for all other classes is zero\\n    then L = -y*log(y_pred)\\n    # but what then happens to class zero?\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical cross-entropy loss\n",
    "\n",
    "# In linear regression, we use mean squared error loss. \n",
    "# Since we are performing classification in this problem, we would like to use a\n",
    "# different meaure for calculating the accuracy of our network using a probability distribution.\n",
    "\n",
    "# For this we would be using the cross-entropy loss function. \n",
    "# Cross-entropy loss is used to compare ground truth probability distributions to some predicted probabilities distribtution\n",
    "\n",
    "# Cross-entropy loss funciton is the most common loss function used with the softmax activation function.\n",
    "\n",
    "\"\"\"\n",
    "    L = - sum[j=1, j=num_classes](y*log(y_pred))\n",
    "    # since the y for all other classes is zero\n",
    "    then L = -y*log(y_pred)\n",
    "    # but what then happens to class zero?\n",
    "\"\"\"\n",
    "\n",
    "# L[i] denotest the loss at a single instance. \n",
    "# Why is it called cross entroyp and not log loss, which is also another type of loss function\n",
    "# Log loss is what is applied to the output of the binary classifier in a logistic regression model.\n",
    "# In our case, we would be dealing with multiple classes and a probability distribution across those \n",
    "# classes for each sample\n",
    "\n",
    "# The target probabilities are one hot encoded so that they would produce a vector\n",
    "# The reason it is called one hot encoded is because only one value is hot (on) and the \n",
    "# rest are cold \"off\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying it out in code\n",
    "import numpy as np\n",
    "\n",
    "# predictions\n",
    "softmax_output = np.array([0.7, 0.1, 0.2])\n",
    "# ground truth\n",
    "target_output = np.array([1, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- np.sum(np.log(softmax_output) * target_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-1.6094379124341003\n",
      "-2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.log(1))\n",
    "print(math.log(0.95))\n",
    "\n",
    "print(math.log(0.2))\n",
    "print(math.log(0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be observered that he log of the values are negative except for the log of 1\n",
    "# That is why we add a negative sign infront of the cross entroy loss function in order to flip the negation sign\n",
    "# So that the value goes higher the worse our prediction confidence is \n",
    "# and once our prediction confidence for the target class is 1, which means that our model is very confident\n",
    "# It will then set our loss value to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The defualt log that is used without any underscore signifies the eulers loss function.\n",
    "# It is also regarded as the natural log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cross entropy loss function on a batch of data\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we would like to average the loss of the batch by fining the average\n",
    "# We will use the numpy mean mehtod to do this\n",
    "np.mean(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows that we can make this work for both one hot encoded values are bare values\n",
    "# To avoid having a zero value in our exponent, we will clip or results and give it a very small value as the defualt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.array([[1], [2], [3]])\n",
    "np.mean(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, predictions, targets):\n",
    "        losses = self.forward(predictions, targets)\n",
    "        total_loss = np.mean(losses)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        predictions = np.clip(predictions, 1e-7, 1 - 1e-7)\n",
    "        predictions = -np.log(predictions)\n",
    "        if len(targets.shape) == 1:\n",
    "            self.output = predictions[range(len(predictions)), targets]\n",
    "        \n",
    "        else:\n",
    "            predictions = predictions * targets\n",
    "            self.output = np.sum(predictions, axis=1, keepdims=True)\n",
    "            \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2, 3], [4, 5, 6]]\n",
    "np.max(data, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss_function.calculate(softmax_outputs, np.array(class_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplementing everying so far\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # use 0.01 to scale down the values\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_outputs)\n",
    "        self.biases = np.zeros((1, n_outputs))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.output = np.dot(X, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        # Returns zero if its less than zero\n",
    "        self.output =  np.maximum(0, X)\n",
    "        self.output\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # we need to scale down our exponent values because of exploding exponents\n",
    "        # we use exponents to remove negative values and convert our values into a probability distribution\n",
    "        X = X - np.max(X, axis=1, keepdims=True)\n",
    "        X = np.exp(X)\n",
    "        self.output = X / np.sum(X, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "        \n",
    "        \n",
    "class Loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        pass\n",
    "    \n",
    "    def calculate(self, predictions, targets):\n",
    "        losses = self.forward(predictions, targets)\n",
    "        loss = np.mean(losses)\n",
    "        return loss\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, predictions, targets):\n",
    "        # first we need to clip the predictions so that we don't run into errors calculating log of zero\n",
    "        predictions = np.clip(predictions, 1e-7, 1 - 1e-7)\n",
    "        # calculate the negative log of all exponents\n",
    "        predictions = - np.log(predictions)\n",
    "        \n",
    "        # dependin on if the targets are one hot encoded or not find \n",
    "        # the cross entropy loss\n",
    "        if (len(targets.shape) == 1):\n",
    "            self.output = predictions[range(len(predictions)), targets].T # get the transpose, so the results would be the same\n",
    "        else:\n",
    "            # perform element wise multiplication (cross product) and get the sum of each row\n",
    "            self.output = np.sum( predictions * targets , axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7 , 0.1 , 0.2 ],\n",
       "       [0.1 , 0.5 , 0.4 ],\n",
       "       [0.02, 0.9 , 0.08]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating accuracy\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "if np.array(class_targets).shape == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "np.mean(predictions == class_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have learnt how to do a forward pass through our model and how to view \n",
    "# Its performance, now we would focus on how to optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
