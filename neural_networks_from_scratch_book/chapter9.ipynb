{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "# We know how to measure the impact of variables on a function using calculus\n",
    "# and calculating the partial derivatives of a particular variable within a function\n",
    "# and then using chain rule for calculating the impact of a variable in nested functions. \n",
    "\n",
    "# With this knowledge, we can now calculate the impart of each of our weights on the loss\n",
    "# function by calculating the partial derivatives of the weights. \n",
    "\n",
    "# Lets experiment this theory on a single neuron. \n",
    "# Minimizing our loss function is the end goal of the machine learning optimization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the forward pass\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -3.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xw0 = x[0] * w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xw1 = x[1] * w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xw2 = x[2] * w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.0, 2.0, 6.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xw0, xw1, xw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = xw0 + xw1 + xw2 + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = max(z, 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have just computed the full forward pass of the a single artifical neuron with \n",
    "# 3 inputs. Let's take all these small functions as one large chained function\n",
    "# which takes inputs, weights and biases. and produces the value y as the output. \n",
    "\n",
    "# The bigger function consists of multiple smaller functions, within that large functoin\n",
    "# we have smaller operation such as addition, multiplication and ma. \n",
    "# The first step of backpropagation is to calculate the partial derivatives of the inputs, weights and biases\n",
    "# used in computing the function. In order to do this we would be making use of the chain rule.\n",
    "# We need to calculate how much each input, weight and bias affects the output.\n",
    "\n",
    "# We will start by calculating the partial derivatives for w[0]\n",
    "\n",
    "# y = ReLU(sum(mul(x[0], w[0]), mul(x[1], w[1]), mul(x[2], w[2]), b))\n",
    "# The above equation contains 3 nested functions, ReLU, sum and multiplication.\n",
    "# We would like to know the impact of a given weight or bias on a loss. This would\n",
    "# involve us calculating the partial derivatives of the weights with respect to the \n",
    "# loss function. \n",
    "\n",
    "# During backpropagation, we calculate the derivative of the loss function. and use it to \n",
    "# multiply with the derivative of the activation function of the output layer. then use it \n",
    "# to multiply the derivatives of the output layer, and so on, we keep moving backward and multiplying\n",
    "# until we reach the leaf paramaters, which are the actual inputs to the the whole function itself. \n",
    "# Then we use the gradients of those leaf parameters to make adjustments in order to improve our loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the function above to calculate the loss of the ReLU activation function. \n",
    "relu_dz = (1. if z > 0 else 0) # ReLU derivative function\n",
    "# Since the value of Z is 6, then the derivative value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The process of going back and calculating the gradient through each step is\n",
    "# called backpropagation using the chain rule. \n",
    "\n",
    "# We  used backpropagation to calculate the inpact of each neuron on the \n",
    "# loss function, by calculating the gradient at a particular point in the \n",
    "# computational graph and moving it sequentially backwards through the computational\n",
    "# graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAll together all the all the partial derivatives of the parameters makek up a combined vector called our gradients..\\nIn this case our gradients would be\\ndx = [drelu_dx0, drelu_dx1, drelu_dx2]\\ndw = [drelu_dw0, drelu_dw1, drelu_dw2]\\ndb = drelu_db\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The partiall derivative of the bias on the output of a neuron is always 1\n",
    "# The partiall derivative of the weights on the output of a neuron is always the value of the corresponding input\n",
    "# The partiall derivative of the input of a neuron is always the value of the corresponding weight\n",
    "\"\"\"\n",
    "All together all the all the partial derivatives of the parameters makek up a combined vector called our gradients..\n",
    "In this case our gradients would be\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db\n",
    "\"\"\"\n",
    "# For this single neuron example we will not be using the dx gradient, but in future examples where this might be a hidden layers, we will need to the gradients of the input, which might be the output of another neuron.\n",
    "# With the gradient values in hand we can then apply these gradients on the weights to hopefully minimize the weight values. \n",
    "# We would be using a simplied version of an optimizere by simply subracting the value of our gradient from the weights.\n",
    "# We need a negative fraction of the gradients, since we would like to reduce our loss function and move in small incremental steps, since\n",
    "# we would be making a lot of changes simultaneously. \n",
    "\n",
    "# With this method we can reduce the output value of our gradients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have performed a single backward pass with a single neuron, now\n",
    "# we would be combining the neurons together to form a layer and experimient \n",
    "# performing the backward pass on that input. \n",
    "\n",
    "\n",
    "# Lets replace this single neuron with a layer of neurons. A layer outputs a vector\n",
    "# rather than a single scalar value. Each neuron in a layer connects to all the neurons of\n",
    "# the previous layer. \n",
    "# At the backpropagation step, this layer will recieve a vector of partial derivatives rather than\n",
    "# a single scalar partial derivative value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2 ,  0.8 , -0.5 ,  1.  ],\n",
       "       [ 0.5 , -0.91,  0.26, -0.5 ],\n",
       "       [-0.26, -0.27,  0.17,  0.87]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dvalues = np.array([[1, 1, 1]])\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2 ,  0.5 , -0.26],\n",
       "       [ 0.8 , -0.91, -0.27],\n",
       "       [-0.5 ,  0.26,  0.17],\n",
       "       [ 1.  , -0.5 ,  0.87]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44],\n",
       "       [-0.38],\n",
       "       [-0.07],\n",
       "       [ 1.37]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.sum(weights, axis=1, keepdims=True)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43999999999999995"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(weights[0] * dvalues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx0 = sum(weights[0] * dvalues[0])\n",
    "dx1 = sum(weights[1] * dvalues[0])\n",
    "dx2 = sum(weights[2] * dvalues[0])\n",
    "dx3  = sum(weights[3] * dvalues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44, -0.38, -0.07,  1.37])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44, -0.38, -0.07,  1.37])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dinputs is a gradient of the neuron funcition with respect to the inputs\n",
    "np.dot(dvalues[0], weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]\n",
      " [ 1.76 -1.52 -0.28  5.48]]\n"
     ]
    }
   ],
   "source": [
    "# Working with a batch of samples\n",
    "dvalues = np.array([[1, 1, 1],\n",
    "                    [2, 2, 2], \n",
    "                    [3, 3, 3],\n",
    "                    [4, 4, 4]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "dinputs = np.dot(dvalues, weights)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.1, -9.1, -9.1],\n",
       "       [16.1, 16.1, 16.1],\n",
       "       [ 9.7,  9.7,  9.7],\n",
       "       [-5.1, -5.1, -5.1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the derivatives of the weights. \n",
    "dvalues = np.array([[1, 1, 1],\n",
    "                    [2, 2, 2],\n",
    "                    [3, 3, 3],\n",
    "                    [4, 4, 4]])\n",
    "\n",
    "inputs = np.array([[1, 2, 3,  2.5],\n",
    "                  [2, 5,  -1, 2],\n",
    "                  [-1.5, 2.7, 3.3, -0.8],\n",
    "                  [-2.4, -1., -0.3, -2.3]])\n",
    "\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "dweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to calculate the derivatives of the biases\n",
    "dvalues = np.array([[1, 1, 1],\n",
    "                    [2, 2, 2],\n",
    "                    [3, 3, 3],\n",
    "                    [4, 4, 4]])\n",
    "\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 10, 10]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbiases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
