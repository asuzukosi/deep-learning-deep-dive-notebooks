{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers and Modules\n",
    "Layers define a single layer operation on a neural network, while modules define a group of related layers, which form a common pattern, or can even be an entire model on its own. We use modules to abstract repeated patterns in neural network design, this allows us to build more complex models with compact code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prebuilt modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "# define a sequential model and test the output on sample generated input\n",
    "\n",
    "# we did not need to define a matching input layer\n",
    "net = tf.keras.models.Sequential(\n",
    "    [ tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# generated sample input code\n",
    "\n",
    "X = tf.random.uniform((2, 14))\n",
    "# the input shape of the network is defined the firs time it is called\n",
    "print(net(X).shape)\n",
    "\n",
    "X = tf.random.uniform((2, 14))\n",
    "print(net(X).shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our custom module\n",
    "\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # we call the constructor of the parent class to \n",
    "        # perform all the necessary intialization\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    # the forward propagation of the model,\n",
    "    # which defines how we get the input from the output\n",
    "    def call(self, X):\n",
    "        return self.out(self.hidden(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sequential module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class  MySequential(tf.keras.Model):\n",
    "    def __init__(self, modules: List[tf.keras.Model]= []):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "        \n",
    "    def add_layer(self, layer: tf.keras.Model):\n",
    "        self.modules.append(layer)\n",
    "        \n",
    "    def call(self, X):\n",
    "        # loop through all the layers and perform the operation on them\n",
    "        # with the output of a layer being the input of the next layer\n",
    "        for layer in self.modules:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    \n",
    "# each layer is executed in the order in which they were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential([tf.keras.layers.Dense(units=256, activation=tf.nn.relu), \n",
    "                    tf.keras.layers.Dense(10)])\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing code in the forward propagation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FixedHiddenMLP, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # randomly generated weights are not updated during training (i.e contant parameters)\n",
    "        self.rand_weights = tf.constant(tf.random.uniform((14, 20)))\n",
    "        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        X = self.flatten(inputs)\n",
    "        # use the constant parameters as well\n",
    "        # as the relu and the matmul functions\n",
    "        X = tf.nn.relu(tf.matmul(X, self.rand_weights) + 1)\n",
    "        \n",
    "        # reuse the fully connected layers\n",
    "        X = self.dense(X)\n",
    "        # control flow\n",
    "        while tf.reduce_sum(tf.math.abs(X)) > 1:\n",
    "            X /= 2\n",
    "        return tf.reduce_sum(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.71274793, 0.65159774, 0.6920341 ],\n",
       "       [0.597064  , 0.17945683, 0.4411521 ],\n",
       "       [0.6865252 , 0.24947512, 0.40614355]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tf.random.uniform(shape=(3, 3))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.71274793, 0.65159774, 0.6920341 ],\n",
       "       [0.597064  , 0.17945683, 0.4411521 ],\n",
       "       [0.6865252 , 0.24947512, 0.40614355]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.abs(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.616197>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.math.abs(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.54806477>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested networks for people who like confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5758785>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential()\n",
    "        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(self.net(inputs))\n",
    "\n",
    "chimera = tf.keras.Sequential()\n",
    "chimera.add(NestMLP())\n",
    "chimera.add(tf.keras.layers.Dense(14))\n",
    "chimera.add(FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelMLP(tf.keras.Model):\n",
    "    def __init__(self, net1, net2):\n",
    "        super(ParallelMLP, self).__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        \n",
    "    def call(self, X):\n",
    "        X1 = self.net1(X)\n",
    "        X2 = self.net2(X)   \n",
    "        X = tf.concat([X1, X2], axis=1)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.        ,  0.        ,  0.5597044 , -0.10258479],\n",
       "       [ 0.        ,  0.        ,  0.87296665, -0.59194946]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.relu),  \n",
    "])\n",
    "\n",
    "net2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.tanh),  \n",
    "])\n",
    "\n",
    "parallel_mlp = ParallelMLP(net1, net2)\n",
    "parallel_mlp(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter management\n",
    "Parameters are what store the learned structural pattern of our data and is the purpose of trainig, once trained the parameters are the value extracted from the training process. The parameters are what is used to make future predictions when the model is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(net.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_53/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[ 0.7134845 ],\n",
       "        [ 0.13386679],\n",
       "        [-0.79644763],\n",
       "        [-0.874894  ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_53/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.ops.resource_variable_ops.ResourceVariable,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.layers[2].weights[1]), tf.convert_to_tensor(net.layers[2].weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.8567994 , -0.13660324, -0.61530447, -0.6061369 ],\n",
       "        [ 0.62813133,  0.18498129, -0.7404247 , -0.50554276],\n",
       "        [-0.2476942 , -0.229617  ,  0.03248268, -0.6440674 ],\n",
       "        [-0.75030273, -0.09672415,  0.5307556 , -0.5516254 ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.7134845 ],\n",
       "        [ 0.13386679],\n",
       "        [-0.79644763],\n",
       "        [-0.874894  ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the weights on a network\n",
    "net.get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tied parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.keras behaves a bit differently. It removes the duplicate layer\n",
    "# automatically\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(16),\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are different\n",
    "print(len(net.layers) == 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.NestMLP"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NestMLP()\n",
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 325ms/step - loss: 0.3637 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1aae550>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)\n",
    "y = tf.random.uniform((2, 1))\n",
    "net.compile(optimizer=tf.optimizers.legacy.Adam(learning_rate=1e-3), \n",
    "            loss=tf.losses.MAE,\n",
    "            metrics=[\"accuracy\"])\n",
    "net.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Initialization\n",
    "Deep learning frameworks provide default random initialization for our parameters, but there are other strategies we can use to initialize our model parameters. We may want to initialize our model parameters based on different protocols, we can also use a custom initializer to initialize our model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## built in initialization\n",
    "We can use the random normal initialization strategy that is used by default in tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_75/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.00093964, -0.00284592, -0.00988948,  0.01653008],\n",
       "        [-0.00289992,  0.0105628 , -0.01163867, -0.00177453],\n",
       "        [ 0.00653719, -0.00986328,  0.01661347,  0.00828161],\n",
       "        [ 0.00509416, -0.00229942,  0.00503318,  0.00372605]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_75/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, \n",
    "                          activation=tf.nn.relu, \n",
    "                          kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01), \n",
    "                          bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_77/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_77/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, \n",
    "                          activation=tf.nn.relu, \n",
    "                          kernel_initializer=tf.initializers.Constant(1), \n",
    "                          bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_79/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.7457518 , -0.57943475, -0.64610535, -0.0158475 ],\n",
       "        [ 0.3413654 ,  0.5317233 ,  0.42933124, -0.67540705],\n",
       "        [ 0.19575137,  0.1263653 ,  0.41023523,  0.69752795],\n",
       "        [ 0.80360955, -0.12941885,  0.78638715,  0.7876795 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_79/bias:0' shape=(4,) dtype=float32, numpy=array([42., 42., 42., 42.], dtype=float32)>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, \n",
    "                          activation=tf.nn.relu, \n",
    "                          kernel_initializer=tf.initializers.GlorotUniform(), \n",
    "                          bias_initializer=tf.initializers.Constant(42)),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_81/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 0.       ,  0.       , -0.       ,  0.       ],\n",
      "       [ 0.       ,  0.       , -7.7973294,  0.       ],\n",
      "       [ 5.177188 ,  0.       ,  0.       , -8.511324 ],\n",
      "       [-0.       ,  0.       , -0.       , -7.1130776]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# we can create our custom initializer classes by subclassing the Initializer parent class\n",
    "# then we modify the call function\n",
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None, **kwargs):\n",
    "        data =  tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor = (tf.abs(data)>=5)\n",
    "        factor = tf.cast(factor, tf.float32)\n",
    "        return data * factor\n",
    "    \n",
    "    \n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu, kernel_initializer=MyInit()),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to set our parameters directly if we wish to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Initialization\n",
    "This allows us to initialize our layers and modules without knowing the input shape of our network. This is because initialization is done on the fly, the first time data passes through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we cannot access the weights because the network is not yet initialized\n",
    "[net.layers[i].get_weights() for i in range(len(net.layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 256), (256,), (256, 10), (10,)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.random.uniform((2, 20))\n",
    "net(X)\n",
    "[w.shape for w in net.get_weights()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of custom layers\n",
    "We often need custom layers for some new algorithm implementations, in that case we would need to implement custom layers for our algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers without parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CenteredLayer, self).__init__()\n",
    "    def call(self, X):\n",
    "        # subtracts each input value by the mean across the input value matrix\n",
    "        return X - tf.reduce_mean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[-2, -1,  0,  1,  2]], dtype=int32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing out the centered layer\n",
    "layer = CenteredLayer()\n",
    "layer(tf.constant([[1, 2, 3, 4, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 128])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using our custom centered layer to create more complex architectures\n",
    "net = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])\n",
    "data = tf.random.uniform((2, 10))\n",
    "net(data).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers with parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would be implementing a custom dense layer\n",
    "class MyDense(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(MyDense, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, X_shape):\n",
    "        self.weight = self.add_weight(name='weight',\n",
    "            shape=[X_shape[-1], self.units],\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias', shape=[self.units],\n",
    "            initializer=tf.zeros_initializer())\n",
    "\n",
    "    def call(self, X):\n",
    "        linear = tf.matmul(X, self.weights[0]) + self.bias\n",
    "        return tf.nn.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0662522 ,  0.01260725, -0.04122119],\n",
       "        [ 0.01992019, -0.0181719 , -0.00875344],\n",
       "        [-0.02290619, -0.02234221,  0.01695419],\n",
       "        [ 0.0264551 , -0.05543638, -0.05993166],\n",
       "        [-0.00914237,  0.0074254 ,  0.03734822]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = MyDense(3)\n",
    "dense(tf.random.uniform((2, 5)))\n",
    "dense.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(tf.random.uniform((2, 5))) # using custom layers to carry out operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[0.00721058],\n",
       "       [0.03586151]], dtype=float32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use these custom layers with parameters to create large sequence networks\n",
    "net = tf.keras.models.Sequential([MyDense(8), MyDense(1)])\n",
    "net(tf.random.uniform((2, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Exercise1, self).__init__()\n",
    "    \n",
    "    def call(self, X):\n",
    "        return tf.reduce_sum(X)\n",
    "\n",
    "class Exercise2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def call(self, X):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File I/O\n",
    "We would often need to interface with the file I/O system to be able to store weights and model information in order to be used for future pedcitosn, we can also store the weights of a model during training at intervals so as to not lose the computation done by the models if the system should fail during training for any reason, these are called checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tensor data a file using numpy\n",
    "import numpy as np\n",
    "np.save(\"x-file.npy\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data from a file using numpy\n",
    "x2 = np.load(\"x-file.npy\", allow_pickle=True)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3.]), array([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.zeros(4)\n",
    "np.save(\"xy-file.npy\", [x, y])\n",
    "x2, y2 = np.load(\"xy-file.npy\", allow_pickle=True)\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'x': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>, 'y': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can store dictionaries with the numpy format\n",
    "mydict = {'x':x, 'y':y}\n",
    "np.save('mydict.npy', mydict)\n",
    "mydict2 = np.load('mydict.npy', allow_pickle=True)\n",
    "mydict2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving model parameters\n",
    "We need to save entire model parameters because when we start to deal with more complex models, it becomes difficult to save all the individual weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.hidden = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(10)\n",
    "        \n",
    "        \n",
    "    def call(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = self.hidden(X)\n",
    "        out = self.out(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "X = tf.random.uniform((2, 20))\n",
    "Y = net(X)\n",
    "\n",
    "net.save_weights(\"mlp.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2c25a0090>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_weights(\"mlp.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with GPUs\n",
    "GPUs enable us to perform more complex computations in a short amount of time, this is because GPUs enable parallelization of operations which is useful for training large neural networks. You can work with GPUs as a single GPU on a device or multiple GPUs on a device, our you could use it as a network of distributed computers, each with their own individual GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "# check if your device has a gpu\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices() # list all the logical computational devices available to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices() # lists all the physical devices available to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_gpus():\n",
    "    return len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "get_num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for an operation between two variables to be computed, the\n",
    "# two variables must be on the same device. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks and GPUs\n",
    "We can store entire neural network architectures on a specific GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    net = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(1)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
