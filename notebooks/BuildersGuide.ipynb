{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers and Modules\n",
    "Layers define a single layer operation on a neural network, while modules define a group of related layers, which form a common pattern, or can even be an entire model on its own. We use modules to abstract repeated patterns in neural network design, this allows us to build more complex models with compact code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prebuilt modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "# define a sequential model and test the output on sample generated input\n",
    "\n",
    "# we did not need to define a matching input layer\n",
    "net = tf.keras.models.Sequential(\n",
    "    [ tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# generated sample input code\n",
    "\n",
    "X = tf.random.uniform((2, 14))\n",
    "# the input shape of the network is defined the firs time it is called\n",
    "print(net(X).shape)\n",
    "\n",
    "X = tf.random.uniform((2, 14))\n",
    "print(net(X).shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our custom module\n",
    "\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # we call the constructor of the parent class to \n",
    "        # perform all the necessary intialization\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    # the forward propagation of the model,\n",
    "    # which defines how we get the input from the output\n",
    "    def call(self, X):\n",
    "        return self.out(self.hidden(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sequential module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class  MySequential(tf.keras.Model):\n",
    "    def __init__(self, modules: List[tf.keras.Model]= []):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "        \n",
    "    def add_layer(self, layer: tf.keras.Model):\n",
    "        self.modules.append(layer)\n",
    "        \n",
    "    def call(self, X):\n",
    "        # loop through all the layers and perform the operation on them\n",
    "        # with the output of a layer being the input of the next layer\n",
    "        for layer in self.modules:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    \n",
    "# each layer is executed in the order in which they were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential([tf.keras.layers.Dense(units=256, activation=tf.nn.relu), \n",
    "                    tf.keras.layers.Dense(10)])\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing code in the forward propagation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FixedHiddenMLP, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # randomly generated weights are not updated during training (i.e contant parameters)\n",
    "        self.rand_weights = tf.constant(tf.random.uniform((14, 20)))\n",
    "        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        X = self.flatten(inputs)\n",
    "        # use the constant parameters as well\n",
    "        # as the relu and the matmul functions\n",
    "        X = tf.nn.relu(tf.matmul(X, self.rand_weights) + 1)\n",
    "        \n",
    "        # reuse the fully connected layers\n",
    "        X = self.dense(X)\n",
    "        # control flow\n",
    "        while tf.reduce_sum(tf.math.abs(X)) > 1:\n",
    "            X /= 2\n",
    "        return tf.reduce_sum(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.71274793, 0.65159774, 0.6920341 ],\n",
       "       [0.597064  , 0.17945683, 0.4411521 ],\n",
       "       [0.6865252 , 0.24947512, 0.40614355]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tf.random.uniform(shape=(3, 3))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.71274793, 0.65159774, 0.6920341 ],\n",
       "       [0.597064  , 0.17945683, 0.4411521 ],\n",
       "       [0.6865252 , 0.24947512, 0.40614355]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.abs(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.616197>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.math.abs(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.54806477>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested networks for people who like confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5758785>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential()\n",
    "        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(self.net(inputs))\n",
    "\n",
    "chimera = tf.keras.Sequential()\n",
    "chimera.add(NestMLP())\n",
    "chimera.add(tf.keras.layers.Dense(14))\n",
    "chimera.add(FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelMLP(tf.keras.Model):\n",
    "    def __init__(self, net1, net2):\n",
    "        super(ParallelMLP, self).__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        \n",
    "    def call(self, X):\n",
    "        X1 = self.net1(X)\n",
    "        X2 = self.net2(X)   \n",
    "        X = tf.concat([X1, X2], axis=1)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.        ,  0.        ,  0.5597044 , -0.10258479],\n",
       "       [ 0.        ,  0.        ,  0.87296665, -0.59194946]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.relu),  \n",
    "])\n",
    "\n",
    "net2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.tanh),  \n",
    "])\n",
    "\n",
    "parallel_mlp = ParallelMLP(net1, net2)\n",
    "parallel_mlp(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter management\n",
    "Parameters are what store the learned structural pattern of our data and is the purpose of trainig, once trained the parameters are the value extracted from the training process. The parameters are what is used to make future predictions when the model is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(net.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_53/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[ 0.7134845 ],\n",
       "        [ 0.13386679],\n",
       "        [-0.79644763],\n",
       "        [-0.874894  ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_53/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.ops.resource_variable_ops.ResourceVariable,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.layers[2].weights[1]), tf.convert_to_tensor(net.layers[2].weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.8567994 , -0.13660324, -0.61530447, -0.6061369 ],\n",
       "        [ 0.62813133,  0.18498129, -0.7404247 , -0.50554276],\n",
       "        [-0.2476942 , -0.229617  ,  0.03248268, -0.6440674 ],\n",
       "        [-0.75030273, -0.09672415,  0.5307556 , -0.5516254 ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.7134845 ],\n",
       "        [ 0.13386679],\n",
       "        [-0.79644763],\n",
       "        [-0.874894  ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the weights on a network\n",
    "net.get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tied parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.keras behaves a bit differently. It removes the duplicate layer\n",
    "# automatically\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(16),\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are different\n",
    "print(len(net.layers) == 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.NestMLP"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NestMLP()\n",
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 325ms/step - loss: 0.3637 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1aae550>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)\n",
    "y = tf.random.uniform((2, 1))\n",
    "net.compile(optimizer=tf.optimizers.legacy.Adam(learning_rate=1e-3), \n",
    "            loss=tf.losses.MAE,\n",
    "            metrics=[\"accuracy\"])\n",
    "net.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Initialization\n",
    "Deep learning frameworks provide default random initialization for our parameters, but there are other strategies we can use to initialize our model parameters. We may want to initialize our model parameters based on different protocols, we can also use a custom initializer to initialize our model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## built in initialization\n",
    "We can use the random normal initialization strategy that is used by default in tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_75/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.00093964, -0.00284592, -0.00988948,  0.01653008],\n",
       "        [-0.00289992,  0.0105628 , -0.01163867, -0.00177453],\n",
       "        [ 0.00653719, -0.00986328,  0.01661347,  0.00828161],\n",
       "        [ 0.00509416, -0.00229942,  0.00503318,  0.00372605]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_75/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, \n",
    "                          activation=tf.nn.relu, \n",
    "                          kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01), \n",
    "                          bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_77/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_77/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, \n",
    "                          activation=tf.nn.relu, \n",
    "                          kernel_initializer=tf.initializers.Constant(1), \n",
    "                          bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_79/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.7457518 , -0.57943475, -0.64610535, -0.0158475 ],\n",
       "        [ 0.3413654 ,  0.5317233 ,  0.42933124, -0.67540705],\n",
       "        [ 0.19575137,  0.1263653 ,  0.41023523,  0.69752795],\n",
       "        [ 0.80360955, -0.12941885,  0.78638715,  0.7876795 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_79/bias:0' shape=(4,) dtype=float32, numpy=array([42., 42., 42., 42.], dtype=float32)>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, \n",
    "                          activation=tf.nn.relu, \n",
    "                          kernel_initializer=tf.initializers.GlorotUniform(), \n",
    "                          bias_initializer=tf.initializers.Constant(42)),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_81/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 0.       ,  0.       , -0.       ,  0.       ],\n",
      "       [ 0.       ,  0.       , -7.7973294,  0.       ],\n",
      "       [ 5.177188 ,  0.       ,  0.       , -8.511324 ],\n",
      "       [-0.       ,  0.       , -0.       , -7.1130776]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# we can create our custom initializer classes by subclassing the Initializer parent class\n",
    "# then we modify the call function\n",
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None, **kwargs):\n",
    "        data =  tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor = (tf.abs(data)>=5)\n",
    "        factor = tf.cast(factor, tf.float32)\n",
    "        return data * factor\n",
    "    \n",
    "    \n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu, kernel_initializer=MyInit()),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to set our parameters directly if we wish to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Initialization\n",
    "This allows us to initialize our layers and modules without knowing the input shape of our network. This is because initialization is done on the fly, the first time data passes through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we cannot access the weights because the network is not yet initialized\n",
    "[net.layers[i].get_weights() for i in range(len(net.layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 256), (256,), (256, 10), (10,)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.random.uniform((2, 20))\n",
    "net(X)\n",
    "[w.shape for w in net.get_weights()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
