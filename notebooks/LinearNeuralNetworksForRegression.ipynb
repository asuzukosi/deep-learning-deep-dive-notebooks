{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Linear Neural Networks for Machine Learning Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0105, -0.3524, -0.8405,  0.1248,  1.1378, -0.4270, -1.3653,  1.2809,\n",
       "          -0.6189,  0.0762],\n",
       "         [ 0.6146, -0.6661,  0.2844,  2.7747, -2.3469, -0.5257, -0.4112, -0.1374,\n",
       "          -1.6002, -2.6443],\n",
       "         [ 0.1022,  0.5095,  0.3900, -1.2284,  2.3054, -0.5433, -0.3673, -1.1440,\n",
       "          -0.4079,  0.5785],\n",
       "         [ 0.8475,  1.0511,  0.2029, -0.5822, -1.0615, -1.7253,  0.5514,  1.7176,\n",
       "           0.7520, -0.6288],\n",
       "         [-0.2764,  0.6178, -0.4972, -2.1459, -0.7055,  1.5842,  1.6377, -0.6056,\n",
       "           1.8395, -0.3558],\n",
       "         [ 0.0044, -0.4361,  0.3105, -0.1385,  0.6621,  1.5718,  0.5231,  0.3330,\n",
       "           1.4507, -0.4212],\n",
       "         [ 0.1110,  0.4385,  1.6466, -0.5679, -1.1906, -0.9303, -1.8452,  2.0042,\n",
       "           0.2904,  0.5847],\n",
       "         [ 0.5209,  0.0863,  0.9270, -1.7087,  0.2636, -1.4006,  1.3385,  0.0141,\n",
       "           0.4232,  2.2421],\n",
       "         [-0.5329, -1.7276,  1.1089, -2.0264,  1.3249,  0.5282,  1.3302,  0.7906,\n",
       "          -1.2325,  0.1138],\n",
       "         [ 0.1085, -0.4922, -0.8443, -0.7001, -0.5470, -0.1673, -0.2258,  0.1969,\n",
       "          -0.6295, -0.8916]]),\n",
       " tensor([-1.8964, -1.2770, -0.2822, -0.7292,  0.6386,  0.6925, -1.0168, -0.2393,\n",
       "          0.7384, -0.4081]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10, 10)\n",
    "y = torch.randn(10)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùë¶ÃÇ =ùê∞‚ä§ùê±+ùëè. is the formula for linear regression. Givent that the upper T means a vector multiplication between the weights w and the sample vector x, added to the bias will give the prediction y hat."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùê≤ÃÇ =ùêóùê∞+ùëè, this will give all the predictions of y for all the samples in matrix X. In this case this is a matrix vector multiplication and the value of y is a vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole point of machine learning is to search for the best parameters w and b, such that given a new unseen sample that has the same distribution as the X matrix the model will be able to accurately predict a value y for the new sample that is not too far from the distribution of y."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùëô(ùëñ)(ùê∞,ùëè)=1/2(ùë¶ÃÇ(ùëñ)‚àíùë¶(ùëñ))^2 is the loss funtion we use to predict if our model is performing properly in a regression probelm. This is known a squared arror loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_pred, y_actual):\n",
    "    return 0.5 * ((y_pred - y_actual)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_actual):\n",
    "    if(len(y_actual)!=len(y_pred)):\n",
    "        raise Exception(\"Length of y_pred and y_actual must be equal\")\n",
    "    \n",
    "    squared_errors = []\n",
    "    for i in range(len(y_actual)): \n",
    "        squared_errors.append(squared_error(y_pred[i], y_actual[i]))\n",
    "        \n",
    "    return sum(squared_errors)/len(squared_errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùê∞‚àó=(ùêó‚ä§ùêó)‚àí1ùêó‚ä§ùê≤, we can us the analytic solution to fine the appropriate weigts for a give problem, but the analytic solution is so restrictive that it would not suffice for solving the most exciting problems in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the appropriate weights using the analytical solution\n",
    "\n",
    "def analystic_solution(X, y):\n",
    "    solution = torch.mm(X, X)\n",
    "    solution = torch.linalg.inv(solution)\n",
    "    solution = solution * (torch.mv(X, y))\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9007e+01, -6.2344e+01,  6.4169e+00,  1.6892e+01,  3.9624e+01,\n",
       "         -9.8062e+00, -8.0019e-01, -2.4872e+01,  1.2526e+00, -1.4920e+01],\n",
       "        [ 1.0613e+01, -1.8492e+01,  2.8515e+00,  1.5286e+00,  1.0414e+01,\n",
       "         -6.5795e-01, -4.3969e-02, -6.7103e+00, -9.4963e-02, -3.8291e+00],\n",
       "        [-2.6523e+00,  4.0194e+00, -3.0021e-01, -1.9942e+00, -2.4172e+00,\n",
       "          4.7917e-01,  6.5802e-02,  2.2544e+00,  2.8787e-01,  7.5786e-01],\n",
       "        [-1.2973e+00,  1.9226e+00, -1.6229e-01, -6.0957e-01, -1.2410e+00,\n",
       "          3.4665e-01,  1.3808e-02,  9.8791e-01, -1.3649e-01,  5.8528e-01],\n",
       "        [-1.1359e+00,  1.3649e+00, -1.5007e-01,  9.0920e-02, -1.0524e+00,\n",
       "          5.6368e-01, -5.5252e-02,  5.3290e-01, -1.6425e-01,  4.3980e-01],\n",
       "        [ 2.2696e+01, -3.7016e+01,  4.4170e+00,  8.0673e+00,  2.2863e+01,\n",
       "         -4.2856e+00, -3.3882e-01, -1.4580e+01,  3.2476e-01, -8.3744e+00],\n",
       "        [-6.0228e-01,  7.5568e-01,  2.2478e-01, -1.4045e+00, -8.5432e-01,\n",
       "          9.9978e-01,  5.0414e-02,  6.3423e-01, -3.5586e-01,  3.8104e-01],\n",
       "        [ 6.4579e+00, -1.0814e+01,  1.3451e+00,  2.0735e+00,  6.2764e+00,\n",
       "         -4.4316e-01, -2.8810e-02, -4.4884e+00, -3.6352e-01, -2.1851e+00],\n",
       "        [-1.5595e+01,  2.5927e+01, -3.4624e+00, -4.0620e+00, -1.5488e+01,\n",
       "          2.0994e+00,  1.8568e-01,  1.0131e+01,  1.1659e-01,  5.6444e+00],\n",
       "        [ 9.0753e+00, -1.4498e+01,  1.6414e+00,  3.0606e+00,  8.9870e+00,\n",
       "         -1.7200e+00, -1.2028e-01, -5.8797e+00,  1.6114e-01, -3.1821e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = analystic_solution(X, y)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice the technique for finding the parameters is by gradually adjusting the weights and reducing the loss function, till we reach a point where further adjustment of the weights will not lead to reduction in the loss function result, this iterative process is known as gradient descent, as we are gradually descending the gradient of the loss function to find the lowest possible value for loss function/objective function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current nature of evaluating our loss function it would mean that we would need to find the loss of every single sample before a single update to the weights, this would be extremely slow, therefore an alternate technique can be used which is to update the gradients after getting the loss for a single sample. Even at that updating the weights has both statistical and performance drawbacks, therefore the solution is to pick the middle ground by breaking the updates into batches. This will lead to minibatch stochastic gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of each weight is the amount it is supposed to change in order to minimize the loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunable parameters that are not updated in the training loop are called hyperparameters, examples of such are: Learning rate, Batch size etc. They can be tuned automatically by a number of techniques, such as Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
