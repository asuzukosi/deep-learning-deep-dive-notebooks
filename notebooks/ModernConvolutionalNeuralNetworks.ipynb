{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Modern Computer Vision architecutres\n",
    "In this section we would be discussing populuar production CNN architecuteres that are used in production applications today. These architectures are a result of rigorous experimentation, this means that the hyper parameters and architecture is definite  and the best combination for that type of problem. \n",
    "We would be looking at: \n",
    "- AlexNet\n",
    "- VGG\n",
    "- NiN\n",
    "- GoogLeNet\n",
    "- ResNet\n",
    "- ResNeXt\n",
    "- DenseNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "AlexNet was the direct successor to LeNet, it was the first architecture to make us of GPUs for training neural networks, and it was able to win the image net competition. Because of the size of the model architecture, the model was split into two across GPUs, this enabled the memory requirements of the model to be met. The architecture also made some other improvements over LeNet such as using ReLU instead of sigmoid as the activation function, It made use of MaxPooling as opposed to average pooling used in LeNet, It also made use of DropOut as a regularization technique to stabilize training in the fully connected layers. The convnet code was used as the standard for implementing deep neural networks for some time, this was before the popularization of deep learning libraries such as tensorflow and pytorch. These advancedments were also due to the availability of a large dataset such as ImageNet, which was required to achieve good performance in deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of AlexNet using the d2l classifier class\n",
    "class AlexNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=96, kernel_size=11,\n",
    "                                   strides=4, activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2), \n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=256, kernel_size=5, \n",
    "                                   padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=384, kernel_size=3,\n",
    "                                   padding='same', activation='relu'),\n",
    "            tf.keras.layers.Conv2D(filters=384, kernel_size=3, \n",
    "                                   padding='same', activation='relu'),\n",
    "            tf.keras.layers.Conv2D(filters=256, kernel_size=3,\n",
    "                                   padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(4096, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(4096, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(num_classes)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D output shape:\t (1, 54, 54, 96)\n",
      "MaxPooling2D output shape:\t (1, 26, 26, 96)\n",
      "Conv2D output shape:\t (1, 26, 26, 256)\n",
      "MaxPooling2D output shape:\t (1, 12, 12, 256)\n",
      "Conv2D output shape:\t (1, 12, 12, 384)\n",
      "Conv2D output shape:\t (1, 12, 12, 384)\n",
      "Conv2D output shape:\t (1, 12, 12, 256)\n",
      "MaxPooling2D output shape:\t (1, 5, 5, 256)\n",
      "Flatten output shape:\t (1, 6400)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# show layer summary\n",
    "AlexNet().layer_summary((1, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 01:41:07.036629: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m d2l\u001b[39m.\u001b[39mtry_gpu():\n\u001b[1;32m      4\u001b[0m     model \u001b[39m=\u001b[39m AlexNet(lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/d2l/tensorflow.py:264\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_batch_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_epoch()\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/d2l/tensorflow.py:279\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    278\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_batch(batch))\n\u001b[0;32m--> 279\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_clip_val \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    281\u001b[0m     grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_gradients(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_clip_val, grads)\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1065\u001b[0m     flat_targets,\n\u001b[1;32m   1066\u001b[0m     flat_sources,\n\u001b[1;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/tensorflow/python/ops/nn_grad.py:582\u001b[0m, in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    573\u001b[0m shape_0, shape_1 \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape_n([op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m], op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]])\n\u001b[1;32m    575\u001b[0m \u001b[39m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[39m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m# in Eager mode.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 582\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39;49mconv2d_backprop_input(\n\u001b[1;32m    583\u001b[0m         shape_0,\n\u001b[1;32m    584\u001b[0m         op\u001b[39m.\u001b[39;49minputs[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    585\u001b[0m         grad,\n\u001b[1;32m    586\u001b[0m         dilations\u001b[39m=\u001b[39;49mdilations,\n\u001b[1;32m    587\u001b[0m         strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[1;32m    588\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m    589\u001b[0m         explicit_paddings\u001b[39m=\u001b[39;49mexplicit_paddings,\n\u001b[1;32m    590\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39;49muse_cudnn_on_gpu,\n\u001b[1;32m    591\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format),\n\u001b[1;32m    592\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39mconv2d_backprop_filter(\n\u001b[1;32m    593\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m    594\u001b[0m         shape_1,\n\u001b[1;32m    595\u001b[0m         grad,\n\u001b[1;32m    596\u001b[0m         dilations\u001b[39m=\u001b[39mdilations,\n\u001b[1;32m    597\u001b[0m         strides\u001b[39m=\u001b[39mstrides,\n\u001b[1;32m    598\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m    599\u001b[0m         explicit_paddings\u001b[39m=\u001b[39mexplicit_paddings,\n\u001b[1;32m    600\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39muse_cudnn_on_gpu,\n\u001b[1;32m    601\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format)\n\u001b[1;32m    602\u001b[0m ]\n",
      "File \u001b[0;32m~/Developer/machine_learning/dive-into-deep-learning/venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py:1619\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   1618\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1619\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   1620\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConv2DBackpropInput\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_sizes, \u001b[39mfilter\u001b[39;49m, out_backprop,\n\u001b[1;32m   1621\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m, strides, \u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_cudnn_on_gpu, \u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1622\u001b[0m       padding, \u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m, explicit_paddings, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1623\u001b[0m       data_format, \u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m, dilations)\n\u001b[1;32m   1624\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1625\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "with d2l.try_gpu():\n",
    "    model = AlexNet(lr=0.01)\n",
    "    trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG (Visual Geometry Group at Oxford)\n",
    "The VGG introduced the concept of convolutional blocks, these blocks consisted of several convolution layers and ending with a pooling layer that was used to reduce the dimensions of the image. The AlexNet architecture proved that feature learning with deep neural networks was possible but it did not provide a framework for developing newer CNN algorithms, VGG introduced the concept of the VGG block, these blocks can be stacked together to create deeper and therefore more powerful neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of single VGG block\n",
    "def vgg_block(num_convs, num_channels):\n",
    "    blk = tf.keras.models.Sequential()\n",
    "    \n",
    "    for _ in range(num_convs):\n",
    "        blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size=3, \n",
    "                                       padding='same', activation='relu'))\n",
    "    \n",
    "    blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG blocks can be stacked and interchanged in different ways, therefore\n",
    "# VGG does not define a single architecture, but rather a family of architectures\n",
    "\n",
    "class VGG(d2l.Classifier):\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = tf.keras.models.Sequential()\n",
    "        for (num_convs, num_channels) in arch:\n",
    "            self.net.add(vgg_block(num_convs, num_channels))\n",
    "        self.net.add(tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(4096, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(4096, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(num_classes)\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11 = VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t (1, 112, 112, 64)\n",
      "Sequential output shape:\t (1, 56, 56, 128)\n",
      "Sequential output shape:\t (1, 28, 28, 256)\n",
      "Sequential output shape:\t (1, 14, 14, 512)\n",
      "Sequential output shape:\t (1, 7, 7, 512)\n",
      "Sequential output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "vgg11.layer_summary((1, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use smaller number of channels to train VGG 11 to be used on fashion MNIST\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "with d2l.try_gpu():\n",
    "    model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
    "    trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network in Network (NiN)\n",
    "This architure proposes the use of 1X1 convolutional layers to add non-linearity in the convolutional layer, it then uses average pooling to group the layers, removing the need for large fully connected layers at the end of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the NiN block\n",
    "def nin_block(out_channels, kernel_size, strides, padding):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(out_channels, kernel_size, \n",
    "                               strides, padding),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(out_channels, 1),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(out_channels, 1),\n",
    "        tf.keras.layers.Activation('relu')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NiN don't make use of fully connected layers, \n",
    "# rather they make use of average pooling, with the last layer\n",
    "# being an NiN block with the number of channels being equal to the number\n",
    "# of target classes, then the average pooling is applied on it to produce the logits\n",
    "\n",
    "class NiN(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = tf.keras.models.Sequential([\n",
    "            nin_block(96, kernel_size=11, strides=4, padding='valid'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "            nin_block(256, kernel_size=5, strides=1, padding='same'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "            nin_block(384, kernel_size=3, strides=1, padding='same'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            nin_block(num_classes, kernel_size=3, strides=1, padding='same'),\n",
    "            tf.keras.layers.GlobalAvgPool2D(),\n",
    "            tf.keras.layers.Flatten()\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
