{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the preliminaries needed for deep learning.\n",
    "# First we will cover reading and processing of data.\n",
    "\n",
    "# n-dimensional array is called a tensor. the tensor class supports \n",
    "# automatic differentiation. Second, it leverages GPUs to accelerate \n",
    "# numerical computation, whereas NumPy only runs on CPUs. \n",
    "# These properties make neural networks both easy to code and fast to run.\n",
    "import torch\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A one dimentional tensor is known as vector, a 2 dimentional tensor is known as a matrix, while a tensor with dimensions greater than 2 is known as the dimentional size order tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pytorch\n",
    "\n",
    "torch_x = torch.arange(12, dtype=torch.float32)\n",
    "torch_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one from pytorch is refered to as a tenso class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using mxnet\n",
    "jax_x = jnp.arange(12, dtype=jnp.float32)\n",
    "jax_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one from jax is regarded as an array class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using tensorflow\n",
    "tf_x = tf.range(12, dtype=tf.float32)\n",
    "tf_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of the result of tf_x is also an array, similar to that of jax, but different from that of pytorch, could it be because they are both developed by google  lol. Wait, it's in a parent class callled tf.Tensor, so its a bit of both, hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the number of items in a tensor in pytorch\n",
    "torch_x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the number of items in an array in jax\n",
    "jax_x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=12>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the number of items in a tf.Tensor in tensorflow\n",
    "tf.size(tf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use the .shape attribute in all the packages to get the shape of the tensor \n",
    "jax_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tf.Tensor(\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# reshaping all the vectors into matrices \n",
    "jax_x = jax_x.reshape(3, 4)\n",
    "tf_x = tf.reshape(tf_x, (3, 4))\n",
    "torch_x = torch_x.reshape(3, 4)\n",
    "\n",
    "print(jax_x)\n",
    "print(torch_x)\n",
    "print(tf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=\n",
       "array([[[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]]], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones((2, 3, 4), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.ones((2, 3, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating values of gaussian distribution with mean of 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6098, -1.1948, -0.4374,  0.7247],\n",
       "         [ 0.4508, -1.0023,  1.4691,  0.8865],\n",
       "         [ 0.7082,  0.2902, -1.6812, -0.9743]],\n",
       "\n",
       "        [[ 0.5307, -1.0626, -0.2514,  2.1197],\n",
       "         [-0.3099,  0.7009, -0.0970,  0.2478],\n",
       "         [ 0.7719, -0.9232,  1.1381, -0.6216]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.1901639 , -1.0996888 ,  0.44367844,  0.5984697 ],\n",
       "       [-0.39189556,  0.69261974,  0.46018356, -2.068578  ],\n",
       "       [-0.21438177, -0.9898306 , -0.6789304 ,  0.27362573]],      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.normal(jax.random.PRNGKey(0), (3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-3.4537306e-04, -2.5005600e-01, -5.2125174e-01, -3.9218172e-01],\n",
       "       [ 3.9020357e-01, -1.5002146e+00,  8.0105311e-01,  5.2865237e-01],\n",
       "       [-3.9408860e-01, -1.5604352e+00,  1.9296621e-01, -3.3896801e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 4., 3.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [4., 3., 2., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing tensors from exact values in torch\n",
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/l0wz5p3d6175vd4ztcxpqfw40000gn/T/ipykernel_8273/1068862704.py:2: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  jnp.array(object=[[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=jnp.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[2., 1., 4., 3.],\n",
       "       [1., 2., 3., 4.],\n",
       "       [4., 3., 2., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing tensors from exact values in jax\n",
    "jnp.array(object=[[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[2., 1., 4., 3.],\n",
       "       [1., 2., 3., 4.],\n",
       "       [4., 3., 2., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing tensors from exact values in tensorflow\n",
    "tf.constant(value=[[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing items within a tensor like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([4., 5., 6., 7.], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_x[0][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax arrays and tensor flows are immutable, to set them we must use special operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.,  1.,  2.,  3.],\n",
       "       [17.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_x.at[1,0].set(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation ResourceStridedSliceAssign: Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nResourceStridedSliceAssign: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  ref (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  ResourceStridedSliceAssign (ResourceStridedSliceAssign) /job:localhost/replica:0/task:0/device:GPU:0\n\nOp: ResourceStridedSliceAssign\nNode attrs: T=DT_FLOAT, Index=DT_INT32, shrink_axis_mask=3, begin_mask=0, ellipsis_mask=0, new_axis_mask=0, end_mask=0\nRegistered kernels:\n  device='XLA_CPU_JIT'; Index in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 16005131165644881776, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='DEFAULT'; T in [DT_INT32]\n  device='CPU'; T in [DT_UINT64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_UINT32]\n  device='CPU'; T in [DT_UINT16]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_UINT8]\n  device='CPU'; T in [DT_INT8]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_BOOL]\n  device='CPU'; T in [DT_STRING]\n  device='CPU'; T in [DT_RESOURCE]\n  device='CPU'; T in [DT_VARIANT]\n  device='CPU'; T in [DT_QINT8]\n  device='CPU'; T in [DT_QUINT8]\n  device='CPU'; T in [DT_QINT32]\n\n\t [[{{node ResourceStridedSliceAssign}}]] [Op:ResourceStridedSliceAssign] name: strided_slice/_assign",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_var \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(tf_x)\n\u001b[0;32m----> 2\u001b[0m x_var[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49massign(\u001b[39m9\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m x_var\n",
      "File \u001b[0;32m~/Developer/pythonProjects/dive-into-deep-learning/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1290\u001b[0m, in \u001b[0;36mstrided_slice.<locals>.assign\u001b[0;34m(val, name)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1288\u001b[0m   name \u001b[39m=\u001b[39m parent_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_assign\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mreturn\u001b[39;00m var\u001b[39m.\u001b[39;49m_strided_slice_assign(\n\u001b[1;32m   1291\u001b[0m     begin\u001b[39m=\u001b[39;49mbegin,\n\u001b[1;32m   1292\u001b[0m     end\u001b[39m=\u001b[39;49mend,\n\u001b[1;32m   1293\u001b[0m     strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[1;32m   1294\u001b[0m     value\u001b[39m=\u001b[39;49mval,\n\u001b[1;32m   1295\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1296\u001b[0m     begin_mask\u001b[39m=\u001b[39;49mbegin_mask,\n\u001b[1;32m   1297\u001b[0m     end_mask\u001b[39m=\u001b[39;49mend_mask,\n\u001b[1;32m   1298\u001b[0m     ellipsis_mask\u001b[39m=\u001b[39;49mellipsis_mask,\n\u001b[1;32m   1299\u001b[0m     new_axis_mask\u001b[39m=\u001b[39;49mnew_axis_mask,\n\u001b[1;32m   1300\u001b[0m     shrink_axis_mask\u001b[39m=\u001b[39;49mshrink_axis_mask)\n",
      "File \u001b[0;32m~/Developer/pythonProjects/dive-into-deep-learning/.venv/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:1435\u001b[0m, in \u001b[0;36mBaseResourceVariable._strided_slice_assign\u001b[0;34m(self, begin, end, strides, value, name, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_strided_slice_assign\u001b[39m(\u001b[39mself\u001b[39m, begin, end, strides, value, name, begin_mask,\n\u001b[1;32m   1431\u001b[0m                           end_mask, ellipsis_mask, new_axis_mask,\n\u001b[1;32m   1432\u001b[0m                           shrink_axis_mask):\n\u001b[1;32m   1433\u001b[0m   \u001b[39mwith\u001b[39;00m _handle_graph(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dependencies():\n\u001b[1;32m   1434\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy_read(\n\u001b[0;32m-> 1435\u001b[0m         gen_array_ops\u001b[39m.\u001b[39;49mresource_strided_slice_assign(\n\u001b[1;32m   1436\u001b[0m             ref\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1437\u001b[0m             begin\u001b[39m=\u001b[39;49mbegin,\n\u001b[1;32m   1438\u001b[0m             end\u001b[39m=\u001b[39;49mend,\n\u001b[1;32m   1439\u001b[0m             strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[1;32m   1440\u001b[0m             value\u001b[39m=\u001b[39;49mops\u001b[39m.\u001b[39;49mconvert_to_tensor(value, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype),\n\u001b[1;32m   1441\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1442\u001b[0m             begin_mask\u001b[39m=\u001b[39;49mbegin_mask,\n\u001b[1;32m   1443\u001b[0m             end_mask\u001b[39m=\u001b[39;49mend_mask,\n\u001b[1;32m   1444\u001b[0m             ellipsis_mask\u001b[39m=\u001b[39;49mellipsis_mask,\n\u001b[1;32m   1445\u001b[0m             new_axis_mask\u001b[39m=\u001b[39;49mnew_axis_mask,\n\u001b[1;32m   1446\u001b[0m             shrink_axis_mask\u001b[39m=\u001b[39;49mshrink_axis_mask))\n",
      "File \u001b[0;32m~/Developer/pythonProjects/dive-into-deep-learning/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:8617\u001b[0m, in \u001b[0;36mresource_strided_slice_assign\u001b[0;34m(ref, begin, end, strides, value, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   8615\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   8616\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 8617\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   8618\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   8619\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/pythonProjects/dive-into-deep-learning/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation ResourceStridedSliceAssign: Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nResourceStridedSliceAssign: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  ref (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  ResourceStridedSliceAssign (ResourceStridedSliceAssign) /job:localhost/replica:0/task:0/device:GPU:0\n\nOp: ResourceStridedSliceAssign\nNode attrs: T=DT_FLOAT, Index=DT_INT32, shrink_axis_mask=3, begin_mask=0, ellipsis_mask=0, new_axis_mask=0, end_mask=0\nRegistered kernels:\n  device='XLA_CPU_JIT'; Index in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 16005131165644881776, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='DEFAULT'; T in [DT_INT32]\n  device='CPU'; T in [DT_UINT64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_UINT32]\n  device='CPU'; T in [DT_UINT16]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_UINT8]\n  device='CPU'; T in [DT_INT8]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_BOOL]\n  device='CPU'; T in [DT_STRING]\n  device='CPU'; T in [DT_RESOURCE]\n  device='CPU'; T in [DT_VARIANT]\n  device='CPU'; T in [DT_QINT8]\n  device='CPU'; T in [DT_QUINT8]\n  device='CPU'; T in [DT_QINT32]\n\n\t [[{{node ResourceStridedSliceAssign}}]] [Op:ResourceStridedSliceAssign] name: strided_slice/_assign"
     ]
    }
   ],
   "source": [
    "x_var = tf.Variable(tf_x)\n",
    "x_var[1, 2].assign(9)\n",
    "x_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflows API is just unneccerily complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17., 17., 17.,  3.],\n",
       "        [17., 17., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_x[:2, :3] = 17\n",
    "torch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[17., 17., 17.,  3.],\n",
       "       [17., 17., 17.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_x.at[:2, :3].set(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_x = torch_x.reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01],\n",
       "        [5.4598e+01, 1.4841e+02, 4.0343e+02, 1.0966e+03],\n",
       "        [2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_x = torch.exp(torch_x)\n",
    "torch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to perform eleemnt wise tensor operation the elements must be of the same shape\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "y = torch.arange(12, dtype=torch.float32)\n",
    "\n",
    "x = x.reshape((3,4))\n",
    "y = y.reshape((2,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[39m+\u001b[39;49m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X > Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X < Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of all the element values in a tesnor\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y[:] = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting a pytorch tensor into a python scalar\n",
    "\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# create cv file for generated data\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''NumRooms,RoofType,Price\n",
    "NA,NA,127500\n",
    "2,NA,106000\n",
    "4,Slate,178100\n",
    "NA,NA,140000''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>RoofType</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Slate</td>\n",
       "      <td>178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms RoofType   Price\n",
       "0       NaN      NaN  127500\n",
       "1       2.0      NaN  106000\n",
       "2       4.0    Slate  178100\n",
       "3       NaN      NaN  140000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read dataframe from csv file\n",
    "df  = pd.read_csv('../data/house_tiny.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NumRooms', 'RoofType', 'Price'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got DataFrame)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(df)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got DataFrame)"
     ]
    }
   ],
   "source": [
    "tf_tensor = torch.from_numpy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       NaN               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       NaN               0             1\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = df.iloc[:, 0:2], df.iloc[:, 2]\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       3.0               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       3.0               0             1\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 0., 1.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 1., 0.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = torch.tensor(inputs.values), torch.tensor(targets.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Failed to interpret file '../data/house_tiny.csv' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/pythonProjects/dive-into-deep-learning/.venv/lib/python3.10/site-packages/numpy/lib/npyio.py:441\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39;49mload(fid, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_kwargs)\n\u001b[1;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: could not find MARK",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m file \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m../data/house_tiny.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      4\u001b[0m file\n",
      "File \u001b[0;32m~/Developer/pythonProjects/dive-into-deep-learning/.venv/lib/python3.10/site-packages/numpy/lib/npyio.py:443\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(fid, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_kwargs)\n\u001b[1;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(\n\u001b[1;32m    444\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to interpret file \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m!r}\u001b[39;00m\u001b[39m as a pickle\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Failed to interpret file '../data/house_tiny.csv' as a pickle"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# can only load pick files\n",
    "file = np.load(\"../data/house_tiny.csv\", allow_pickle=True)\n",
    "file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of all (continuous) real-valued scalars by R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar arithemetic using pytorch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are rows in the dataset, vectors are denoted by bold lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are denoted by bold capitalized alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a matrix A\n",
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the transpose of a matrix A\n",
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A syemtric matrix is one whose transpose is the same as its original form\n",
    "\n",
    "S = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "S.T == S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       "\n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]],\n",
       "\n",
       "\n",
       "        [[[24, 25, 26, 27],\n",
       "          [28, 29, 30, 31],\n",
       "          [32, 33, 34, 35]],\n",
       "\n",
       "         [[36, 37, 38, 39],\n",
       "          [40, 41, 42, 43],\n",
       "          [44, 45, 46, 47]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(48).reshape(2, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peforming vector(or matrix) to scalar operations\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.75712889109438"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.sum() / x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x), A@x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "torch.mm(A, B), A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm tells us how big the vector is\n",
    "# Hypotenus is the euclidean norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is known as the L2 norm\n",
    "\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 norm is the manhattan distance\n",
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norm of a matrix\n",
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T.T == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newA = torch.tensor([[3, 4],[7, 8]])\n",
    "newB = torch.tensor([[6, 9], [4, 1]])\n",
    "\n",
    "sumAB = newA + newB\n",
    "sumTransposeAB = newA.T + newB.T\n",
    "\n",
    "sumTransposeAB == sumAB.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample function\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "for h in 10.0**np.arange(-1, -6, -1):\n",
    "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_svg_display():  #@save\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_figsize(figsize=(3.5, 2.5)):  #@save\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "\n",
    "    def has_one_axis(X):  # True if X (tensor or list) has 1 axis\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X): X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    if axes is None:\n",
    "        axes = d2l.plt.gca()\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"183.35625pt\" viewBox=\"0 0 243.529359 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-03-21T12:13:59.972744</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 243.529359 183.35625 \nL 243.529359 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 49.480398 145.8 \nL 49.480398 7.2 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m9e590694a7\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m9e590694a7\" x=\"49.480398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.702968 145.8 \nL 110.702968 7.2 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m9e590694a7\" x=\"110.702968\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 171.925539 145.8 \nL 171.925539 7.2 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m9e590694a7\" x=\"171.925539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 233.148109 145.8 \nL 233.148109 7.2 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m9e590694a7\" x=\"233.148109\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 40.603125 116.769994 \nL 235.903125 116.769994 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m1672948c05\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1672948c05\" x=\"40.603125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 120.569213) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 40.603125 78.886651 \nL 235.903125 78.886651 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m1672948c05\" x=\"40.603125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 82.685869) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 40.603125 41.003307 \nL 235.903125 41.003307 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m1672948c05\" x=\"40.603125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.802526) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 85.121094) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.480398 116.769994 \nL 55.602655 119.573361 \nL 61.724912 121.922129 \nL 67.847169 123.816296 \nL 73.969426 125.255863 \nL 80.091683 126.24083 \nL 86.21394 126.771197 \nL 92.336197 126.846963 \nL 98.458454 126.46813 \nL 104.580711 125.634696 \nL 110.702968 124.346663 \nL 116.825225 122.604029 \nL 122.947482 120.406795 \nL 129.069739 117.754961 \nL 135.191996 114.648527 \nL 141.314254 111.087492 \nL 147.436511 107.071858 \nL 153.558768 102.601624 \nL 159.681025 97.676789 \nL 165.803282 92.297354 \nL 171.925539 86.463319 \nL 178.047796 80.174684 \nL 184.170053 73.431449 \nL 190.29231 66.233614 \nL 196.414567 58.581179 \nL 202.536824 50.474143 \nL 208.659081 41.912508 \nL 214.781338 32.896272 \nL 220.903595 23.425436 \nL 227.025852 13.5 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.480398 139.5 \nL 55.602655 137.984666 \nL 61.724912 136.469333 \nL 67.847169 134.953999 \nL 73.969426 133.438665 \nL 80.091683 131.923331 \nL 86.21394 130.407998 \nL 92.336197 128.892664 \nL 98.458454 127.37733 \nL 104.580711 125.861996 \nL 110.702968 124.346663 \nL 116.825225 122.831329 \nL 122.947482 121.315995 \nL 129.069739 119.800661 \nL 135.191996 118.285328 \nL 141.314254 116.769994 \nL 147.436511 115.25466 \nL 153.558768 113.739327 \nL 159.681025 112.223993 \nL 165.803282 110.708659 \nL 171.925539 109.193325 \nL 178.047796 107.677992 \nL 184.170053 106.162658 \nL 190.29231 104.647324 \nL 196.414567 103.13199 \nL 202.536824 101.616657 \nL 208.659081 100.101323 \nL 214.781338 98.585989 \nL 220.903595 97.070655 \nL 227.025852 95.555322 \n\" clip-path=\"url(#p5c2163cb37)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 145.8 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 145.8 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298438 \nL 59.603125 20.298438 \nL 69.603125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798438) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 49.603125 34.976562 \nL 59.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5c2163cb37\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "Calculating the derivative by hand can be complex and error prone, so we use automatic differentiation which is provided by most deep learning frameworks.\n",
    "\n",
    "To calculate derivatives, automatic differentiation works backwards through this graph applying the chain rule. The computational algorithm for applying the chain rule in this fashion is called backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()  # Reset the gradient\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic control flow is very common in deep learning. For instance, when processing text, the computational graph depends on the length of the input. In these cases, automatic differentiation becomes vital for statistical modeling since it is impossible to compute the gradient a priori."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "Probability is concerned with reasoning under uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [random.random() > 0.5 for _ in range(100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads, tails:  [52, 48]\n"
     ]
    }
   ],
   "source": [
    "num_tosses = 100\n",
    "heads = sum([random.random() > 0.5 for _ in range(100)])\n",
    "tails = num_tosses - heads\n",
    "print(\"heads, tails: \", [heads, tails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58., 42.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair_probs = torch.tensor([0.5, 0.5])\n",
    "Multinomial(100, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.4500])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Multinomial(100, fair_probs).sample() / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4959, 0.5041])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Multinomial(10000, fair_probs).sample()\n",
    "counts / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"306.596693pt\" height=\"238.79625pt\" viewBox=\"0 0 306.596693 238.79625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-03-22T00:08:48.167436</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 238.79625 \nL 306.596693 238.79625 \nL 306.596693 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 201.24 \nL 294.88125 201.24 \nL 294.88125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m6c0805981b\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6c0805981b\" x=\"55.194886\" y=\"201.24\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(52.013636 215.838437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m6c0805981b\" x=\"100.853998\" y=\"201.24\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2000 -->\n      <g transform=\"translate(88.128998 215.838437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m6c0805981b\" x=\"146.513109\" y=\"201.24\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4000 -->\n      <g transform=\"translate(133.788109 215.838437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m6c0805981b\" x=\"192.17222\" y=\"201.24\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6000 -->\n      <g transform=\"translate(179.44722 215.838437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m6c0805981b\" x=\"237.831332\" y=\"201.24\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8000 -->\n      <g transform=\"translate(225.106332 215.838437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m6c0805981b\" x=\"283.490443\" y=\"201.24\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10000 -->\n      <g transform=\"translate(267.584193 215.838437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Samples -->\n     <g transform=\"translate(147.978125 229.516562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"124.755859\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"222.167969\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"285.644531\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"313.427734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"374.951172\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m7ef7792c16\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m7ef7792c16\" x=\"43.78125\" y=\"192.42\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 196.219219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m7ef7792c16\" x=\"43.78125\" y=\"157.14\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 160.939219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m7ef7792c16\" x=\"43.78125\" y=\"121.86\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 125.659219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m7ef7792c16\" x=\"43.78125\" y=\"86.58\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 90.379219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m7ef7792c16\" x=\"43.78125\" y=\"51.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 55.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m7ef7792c16\" x=\"43.78125\" y=\"16.02\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 19.819219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Estimated probability -->\n     <g transform=\"translate(14.798438 157.743437) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"154.492188\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"182.275391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"279.6875\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"340.966797\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"441.699219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"505.175781\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"536.962891\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"600.439453\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"639.302734\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"700.484375\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"763.960938\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"825.240234\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"888.716797\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"916.5\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"944.283203\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"972.066406\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"1011.275391\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 55.194886 16.02 \nL 55.217716 16.02 \nL 55.286205 121.859999 \nL 55.331864 91.619995 \nL 55.423182 80.165456 \nL 55.377523 94.419995 \nL 55.446011 89.520004 \nL 55.5145 110.1 \nL 55.605818 108.862103 \nL 55.674307 96.201815 \nL 55.765625 97.43538 \nL 55.856943 104.22 \nL 55.879773 101.374841 \nL 55.925432 96.201815 \nL 55.993921 99.319998 \nL 56.199387 110.1 \nL 56.267875 115.245 \nL 56.336364 112.867058 \nL 56.519001 105.714917 \nL 56.610319 111.22 \nL 56.678807 109.565455 \nL 56.770126 106.74 \nL 56.792955 107.946758 \nL 56.838614 107.844659 \nL 57.021251 114.019999 \nL 57.181058 110.233636 \nL 57.203887 111.157077 \nL 57.249546 109.066154 \nL 57.272376 109.972173 \nL 57.318035 107.973191 \nL 57.386524 108.766392 \nL 57.409353 109.619997 \nL 57.455012 107.748002 \nL 57.523501 108.501554 \nL 57.56916 106.74 \nL 57.637649 109.120002 \nL 57.683308 107.427275 \nL 57.706137 106.603782 \nL 57.751797 108.122656 \nL 57.774626 108.862103 \nL 57.820285 107.261381 \nL 57.843115 107.989231 \nL 57.865944 107.20983 \nL 57.911603 108.630001 \nL 57.934433 109.322481 \nL 57.980092 107.805368 \nL 58.002922 108.487743 \nL 58.117069 106.271164 \nL 58.139899 106.933846 \nL 58.185558 108.229092 \nL 58.276876 108.111175 \nL 58.299706 107.438977 \nL 58.345365 108.661728 \nL 58.368195 108.000002 \nL 58.391024 108.598721 \nL 58.436683 107.303916 \nL 58.528001 106.019999 \nL 58.550831 106.603782 \nL 58.59649 107.748002 \nL 58.664979 107.102353 \nL 58.756297 104.781782 \nL 58.801956 105.88415 \nL 58.870445 107.486665 \nL 58.938933 106.892725 \nL 59.327036 99.373841 \nL 59.395525 98.975677 \nL 59.464013 100.466809 \nL 59.486843 100.019995 \nL 59.532502 100.987538 \nL 59.578161 100.107043 \nL 59.692309 100.65636 \nL 59.737968 99.810004 \nL 59.783627 100.726932 \nL 59.806457 100.309652 \nL 59.852116 101.208296 \nL 59.920604 100.82769 \nL 59.966264 100.019995 \nL 60.011923 100.891701 \nL 60.057582 100.098505 \nL 60.217389 101.42633 \nL 60.263048 101.451385 \nL 60.445684 99.256365 \nL 60.559832 100.482717 \nL 60.582662 100.126326 \nL 60.719639 98.775559 \nL 60.628321 100.160581 \nL 60.742468 99.15934 \nL 60.856616 99.615185 \nL 60.947934 99.687986 \nL 61.062082 98.066509 \nL 61.084912 98.430807 \nL 61.130571 97.79931 \nL 61.19906 96.869996 \nL 61.267548 97.282923 \nL 61.473014 99.106958 \nL 61.609992 97.339153 \nL 61.70131 97.43538 \nL 61.724139 97.766337 \nL 61.769798 97.200628 \nL 61.792628 97.528967 \nL 61.815458 97.248867 \nL 61.861117 97.898501 \nL 61.906776 98.539324 \nL 61.975264 98.300535 \nL 62.020924 98.339995 \nL 62.157901 97.302353 \nL 62.386196 99.754173 \nL 62.409026 99.490034 \nL 62.431856 99.227546 \nL 62.500344 99.548977 \nL 62.660151 100.455369 \nL 62.682981 100.198727 \nL 62.72864 100.755951 \nL 62.751469 100.500728 \nL 62.888447 101.610529 \nL 62.911276 101.35805 \nL 62.934106 101.107054 \nL 62.979765 101.641052 \nL 63.048254 101.919133 \nL 63.071083 101.67087 \nL 63.093913 101.424038 \nL 63.162401 101.699995 \nL 63.276549 102.977751 \nL 63.345038 102.74179 \nL 63.390697 102.259995 \nL 63.436356 102.758118 \nL 63.527674 103.256063 \nL 63.550504 103.018367 \nL 63.710311 101.861715 \nL 63.596163 103.024876 \nL 63.75597 101.874258 \nL 64.189731 105.336457 \nL 64.349538 104.22 \nL 64.372368 104.43886 \nL 64.646322 106.13277 \nL 64.669152 105.916155 \nL 64.714811 106.330049 \nL 64.851788 107.132266 \nL 64.874618 106.91788 \nL 64.897448 106.704509 \nL 64.943107 107.105044 \nL 64.965936 107.303916 \nL 65.011595 106.880323 \nL 65.034425 106.670001 \nL 65.080084 107.065164 \nL 65.102914 106.855862 \nL 65.171402 107.441916 \nL 65.217061 107.026366 \nL 65.239891 106.819998 \nL 65.28555 107.206454 \nL 65.30838 107.001079 \nL 65.354039 107.384124 \nL 65.422527 107.166548 \nL 65.468186 106.762348 \nL 65.513846 107.14053 \nL 65.627993 107.686378 \nL 65.650823 107.486665 \nL 65.901948 105.721279 \nL 65.924778 105.905352 \nL 65.947607 106.088641 \nL 65.993266 105.708609 \nL 66.198732 104.767824 \nL 66.31288 105.6659 \nL 66.381369 105.477432 \nL 66.404198 105.295609 \nL 66.449857 105.648341 \nL 66.472687 105.823635 \nL 66.541176 105.63687 \nL 66.700982 105.093267 \nL 66.83796 105.42822 \nL 66.906448 104.906381 \nL 66.952108 105.245582 \nL 66.974937 105.414199 \nL 67.020596 105.06971 \nL 67.134744 104.55664 \nL 67.157574 104.724001 \nL 67.271721 105.218492 \nL 67.294551 105.050511 \nL 67.31738 104.883155 \nL 67.36304 105.211011 \nL 67.385869 105.044297 \nL 67.568506 105.681876 \nL 67.682653 105.185693 \nL 67.705483 105.344589 \nL 67.888119 106.278529 \nL 67.910949 106.116772 \nL 68.093585 104.843322 \nL 68.139244 105.15169 \nL 68.162074 105.305061 \nL 68.207733 104.99233 \nL 68.230563 105.145176 \nL 68.299051 104.680172 \nL 68.34471 104.984297 \nL 68.39037 105.286319 \nL 68.436029 104.979035 \nL 68.664324 104.070761 \nL 68.846961 104.956229 \nL 68.86979 104.807999 \nL 68.938279 104.658807 \nL 68.961108 104.804104 \nL 68.983938 104.948927 \nL 69.029597 104.655916 \nL 69.189404 104.22 \nL 69.257893 104.077048 \nL 69.303552 104.362489 \nL 69.326381 104.22 \nL 69.37204 104.503601 \nL 69.554677 105.34 \nL 69.577506 105.198446 \nL 69.645995 105.054701 \nL 69.668825 105.192285 \nL 69.691654 105.329433 \nL 69.737313 105.049465 \nL 69.89712 104.630235 \nL 70.056927 105.302206 \nL 70.079757 105.165484 \nL 70.262393 104.620304 \nL 70.467859 105.273135 \nL 70.513518 105.007502 \nL 70.559177 105.266884 \nL 70.718984 105.644672 \nL 70.833132 105.505715 \nL 70.878791 105.758373 \nL 70.92445 105.498261 \nL 71.015768 105.236713 \nL 71.038598 105.362159 \nL 71.107087 105.230888 \nL 71.152746 105.480003 \nL 71.403871 104.344052 \nL 71.4267 104.467753 \nL 71.44953 104.591106 \nL 71.495189 104.343359 \nL 71.518019 104.46637 \nL 71.540848 104.343012 \nL 71.586507 104.58801 \nL 71.609337 104.464998 \nL 71.632166 104.58699 \nL 71.677826 104.341992 \nL 71.883292 103.738037 \nL 71.997439 104.100327 \nL 72.020269 103.980979 \nL 72.225735 103.393495 \nL 72.385542 103.752094 \nL 72.522519 103.29158 \nL 72.545349 103.408699 \nL 72.613837 103.296438 \nL 72.682326 103.645028 \nL 72.773644 103.419223 \nL 72.796474 103.534512 \nL 72.819303 103.649496 \nL 72.864962 103.423356 \nL 72.887792 103.538045 \nL 73.024769 103.317698 \nL 73.161747 103.548423 \nL 73.321554 102.777738 \nL 73.367213 103.00268 \nL 73.50419 103.23146 \nL 73.549849 103.014782 \nL 73.595508 103.236359 \nL 73.663997 103.348894 \nL 73.686826 103.241207 \nL 73.732486 103.026642 \nL 73.778145 103.246012 \nL 73.869463 103.466149 \nL 73.892292 103.359513 \nL 74.211906 102.316404 \nL 74.440202 102.965975 \nL 74.463031 102.863072 \nL 74.50869 103.074545 \nL 74.53152 102.971884 \nL 74.577179 103.182348 \nL 74.645668 103.082599 \nL 74.668497 102.980653 \nL 74.714156 103.189624 \nL 74.851134 103.401433 \nL 74.942452 103.201526 \nL 74.965282 103.304429 \nL 75.010941 103.50953 \nL 75.079429 103.410823 \nL 75.193577 103.314869 \nL 75.239236 103.517616 \nL 75.284895 103.318981 \nL 75.353384 103.22226 \nL 75.376214 103.32305 \nL 75.490361 103.625398 \nL 75.513191 103.527068 \nL 75.55885 103.331082 \nL 75.604509 103.53017 \nL 75.695827 103.925674 \nL 75.764316 103.82887 \nL 75.832805 103.927619 \nL 75.878464 103.733779 \nL 75.901293 103.831456 \nL 75.946952 103.638456 \nL 75.969782 103.542282 \nL 76.015441 103.736975 \nL 76.038271 103.834001 \nL 76.08393 103.642273 \nL 76.152418 103.740129 \nL 76.266566 103.265452 \nL 76.426373 103.935789 \nL 76.472032 103.747331 \nL 76.494862 103.653408 \nL 76.540521 103.843074 \nL 76.60901 103.938207 \nL 76.631839 103.844683 \nL 76.905794 103.108233 \nL 76.974282 103.019377 \nL 77.019942 103.206205 \nL 77.042771 103.115193 \nL 77.08843 103.301254 \nL 77.11126 103.210431 \nL 77.271067 103.673301 \nL 77.293896 103.582846 \nL 77.362385 103.494075 \nL 77.385214 103.585464 \nL 77.453703 103.49705 \nL 77.522192 103.769538 \nL 77.545021 103.679998 \nL 77.59068 103.860738 \nL 77.61351 103.950824 \nL 77.659169 103.772282 \nL 77.955953 102.982724 \nL 78.092931 103.165819 \nL 78.207078 102.908798 \nL 78.229908 102.997423 \nL 78.252738 103.085879 \nL 78.298397 102.913982 \nL 78.389715 102.745659 \nL 78.412544 102.833747 \nL 78.481033 102.924212 \nL 78.503863 102.839173 \nL 78.595181 102.672627 \nL 78.61801 102.760021 \nL 78.800647 103.112176 \nL 78.823476 103.028104 \nL 78.869136 103.200348 \nL 78.891965 103.116434 \nL 78.914795 103.202304 \nL 78.960454 103.034969 \nL 78.983283 103.120671 \nL 79.211579 102.628551 \nL 79.325727 102.886161 \nL 79.348556 102.804139 \nL 79.371386 102.722265 \nL 79.417045 102.891187 \nL 79.485534 102.977751 \nL 79.508363 102.896171 \nL 79.531193 102.814748 \nL 79.576852 102.982398 \nL 79.599681 102.901123 \nL 79.64534 103.068131 \nL 79.713829 102.989306 \nL 79.827977 102.586662 \nL 79.873636 102.752714 \nL 79.964954 102.920553 \nL 79.987784 102.840603 \nL 80.124761 102.68679 \nL 80.147591 102.768811 \nL 80.19325 102.610509 \nL 80.353057 102.380835 \nL 80.375886 102.462394 \nL 80.421545 102.306079 \nL 80.467205 102.150321 \nL 80.512864 102.312976 \nL 80.581352 102.397364 \nL 80.604182 102.319821 \nL 80.741159 102.015002 \nL 80.763989 102.095646 \nL 80.832477 102.179782 \nL 80.855307 102.103196 \nL 80.946625 101.95445 \nL 80.969455 102.034517 \nL 81.037943 102.273842 \nL 81.083603 102.121848 \nL 81.22058 101.978286 \nL 81.448875 102.457537 \nL 81.654341 101.938963 \nL 81.677171 102.016895 \nL 81.700001 102.0947 \nL 81.74566 101.946807 \nL 81.768489 102.024465 \nL 81.791319 101.950707 \nL 81.836978 102.105614 \nL 82.065273 102.572805 \nL 82.202251 102.283179 \nL 82.22508 102.359239 \nL 82.316399 102.662218 \nL 82.384887 102.592151 \nL 82.704501 101.879705 \nL 82.864308 102.111344 \nL 82.978456 101.902762 \nL 83.001285 101.977014 \nL 83.024115 102.05115 \nL 83.069774 101.910343 \nL 83.25241 101.638539 \nL 83.27524 101.712286 \nL 83.320899 101.573288 \nL 83.389388 101.508351 \nL 83.412217 101.581846 \nL 83.435047 101.655215 \nL 83.480706 101.517099 \nL 83.503535 101.590342 \nL 83.526365 101.521452 \nL 83.572024 101.66759 \nL 83.640513 101.74446 \nL 83.663342 101.67577 \nL 83.686172 101.607186 \nL 83.731831 101.752377 \nL 83.754661 101.683898 \nL 83.823149 101.900796 \nL 83.868808 101.764153 \nL 83.937297 101.840003 \nL 83.982956 101.70399 \nL 84.051445 101.640242 \nL 84.142763 101.926388 \nL 84.188422 101.791206 \nL 84.256911 101.866152 \nL 84.27974 101.937176 \nL 84.325399 101.802614 \nL 84.462377 101.401453 \nL 84.508036 101.543112 \nL 84.576525 101.617826 \nL 84.599354 101.551418 \nL 84.622184 101.485115 \nL 84.667843 101.62588 \nL 84.690672 101.696105 \nL 84.736331 101.563783 \nL 84.941797 101.243928 \nL 85.033116 101.38789 \nL 85.055945 101.32267 \nL 85.078775 101.257555 \nL 85.124434 101.396522 \nL 85.147263 101.331502 \nL 85.352729 101.684749 \nL 85.421218 101.623924 \nL 85.444048 101.692393 \nL 85.740832 102.309769 \nL 85.85498 102.251247 \nL 85.991957 102.521337 \nL 86.014787 102.457305 \nL 86.220253 102.144706 \nL 86.243082 102.21104 \nL 86.288741 102.084564 \nL 86.425719 101.707365 \nL 86.471378 101.839698 \nL 86.745332 102.370542 \nL 86.88231 102.251531 \nL 86.905139 102.316404 \nL 86.950798 102.192409 \nL 86.996457 102.068783 \nL 87.042117 102.198224 \nL 87.110605 102.391696 \nL 87.156264 102.268396 \nL 87.338901 101.903887 \nL 87.36173 101.968087 \nL 87.430219 102.16013 \nL 87.475878 102.038375 \nL 87.498708 101.977624 \nL 87.544367 102.105193 \nL 87.567196 102.044526 \nL 87.590026 102.108169 \nL 87.635685 101.987087 \nL 87.658515 102.050635 \nL 87.704174 101.9299 \nL 87.772662 101.996465 \nL 87.90964 102.128787 \nL 88.069447 101.955323 \nL 88.206424 102.086625 \nL 88.274913 101.908556 \nL 88.320572 102.033223 \nL 88.38906 102.098349 \nL 88.41189 102.039227 \nL 88.617356 101.751599 \nL 88.799992 102.004519 \nL 88.91414 101.952337 \nL 88.982629 102.016485 \nL 89.005458 101.958456 \nL 89.188095 101.73383 \nL 89.370731 101.982618 \nL 89.41639 101.868002 \nL 89.462049 101.98858 \nL 89.713175 102.412862 \nL 89.987129 101.964397 \nL 90.055618 102.141993 \nL 90.101277 102.029407 \nL 90.124107 101.973229 \nL 90.169766 102.09123 \nL 90.192595 102.035126 \nL 90.238254 102.152812 \nL 90.283913 102.040804 \nL 90.557868 101.602454 \nL 90.626357 101.550713 \nL 90.672016 101.66759 \nL 90.740505 101.615891 \nL 90.763334 101.67414 \nL 90.831823 101.622557 \nL 90.877482 101.738666 \nL 90.945971 101.687136 \nL 90.9688 101.744996 \nL 91.128607 101.924001 \nL 91.242755 101.763795 \nL 91.265584 101.82114 \nL 91.448221 102.05524 \nL 91.585198 101.952789 \nL 91.699346 102.015002 \nL 91.813494 101.96692 \nL 91.973301 102.140847 \nL 92.201596 101.827396 \nL 92.338573 101.944567 \nL 92.475551 101.629055 \nL 92.544039 101.687683 \nL 92.589699 101.798398 \nL 92.658187 101.749107 \nL 92.863653 101.495472 \nL 92.932142 101.553742 \nL 92.954971 101.502053 \nL 92.977801 101.450439 \nL 93.02346 101.560166 \nL 93.206097 101.784708 \nL 93.365903 101.636741 \nL 93.411563 101.745133 \nL 93.480051 101.696998 \nL 93.662688 101.499719 \nL 93.913813 101.881166 \nL 93.959472 101.780093 \nL 94.027961 101.836218 \nL 94.096449 101.892143 \nL 94.119279 101.841801 \nL 94.187767 101.794371 \nL 94.210597 101.847373 \nL 94.256256 101.953178 \nL 94.324745 101.905717 \nL 94.393233 101.858413 \nL 94.416063 101.9111 \nL 94.57587 102.073766 \nL 94.598699 102.023939 \nL 94.644359 102.128504 \nL 94.667188 102.078729 \nL 94.690018 102.130922 \nL 94.735677 102.031541 \nL 94.781336 101.932392 \nL 94.826995 102.036577 \nL 94.895484 102.192409 \nL 94.941143 102.093481 \nL 94.963972 102.044095 \nL 95.009631 102.147682 \nL 95.07812 102.201693 \nL 95.10095 102.152423 \nL 95.215097 102.007453 \nL 95.237927 102.058973 \nL 95.420563 102.26889 \nL 95.626029 102.029932 \nL 95.740177 102.085732 \nL 95.763007 102.037324 \nL 95.808666 102.138881 \nL 95.877155 102.092913 \nL 95.922814 102.194113 \nL 96.173939 101.764542 \nL 96.196768 101.814989 \nL 96.425064 102.218779 \nL 96.447893 102.171107 \nL 96.539212 102.078277 \nL 96.562041 102.128104 \nL 96.676189 102.37644 \nL 96.721848 102.281539 \nL 96.790337 102.236348 \nL 96.813166 102.285786 \nL 97.018632 102.535879 \nL 97.041462 102.488701 \nL 97.087121 102.586662 \nL 97.109951 102.539548 \nL 97.13278 102.588439 \nL 97.178439 102.494347 \nL 97.201269 102.543196 \nL 97.224098 102.496219 \nL 97.269757 102.593749 \nL 97.315417 102.691069 \nL 97.383905 102.645847 \nL 97.406735 102.599027 \nL 97.452394 102.696021 \nL 97.475223 102.649254 \nL 97.543712 102.794351 \nL 97.612201 102.749213 \nL 97.794837 102.566549 \nL 97.840496 102.662702 \nL 97.886155 102.570081 \nL 98.068792 102.389341 \nL 98.114451 102.485074 \nL 98.16011 102.393232 \nL 98.205769 102.301589 \nL 98.274258 102.351353 \nL 98.479724 102.592698 \nL 98.548213 102.456002 \nL 98.593872 102.550598 \nL 98.639531 102.644995 \nL 98.68519 102.5541 \nL 98.776508 102.372876 \nL 98.844997 102.421883 \nL 98.913485 102.562795 \nL 98.981974 102.519424 \nL 99.004804 102.47437 \nL 99.050463 102.567968 \nL 99.141781 102.662996 \nL 99.164611 102.618026 \nL 99.370077 102.39769 \nL 99.415736 102.490583 \nL 99.484224 102.447822 \nL 99.552713 102.405186 \nL 99.575543 102.45147 \nL 99.598372 102.497701 \nL 99.644031 102.408908 \nL 99.73535 102.23189 \nL 99.803838 102.280056 \nL 99.986475 102.467683 \nL 100.351748 101.947038 \nL 100.374577 101.992722 \nL 100.420236 101.905969 \nL 100.534384 101.867403 \nL 100.671361 102.051508 \nL 100.694191 102.008368 \nL 100.785509 101.924506 \nL 100.808339 101.96977 \nL 100.922486 102.107422 \nL 100.945316 102.064493 \nL 101.150782 101.855154 \nL 101.173612 101.900102 \nL 101.219271 101.814947 \nL 101.310589 101.645131 \nL 101.379078 101.69253 \nL 101.516055 101.786895 \nL 101.607373 101.704958 \nL 101.630203 101.749538 \nL 101.858498 102.020396 \nL 102.041135 101.85712 \nL 102.063964 101.901206 \nL 102.109623 101.81767 \nL 102.178112 101.692656 \nL 102.223771 101.780702 \nL 102.29226 101.741516 \nL 102.383578 101.916904 \nL 102.406408 101.875394 \nL 102.452067 101.96283 \nL 102.748851 102.357809 \nL 102.908658 102.153138 \nL 102.931487 102.196289 \nL 102.999976 102.157092 \nL 103.068465 102.286154 \nL 103.182612 102.081053 \nL 103.228272 102.166891 \nL 103.502226 102.511832 \nL 103.707692 102.31163 \nL 103.753351 102.396312 \nL 103.82184 102.357494 \nL 103.890329 102.31878 \nL 103.913158 102.360984 \nL 103.935988 102.403147 \nL 103.981647 102.322334 \nL 104.004476 102.364465 \nL 104.072965 102.24353 \nL 104.118624 102.327654 \nL 104.187113 102.289214 \nL 104.232772 102.373097 \nL 104.255602 102.332932 \nL 104.301261 102.416657 \nL 104.32409 102.458462 \nL 104.369749 102.378238 \nL 104.461068 102.299949 \nL 104.483897 102.34167 \nL 104.689363 102.552775 \nL 104.872 102.396848 \nL 104.963318 102.561954 \nL 105.031806 102.52385 \nL 105.191613 102.408498 \nL 105.214443 102.449567 \nL 105.260102 102.370773 \nL 105.282932 102.331439 \nL 105.328591 102.41345 \nL 105.465568 102.578514 \nL 105.488398 102.539233 \nL 105.671034 102.385819 \nL 105.830841 102.590342 \nL 105.85367 102.551356 \nL 105.8765 102.51239 \nL 105.922159 102.593276 \nL 105.990648 102.635091 \nL 106.013477 102.596199 \nL 106.218943 102.405512 \nL 106.355921 102.489048 \nL 106.424409 102.530642 \nL 106.470068 102.453646 \nL 106.492898 102.493664 \nL 106.538557 102.416805 \nL 106.652705 102.225235 \nL 106.698364 102.305154 \nL 106.881 102.467683 \nL 106.972319 102.315289 \nL 107.040807 102.356621 \nL 107.200614 102.47845 \nL 107.269103 102.442081 \nL 107.291932 102.481499 \nL 107.474569 102.641568 \nL 107.588717 102.529759 \nL 107.611546 102.568893 \nL 107.771353 102.688745 \nL 107.93116 102.578892 \nL 108.113796 102.73669 \nL 108.227944 102.70193 \nL 108.319262 102.704538 \nL 108.387751 102.592971 \nL 108.43341 102.669977 \nL 108.547558 102.711015 \nL 108.707365 102.452238 \nL 108.753024 102.528908 \nL 109.072638 102.912499 \nL 109.118297 102.838963 \nL 109.163956 102.914718 \nL 109.323763 103.030122 \nL 109.346592 102.993449 \nL 109.392252 103.068762 \nL 109.529229 103.145748 \nL 109.643377 103.11103 \nL 109.666206 103.14845 \nL 109.711865 103.075502 \nL 109.826013 102.967373 \nL 109.848843 103.00472 \nL 109.96299 103.117496 \nL 109.98582 103.081222 \nL 110.077138 103.009767 \nL 110.099968 103.046935 \nL 110.191286 103.19527 \nL 110.236945 103.122984 \nL 110.373922 103.052759 \nL 110.5109 103.201189 \nL 110.533729 103.165241 \nL 110.556559 103.129314 \nL 110.602218 103.202872 \nL 110.784854 103.351039 \nL 110.899002 103.244413 \nL 110.921832 103.280929 \nL 111.172957 103.536836 \nL 111.264275 103.466149 \nL 111.287105 103.502339 \nL 111.355593 103.610741 \nL 111.401252 103.539612 \nL 111.53823 103.398374 \nL 111.561059 103.434417 \nL 111.698037 103.507564 \nL 111.789355 103.437581 \nL 111.812184 103.473446 \nL 112.200287 103.937534 \nL 112.360094 103.832697 \nL 112.382923 103.868046 \nL 112.428582 103.797989 \nL 112.451412 103.763008 \nL 112.497071 103.833622 \nL 112.611219 103.869444 \nL 112.656878 103.799671 \nL 112.702537 103.870001 \nL 112.953662 104.115457 \nL 113.273276 103.769475 \nL 113.296106 103.804287 \nL 113.341765 103.735387 \nL 113.433083 103.736144 \nL 113.455912 103.770883 \nL 113.501572 103.702194 \nL 113.57006 103.668317 \nL 113.59289 103.703003 \nL 113.821185 103.911007 \nL 113.980992 103.740655 \nL 114.003822 103.775068 \nL 114.186458 103.912921 \nL 114.323436 103.845545 \nL 114.528902 104.016465 \nL 114.62022 103.881294 \nL 114.665879 103.949237 \nL 114.780027 103.983534 \nL 114.917004 103.916674 \nL 115.008322 103.984439 \nL 115.031152 103.950888 \nL 115.09964 103.917599 \nL 115.12247 103.951298 \nL 115.305106 104.153035 \nL 115.327936 104.119578 \nL 115.442084 104.086364 \nL 115.579061 104.153329 \nL 115.601891 104.120041 \nL 115.64755 104.186701 \nL 115.693209 104.253272 \nL 115.738868 104.186754 \nL 115.830186 104.120409 \nL 115.853016 104.153634 \nL 116.035652 104.286166 \nL 116.104141 104.253046 \nL 116.12697 104.286066 \nL 116.218289 104.285966 \nL 116.332436 104.25292 \nL 116.469414 104.318545 \nL 116.560732 104.187196 \nL 116.606391 104.252778 \nL 116.629221 104.285525 \nL 116.67488 104.22 \nL 116.766198 104.22 \nL 116.926005 104.383029 \nL 116.948834 104.350377 \nL 117.108641 104.25251 \nL 117.17713 104.284947 \nL 117.19996 104.252463 \nL 117.314107 104.22 \nL 117.542403 104.478272 \nL 117.565232 104.445904 \nL 117.588062 104.413562 \nL 117.633721 104.477894 \nL 117.747869 104.573957 \nL 117.770698 104.541663 \nL 118.021824 104.316111 \nL 118.158801 104.379843 \nL 118.295778 104.315696 \nL 118.318608 104.347548 \nL 118.364267 104.283727 \nL 118.387096 104.251853 \nL 118.432756 104.315491 \nL 118.524074 104.442487 \nL 118.569733 104.378802 \nL 118.683881 104.346818 \nL 118.70671 104.37846 \nL 118.889347 104.314807 \nL 119.003494 104.409272 \nL 119.026324 104.377667 \nL 119.117642 104.314465 \nL 119.163301 104.37733 \nL 119.368767 104.533656 \nL 119.551404 104.407659 \nL 119.574233 104.43886 \nL 119.688381 104.5321 \nL 119.73404 104.469503 \nL 119.939506 104.375448 \nL 120.007995 104.406339 \nL 120.053654 104.344137 \nL 120.190631 104.28194 \nL 120.213461 104.312872 \nL 120.350438 104.312678 \nL 120.487416 104.250828 \nL 120.510245 104.281635 \nL 120.601563 104.28155 \nL 120.624393 104.250765 \nL 120.692882 104.22 \nL 120.738541 104.281419 \nL 120.898348 104.311905 \nL 120.966836 104.281209 \nL 121.012495 104.342328 \nL 121.172302 104.372541 \nL 121.354939 104.311274 \nL 121.537575 104.371705 \nL 121.834359 104.159585 \nL 122.039825 104.370564 \nL 122.085485 104.310275 \nL 122.245291 104.280042 \nL 122.382269 104.279921 \nL 122.519246 104.279795 \nL 122.701883 104.339269 \nL 122.724712 104.309424 \nL 122.793201 104.398664 \nL 122.930178 104.398301 \nL 122.953008 104.368535 \nL 123.021496 104.457417 \nL 123.386769 104.751328 \nL 123.546576 104.720636 \nL 123.660724 104.749198 \nL 123.683553 104.719632 \nL 123.820531 104.60131 \nL 123.86619 104.65968 \nL 123.980338 104.688216 \nL 124.003167 104.658807 \nL 124.208633 104.511666 \nL 124.231463 104.540727 \nL 124.36844 104.656489 \nL 124.414099 104.59804 \nL 124.551077 104.53925 \nL 124.573906 104.568159 \nL 124.710883 104.625383 \nL 124.733713 104.596305 \nL 124.87069 104.537788 \nL 124.89352 104.566561 \nL 125.007668 104.594828 \nL 125.030497 104.565882 \nL 125.144645 104.536542 \nL 125.167475 104.565204 \nL 125.281622 104.535922 \nL 125.39577 104.44939 \nL 125.441429 104.50655 \nL 125.578407 104.563191 \nL 125.601236 104.534492 \nL 125.806702 104.391041 \nL 125.829532 104.419482 \nL 125.966509 104.47598 \nL 125.989339 104.447465 \nL 126.263293 104.276646 \nL 126.308952 104.333223 \nL 126.377441 104.248278 \nL 126.400271 104.22 \nL 126.468759 104.304724 \nL 126.491589 104.332934 \nL 126.560077 104.248205 \nL 126.651396 104.248168 \nL 126.674225 104.27632 \nL 126.788373 104.304351 \nL 126.811203 104.276214 \nL 126.92535 104.191937 \nL 126.971009 104.248047 \nL 127.153646 104.359866 \nL 127.176475 104.331856 \nL 127.678726 103.886751 \nL 127.929851 104.081622 \nL 127.95268 104.054001 \nL 128.089658 103.943853 \nL 128.135317 103.999222 \nL 128.226635 103.999495 \nL 128.249465 103.972011 \nL 128.386442 103.972484 \nL 128.409271 104.000052 \nL 128.47776 103.917852 \nL 128.660397 103.808998 \nL 128.683226 103.836524 \nL 128.865863 103.892123 \nL 128.98001 103.810785 \nL 129.025669 103.865564 \nL 129.048499 103.892933 \nL 129.116988 103.811542 \nL 129.322454 103.731203 \nL 129.50509 103.786581 \nL 129.687727 103.625513 \nL 129.733386 103.679893 \nL 129.984511 103.816273 \nL 130.121488 103.763282 \nL 130.144318 103.790282 \nL 130.167147 103.817262 \nL 130.235636 103.737154 \nL 130.395443 103.711415 \nL 130.418272 103.738321 \nL 130.486761 103.65856 \nL 130.669397 103.606577 \nL 130.829204 103.687716 \nL 130.852034 103.661262 \nL 131.011841 103.635891 \nL 131.171648 103.716609 \nL 131.194477 103.690271 \nL 131.399943 103.612455 \nL 131.58258 103.666614 \nL 131.742387 103.641463 \nL 131.947853 103.774143 \nL 131.970682 103.748057 \nL 132.107659 103.748898 \nL 132.244637 103.749739 \nL 132.495762 103.621066 \nL 132.60991 103.647951 \nL 132.632739 103.622117 \nL 132.883864 103.494506 \nL 133.043671 103.521843 \nL 133.180649 103.471448 \nL 133.203478 103.497471 \nL 133.386115 103.550652 \nL 133.61441 103.449915 \nL 133.842706 103.656909 \nL 133.888365 103.606083 \nL 133.911194 103.58068 \nL 133.979683 103.657887 \nL 134.185149 103.78677 \nL 134.207979 103.761421 \nL 134.550422 103.535101 \nL 134.961354 103.891934 \nL 134.984183 103.866794 \nL 135.16682 103.817262 \nL 135.303797 103.817945 \nL 135.463604 103.793668 \nL 135.646241 103.844683 \nL 135.737559 103.845104 \nL 135.760388 103.870201 \nL 135.874536 103.895646 \nL 135.897366 103.87079 \nL 136.057173 103.846586 \nL 136.262639 104.071013 \nL 136.331127 103.996709 \nL 136.445275 103.922699 \nL 136.490934 103.972389 \nL 136.673571 104.121176 \nL 136.71923 104.071844 \nL 136.810548 104.022689 \nL 136.856207 104.072096 \nL 137.267139 104.367163 \nL 137.381287 104.391451 \nL 137.495435 104.366758 \nL 137.632412 104.366511 \nL 137.792219 104.439343 \nL 137.815048 104.414919 \nL 137.952026 104.365948 \nL 137.974855 104.390226 \nL 138.317299 104.607482 \nL 138.477105 104.534224 \nL 138.499935 104.558302 \nL 138.728231 104.653771 \nL 138.956526 104.556456 \nL 139.116333 104.675751 \nL 139.161992 104.627559 \nL 139.298969 104.579025 \nL 139.321799 104.602856 \nL 139.458776 104.60223 \nL 139.687072 104.458248 \nL 139.709902 104.482005 \nL 139.892538 104.576507 \nL 139.915368 104.55265 \nL 140.052345 104.552114 \nL 140.075174 104.575739 \nL 140.143663 104.504363 \nL 140.28064 104.456587 \nL 140.30347 104.480175 \nL 140.531766 104.573836 \nL 140.760061 104.478787 \nL 141.034016 104.665573 \nL 141.056845 104.642011 \nL 141.216652 104.571024 \nL 141.239482 104.594323 \nL 141.444948 104.710127 \nL 141.467777 104.686665 \nL 141.696073 104.592347 \nL 141.810221 104.6151 \nL 141.83305 104.591758 \nL 141.924368 104.544948 \nL 141.970028 104.591174 \nL 142.129834 104.706269 \nL 142.175494 104.659727 \nL 142.3353 104.635818 \nL 142.449448 104.612203 \nL 142.609255 104.588462 \nL 142.814721 104.656521 \nL 142.906039 104.656063 \nL 142.928869 104.678895 \nL 143.134335 104.838065 \nL 143.179994 104.791986 \nL 143.38546 104.722172 \nL 143.636585 104.834553 \nL 143.750733 104.856496 \nL 143.864881 104.832971 \nL 144.275813 104.558969 \nL 144.458449 104.603381 \nL 144.549767 104.648046 \nL 144.663915 104.624999 \nL 144.869381 104.556724 \nL 144.983529 104.533877 \nL 145.280313 104.376421 \nL 145.371631 104.331619 \nL 145.531438 104.309135 \nL 145.828222 104.46432 \nL 145.988029 104.441719 \nL 146.170666 104.485527 \nL 146.284814 104.507297 \nL 146.421791 104.506866 \nL 146.581598 104.484334 \nL 146.764234 104.527779 \nL 146.946871 104.483282 \nL 147.152337 104.548371 \nL 147.334973 104.504022 \nL 147.540439 104.56879 \nL 147.677416 104.568274 \nL 147.928542 104.675872 \nL 148.088348 104.653414 \nL 148.202496 104.631239 \nL 148.316644 104.652352 \nL 148.567769 104.758987 \nL 148.681917 104.779863 \nL 148.887383 104.843096 \nL 149.115678 104.755843 \nL 149.252656 104.755065 \nL 149.480951 104.668365 \nL 149.57227 104.625272 \nL 149.754906 104.581911 \nL 149.914713 104.602556 \nL 150.097349 104.559395 \nL 150.257156 104.579998 \nL 150.394134 104.579483 \nL 150.5996 104.642011 \nL 150.782236 104.599081 \nL 151.010532 104.682223 \nL 151.215998 104.618338 \nL 151.307316 104.576065 \nL 151.672589 104.366059 \nL 151.832396 104.386651 \nL 152.12918 104.240766 \nL 152.243328 104.22 \nL 152.357475 104.240718 \nL 152.517282 104.261368 \nL 152.699919 104.22 \nL 152.859726 104.240613 \nL 153.019532 104.22 \nL 153.270658 104.32263 \nL 153.453294 104.281461 \nL 153.63593 104.322251 \nL 153.887056 104.22 \nL 153.978374 104.179247 \nL 154.115351 104.179299 \nL 154.275158 104.199686 \nL 154.480624 104.138893 \nL 154.617601 104.139009 \nL 154.777408 104.118926 \nL 154.937215 104.139272 \nL 155.21117 104.018726 \nL 155.348147 104.018999 \nL 155.485124 104.019272 \nL 155.622102 104.019546 \nL 155.964545 103.840435 \nL 156.170011 103.901008 \nL 156.352648 103.86179 \nL 156.489625 103.862273 \nL 156.580943 103.902301 \nL 156.786409 103.96239 \nL 156.946216 103.943012 \nL 157.128852 103.983009 \nL 157.311489 103.944001 \nL 157.402807 103.904856 \nL 157.562614 103.885689 \nL 157.79091 103.964914 \nL 157.927887 103.965261 \nL 158.042035 103.945967 \nL 158.179012 103.946335 \nL 158.29316 103.927104 \nL 158.704092 103.694886 \nL 158.886728 103.734641 \nL 159.092194 103.677464 \nL 159.32049 103.755995 \nL 159.480297 103.737395 \nL 159.617274 103.738037 \nL 159.731422 103.757814 \nL 159.914058 103.797074 \nL 160.051036 103.797621 \nL 160.233672 103.836692 \nL 160.370649 103.837186 \nL 160.667434 103.971874 \nL 160.804411 103.972189 \nL 160.941388 103.972515 \nL 161.124025 103.934927 \nL 161.329491 103.992387 \nL 161.489298 103.973788 \nL 161.580616 103.936157 \nL 161.740423 103.917684 \nL 161.831741 103.88019 \nL 161.991548 103.861842 \nL 162.265503 103.975575 \nL 162.448139 103.938449 \nL 162.676435 104.013973 \nL 162.859071 103.976921 \nL 162.996048 103.977226 \nL 163.247173 103.884637 \nL 163.40698 103.903742 \nL 163.658105 103.811668 \nL 163.749424 103.774921 \nL 163.93206 103.738636 \nL 164.206015 103.850655 \nL 164.388651 103.814402 \nL 164.502799 103.796402 \nL 164.753924 103.705495 \nL 164.95939 103.761484 \nL 165.233345 103.652861 \nL 165.415981 103.690323 \nL 165.530129 103.709123 \nL 165.735595 103.764701 \nL 165.895402 103.747174 \nL 166.192186 103.875395 \nL 166.351993 103.857784 \nL 166.5118 103.876383 \nL 166.648777 103.876815 \nL 166.854243 103.931531 \nL 167.173857 103.788526 \nL 167.402153 103.861169 \nL 167.53913 103.861611 \nL 167.721766 103.897969 \nL 167.835914 103.916169 \nL 167.972891 103.916537 \nL 168.109869 103.916905 \nL 168.292505 103.953001 \nL 168.56646 103.847101 \nL 168.726267 103.865354 \nL 168.840415 103.883428 \nL 168.954562 103.866069 \nL 169.045881 103.901712 \nL 169.319835 104.008316 \nL 169.548131 103.938323 \nL 169.707938 103.956292 \nL 169.890574 103.921616 \nL 170.027551 103.921963 \nL 170.187358 103.904877 \nL 170.324336 103.870275 \nL 170.438483 103.888096 \nL 170.689609 103.941109 \nL 170.940734 103.889537 \nL 171.237518 103.977121 \nL 171.465813 103.94297 \nL 171.579961 103.891345 \nL 171.762598 103.89186 \nL 171.968064 103.909682 \nL 172.196359 103.875868 \nL 172.356166 103.859161 \nL 172.607291 103.808493 \nL 172.835587 103.843516 \nL 173.17803 103.725167 \nL 173.497644 103.828607 \nL 173.68028 103.829206 \nL 173.885746 103.846849 \nL 174.068383 103.847417 \nL 174.296678 103.881935 \nL 174.661951 103.748162 \nL 174.913076 103.799598 \nL 175.209861 103.716767 \nL 175.438156 103.751211 \nL 175.689281 103.702057 \nL 175.917577 103.736397 \nL 176.123043 103.720562 \nL 176.237191 103.671145 \nL 176.442657 103.655468 \nL 176.648123 103.672996 \nL 176.899248 103.624504 \nL 177.036225 103.59212 \nL 177.333009 103.511234 \nL 177.7896 103.710931 \nL 178.017896 103.679094 \nL 178.223362 103.696369 \nL 178.33751 103.745891 \nL 178.542976 103.763008 \nL 178.702783 103.779894 \nL 178.999567 103.862252 \nL 179.250692 103.814297 \nL 179.501817 103.863703 \nL 179.684454 103.864229 \nL 179.86709 103.864744 \nL 179.981238 103.913467 \nL 180.163874 103.913919 \nL 180.300852 103.94644 \nL 180.506318 103.962947 \nL 180.825931 103.867457 \nL 181.122716 103.948217 \nL 181.305352 103.948616 \nL 181.533648 103.980979 \nL 181.853261 103.886214 \nL 182.127216 103.950372 \nL 182.332682 103.934969 \nL 182.538148 103.951245 \nL 182.720784 103.951624 \nL 182.903421 103.952013 \nL 183.063228 103.968089 \nL 183.337182 104.031469 \nL 183.542648 104.016086 \nL 183.816603 104.07913 \nL 184.090558 104.016959 \nL 184.341683 104.064116 \nL 184.569978 104.033267 \nL 184.752615 104.03353 \nL 184.958081 104.018315 \nL 185.140717 104.018599 \nL 185.483161 103.910965 \nL 185.734286 103.957816 \nL 186.03107 103.881483 \nL 186.168047 103.851086 \nL 186.327854 103.866889 \nL 186.464832 103.836587 \nL 186.647468 103.837123 \nL 186.830104 103.837649 \nL 187.149718 103.747037 \nL 187.469332 103.839499 \nL 187.720457 103.794646 \nL 187.857434 103.764733 \nL 188.131389 103.705096 \nL 188.268366 103.675372 \nL 188.496662 103.6461 \nL 188.656469 103.631696 \nL 188.861935 103.617543 \nL 189.09023 103.648645 \nL 189.364185 103.589786 \nL 189.523992 103.57555 \nL 189.752288 103.546719 \nL 189.934924 103.547634 \nL 190.071901 103.578168 \nL 190.277367 103.594044 \nL 190.414345 103.624451 \nL 190.688299 103.685098 \nL 190.825277 103.715326 \nL 191.007913 103.715999 \nL 191.16772 103.731402 \nL 191.327527 103.717187 \nL 191.647141 103.629835 \nL 191.829777 103.630623 \nL 192.17222 103.529213 \nL 192.400516 103.559715 \nL 192.628812 103.531505 \nL 192.902766 103.591352 \nL 193.016914 103.635702 \nL 193.176721 103.621791 \nL 193.382187 103.608112 \nL 193.587653 103.623558 \nL 193.793119 103.609921 \nL 193.907267 103.566886 \nL 194.021414 103.610919 \nL 194.341028 103.699134 \nL 194.706301 103.585054 \nL 194.980256 103.643903 \nL 195.25421 103.587536 \nL 195.391188 103.559431 \nL 195.665142 103.50339 \nL 195.939097 103.562007 \nL 196.076074 103.591226 \nL 196.28154 103.606409 \nL 196.509836 103.578904 \nL 196.760961 103.622706 \nL 197.034916 103.567086 \nL 197.240382 103.582205 \nL 197.445848 103.568978 \nL 197.605655 103.555562 \nL 197.811121 103.542409 \nL 198.016587 103.557475 \nL 198.267712 103.516428 \nL 198.427519 103.503159 \nL 198.655814 103.476232 \nL 198.906939 103.519551 \nL 199.226553 103.437245 \nL 199.500508 103.494537 \nL 199.865781 103.385031 \nL 200.025588 103.372057 \nL 200.162565 103.400634 \nL 200.390861 103.429654 \nL 200.641986 103.389489 \nL 200.893111 103.432377 \nL 201.144236 103.392349 \nL 201.372531 103.42119 \nL 201.577997 103.408552 \nL 201.760634 103.409561 \nL 201.9661 103.396986 \nL 202.125907 103.384179 \nL 202.422691 103.317488 \nL 202.742305 103.401307 \nL 203.016259 103.348347 \nL 203.267385 103.390614 \nL 203.609828 103.297574 \nL 203.952271 103.394431 \nL 204.157737 103.382056 \nL 204.317544 103.36946 \nL 204.52301 103.357147 \nL 204.705647 103.358199 \nL 204.911113 103.345929 \nL 205.162238 103.38767 \nL 205.367704 103.3754 \nL 205.57317 103.389941 \nL 205.824295 103.351228 \nL 206.05259 103.379238 \nL 206.258056 103.367052 \nL 206.57767 103.448653 \nL 206.851625 103.396944 \nL 206.988602 103.371163 \nL 207.148409 103.385304 \nL 207.399534 103.426363 \nL 207.62783 103.401128 \nL 207.833296 103.415428 \nL 208.13008 103.351165 \nL 208.267057 103.325636 \nL 208.65516 103.209822 \nL 208.951944 103.277249 \nL 209.15741 103.265431 \nL 209.499853 103.358872 \nL 209.728149 103.33409 \nL 209.887956 103.321988 \nL 210.139081 103.284462 \nL 210.321717 103.285566 \nL 210.481524 103.299498 \nL 210.823967 103.392076 \nL 211.143581 103.31631 \nL 211.349047 103.330389 \nL 211.531684 103.331429 \nL 211.782809 103.371426 \nL 211.896957 103.410591 \nL 212.170911 103.463299 \nL 212.330718 103.476884 \nL 212.467695 103.451933 \nL 212.627502 103.465497 \nL 212.832968 103.479249 \nL 213.152582 103.404272 \nL 213.403707 103.443753 \nL 213.76898 103.343962 \nL 214.020105 103.358031 \nL 214.225571 103.371804 \nL 214.431037 103.360249 \nL 214.682162 103.348978 \nL 214.956117 103.375684 \nL 215.161583 103.389352 \nL 215.618174 103.517216 \nL 215.892129 103.493349 \nL 216.234572 103.557402 \nL 216.554186 103.508815 \nL 216.828141 103.534922 \nL 217.102095 103.511213 \nL 217.330391 103.512212 \nL 217.672834 103.451755 \nL 217.992448 103.502728 \nL 218.175084 103.528235 \nL 218.517528 103.591321 \nL 218.791482 103.567758 \nL 218.951289 103.531516 \nL 219.133926 103.556845 \nL 219.476369 103.619499 \nL 219.795983 103.571743 \nL 220.047108 103.584939 \nL 220.252574 103.597924 \nL 220.48087 103.598786 \nL 220.709165 103.599648 \nL 221.074438 103.673837 \nL 221.325563 103.662534 \nL 221.576688 103.675477 \nL 221.941961 103.604232 \nL 222.170257 103.605074 \nL 222.421382 103.59396 \nL 222.786655 103.667402 \nL 223.01495 103.668149 \nL 223.197587 103.644786 \nL 223.494371 103.60991 \nL 223.745496 103.622759 \nL 223.950962 103.635418 \nL 224.293405 103.696138 \nL 224.56736 103.673206 \nL 224.955462 103.757478 \nL 225.206588 103.746312 \nL 225.480542 103.770726 \nL 225.686008 103.783069 \nL 226.09694 103.878371 \nL 226.325236 103.878823 \nL 226.462213 103.832076 \nL 226.667679 103.844283 \nL 227.055782 103.927136 \nL 227.421054 103.857616 \nL 227.67218 103.869812 \nL 227.900475 103.870275 \nL 228.128771 103.870737 \nL 228.379896 103.859624 \nL 228.69951 103.906696 \nL 228.859316 103.941772 \nL 229.110442 103.953747 \nL 229.338737 103.954094 \nL 229.70401 104.023867 \nL 229.909476 104.035622 \nL 230.092112 104.012795 \nL 230.251919 104.047482 \nL 230.525874 104.070719 \nL 230.73134 104.082368 \nL 231.005295 104.105489 \nL 231.210761 104.117055 \nL 231.393397 104.094313 \nL 231.690181 104.060299 \nL 232.009795 104.106131 \nL 232.306579 104.072222 \nL 232.580534 104.095154 \nL 232.877318 104.061371 \nL 233.31108 104.163486 \nL 233.653523 104.107182 \nL 233.904648 104.118611 \nL 234.201432 104.085039 \nL 234.406898 104.073957 \nL 234.589535 104.096552 \nL 234.749342 104.063022 \nL 235.183103 103.962727 \nL 235.548376 104.030228 \nL 235.753842 104.041594 \nL 235.959308 104.030659 \nL 236.37024 103.942192 \nL 236.689854 103.987046 \nL 236.849661 104.020502 \nL 237.123616 104.04294 \nL 237.374741 104.032131 \nL 237.717184 104.087636 \nL 237.991139 104.065808 \nL 238.150946 104.03293 \nL 238.379241 104.033162 \nL 238.630366 104.044391 \nL 238.92715 104.011797 \nL 239.109787 103.990116 \nL 239.360912 103.979497 \nL 239.680526 104.023562 \nL 239.908821 104.023804 \nL 240.228435 104.067669 \nL 240.685026 103.959499 \nL 240.958981 103.981568 \nL 241.187277 103.981852 \nL 241.552549 104.047146 \nL 241.780845 104.047356 \nL 242.123288 104.101525 \nL 242.397243 104.080192 \nL 242.739686 104.13412 \nL 242.967982 104.134225 \nL 243.196277 104.13433 \nL 243.470232 104.11307 \nL 243.698528 104.113196 \nL 243.995312 104.081369 \nL 244.200778 104.070866 \nL 244.58888 103.996761 \nL 244.976983 104.071476 \nL 245.159619 104.09282 \nL 245.433574 104.114163 \nL 245.707529 104.093188 \nL 245.981483 104.114468 \nL 246.301097 104.072506 \nL 246.552222 104.08322 \nL 246.734859 104.104375 \nL 246.917495 104.083483 \nL 247.077302 104.115078 \nL 247.237109 104.083714 \nL 247.442575 104.094334 \nL 247.762189 104.136359 \nL 247.990484 104.136454 \nL 248.264439 104.15743 \nL 248.538393 104.136696 \nL 248.789519 104.147199 \nL 249.086303 104.116161 \nL 249.383087 104.14742 \nL 249.542894 104.178563 \nL 249.74836 104.168259 \nL 249.885337 104.209654 \nL 250.159292 104.230325 \nL 250.75286 104.065567 \nL 250.958326 104.055452 \nL 251.323599 103.994164 \nL 251.551895 103.994427 \nL 251.78019 103.99469 \nL 252.008486 103.994953 \nL 252.259611 103.985017 \nL 252.647713 104.05685 \nL 252.898839 104.046883 \nL 253.149964 104.057271 \nL 253.401089 104.047314 \nL 253.743532 104.098318 \nL 253.948998 104.10857 \nL 254.200123 104.098592 \nL 254.793692 103.937566 \nL 255.021987 103.917736 \nL 255.41009 103.868046 \nL 255.615556 103.838269 \nL 255.957999 103.808829 \nL 256.368931 103.869717 \nL 256.551567 103.91004 \nL 256.802693 103.90043 \nL 257.008159 103.93069 \nL 257.556068 104.050868 \nL 258.058318 103.952034 \nL 258.332273 103.952402 \nL 258.788864 103.873881 \nL 259.131307 103.904078 \nL 259.405262 103.904509 \nL 259.702046 103.914813 \nL 259.930342 103.934822 \nL 260.341274 103.994269 \nL 260.683717 103.965261 \nL 261.003331 103.985217 \nL 261.414263 103.927104 \nL 261.642558 103.907926 \nL 262.09915 103.830773 \nL 262.327445 103.811752 \nL 262.669888 103.783322 \nL 262.943843 103.78389 \nL 263.217798 103.784468 \nL 263.446093 103.765605 \nL 263.697218 103.775815 \nL 263.971173 103.776393 \nL 264.313616 103.748236 \nL 264.541912 103.72952 \nL 264.884355 103.70151 \nL 265.203969 103.721477 \nL 265.500753 103.712613 \nL 265.729049 103.694035 \nL 265.957344 103.713707 \nL 266.20847 103.70477 \nL 266.505254 103.695969 \nL 266.779208 103.696642 \nL 267.098822 103.678432 \nL 267.418436 103.698219 \nL 267.783709 103.66123 \nL 267.989175 103.633388 \nL 268.354448 103.59661 \nL 268.674062 103.616408 \nL 269.130653 103.542409 \nL 269.450266 103.562207 \nL 269.724221 103.563048 \nL 269.975346 103.554447 \nL 270.27213 103.546004 \nL 270.546085 103.546856 \nL 270.957017 103.49215 \nL 271.185313 103.474276 \nL 271.641904 103.401433 \nL 271.938688 103.411853 \nL 272.235472 103.403673 \nL 272.555086 103.423398 \nL 272.943188 103.378586 \nL 273.37695 103.444857 \nL 273.696564 103.427561 \nL 274.130325 103.493507 \nL 274.312962 103.530864 \nL 274.632576 103.550221 \nL 274.952189 103.532872 \nL 275.203314 103.524503 \nL 275.40878 103.552576 \nL 275.95669 103.66368 \nL 276.253474 103.655321 \nL 276.732895 103.738331 \nL 277.006849 103.738931 \nL 277.394952 103.785067 \nL 277.760225 103.749603 \nL 278.057009 103.759265 \nL 278.376623 103.741875 \nL 278.604918 103.724347 \nL 278.810384 103.751811 \nL 279.03868 103.734294 \nL 279.312635 103.734893 \nL 279.586589 103.735482 \nL 280.020351 103.673732 \nL 280.408453 103.719374 \nL 280.728067 103.702225 \nL 280.933533 103.67594 \nL 281.275976 103.650054 \nL 281.572761 103.659685 \nL 281.755397 103.695685 \nL 282.075011 103.714169 \nL 282.348965 103.714779 \nL 282.531602 103.679767 \nL 282.759897 103.697999 \nL 283.12517 103.734168 \nL 283.330636 103.761084 \nL 283.467614 103.796643 \nL 283.467614 103.796643 \n\" clip-path=\"url(#p533b02d9f9)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 55.194886 192.42 \nL 55.217716 192.42 \nL 55.286205 86.579996 \nL 55.331864 116.819999 \nL 55.423182 128.274544 \nL 55.377523 114.019999 \nL 55.446011 118.920002 \nL 55.5145 98.339995 \nL 55.605818 99.577891 \nL 55.674307 112.238179 \nL 55.765625 111.004615 \nL 55.856943 104.22 \nL 55.879773 107.065164 \nL 55.925432 112.238179 \nL 55.993921 109.120002 \nL 56.199387 98.339995 \nL 56.267875 93.195 \nL 56.336364 95.572936 \nL 56.519001 102.725083 \nL 56.610319 97.219995 \nL 56.678807 98.874551 \nL 56.770126 101.699995 \nL 56.792955 100.493242 \nL 56.838614 100.595346 \nL 57.021251 94.419995 \nL 57.181058 98.206359 \nL 57.203887 97.282923 \nL 57.249546 99.373841 \nL 57.272376 98.467827 \nL 57.318035 100.466809 \nL 57.386524 99.673613 \nL 57.409353 98.820003 \nL 57.455012 100.692003 \nL 57.523501 99.938446 \nL 57.56916 101.699995 \nL 57.637649 99.319998 \nL 57.683308 101.01273 \nL 57.706137 101.836218 \nL 57.751797 100.317349 \nL 57.774626 99.577891 \nL 57.820285 101.178624 \nL 57.843115 100.450764 \nL 57.865944 101.230165 \nL 57.911603 99.810004 \nL 57.934433 99.117525 \nL 57.980092 100.634638 \nL 58.002922 99.952262 \nL 58.117069 102.168836 \nL 58.139899 101.506154 \nL 58.185558 100.210913 \nL 58.276876 100.32882 \nL 58.299706 101.001017 \nL 58.345365 99.778272 \nL 58.368195 100.439998 \nL 58.391024 99.841274 \nL 58.436683 101.136084 \nL 58.528001 102.420001 \nL 58.550831 101.836218 \nL 58.59649 100.692003 \nL 58.664979 101.337642 \nL 58.756297 103.658213 \nL 58.801956 102.555845 \nL 58.870445 100.953335 \nL 58.938933 101.547275 \nL 59.327036 109.066154 \nL 59.395525 109.464323 \nL 59.464013 107.973191 \nL 59.486843 108.42 \nL 59.532502 107.452462 \nL 59.578161 108.332952 \nL 59.692309 107.783635 \nL 59.737968 108.630001 \nL 59.783627 107.713068 \nL 59.806457 108.130342 \nL 59.852116 107.23171 \nL 59.920604 107.61231 \nL 59.966264 108.42 \nL 60.011923 107.548299 \nL 60.057582 108.341495 \nL 60.217389 107.013665 \nL 60.263048 106.98861 \nL 60.445684 109.183635 \nL 60.559832 107.957288 \nL 60.582662 108.313669 \nL 60.719639 109.664446 \nL 60.628321 108.279414 \nL 60.742468 109.280655 \nL 60.856616 108.82482 \nL 60.947934 108.752014 \nL 61.062082 110.373486 \nL 61.084912 110.009188 \nL 61.130571 110.64069 \nL 61.19906 111.569998 \nL 61.267548 111.157077 \nL 61.473014 109.333042 \nL 61.609992 111.100852 \nL 61.70131 111.004615 \nL 61.724139 110.673658 \nL 61.769798 111.239377 \nL 61.792628 110.911033 \nL 61.815458 111.191133 \nL 61.861117 110.541504 \nL 61.906776 109.900676 \nL 61.975264 110.139465 \nL 62.020924 110.1 \nL 62.157901 111.137647 \nL 62.386196 108.685821 \nL 62.409026 108.949971 \nL 62.431856 109.212454 \nL 62.500344 108.891028 \nL 62.660151 107.984636 \nL 62.682981 108.241278 \nL 62.72864 107.684049 \nL 62.751469 107.939277 \nL 62.888447 106.829466 \nL 62.911276 107.081945 \nL 62.934106 107.332941 \nL 62.979765 106.798948 \nL 63.048254 106.520872 \nL 63.071083 106.769135 \nL 63.093913 107.015968 \nL 63.162401 106.74 \nL 63.276549 105.462254 \nL 63.345038 105.69821 \nL 63.390697 106.18 \nL 63.436356 105.681876 \nL 63.527674 105.183937 \nL 63.550504 105.421633 \nL 63.710311 106.578291 \nL 63.596163 105.415124 \nL 63.75597 106.565747 \nL 64.189731 103.103543 \nL 64.349538 104.22 \nL 64.372368 104.001146 \nL 64.646322 102.307225 \nL 64.669152 102.52385 \nL 64.714811 102.109956 \nL 64.851788 101.307739 \nL 64.874618 101.522115 \nL 64.897448 101.735491 \nL 64.943107 101.33495 \nL 64.965936 101.136084 \nL 65.011595 101.559672 \nL 65.034425 101.769999 \nL 65.080084 101.374841 \nL 65.102914 101.584138 \nL 65.171402 100.998084 \nL 65.217061 101.413639 \nL 65.239891 101.620002 \nL 65.28555 101.23354 \nL 65.30838 101.438915 \nL 65.354039 101.05587 \nL 65.422527 101.273452 \nL 65.468186 101.677652 \nL 65.513846 101.299475 \nL 65.627993 100.753627 \nL 65.650823 100.953335 \nL 65.901948 102.718721 \nL 65.924778 102.534648 \nL 65.947607 102.351353 \nL 65.993266 102.731391 \nL 66.198732 103.672176 \nL 66.31288 102.7741 \nL 66.381369 102.962568 \nL 66.404198 103.144391 \nL 66.449857 102.791659 \nL 66.472687 102.616365 \nL 66.541176 102.80313 \nL 66.700982 103.346728 \nL 66.83796 103.011785 \nL 66.906448 103.533619 \nL 66.952108 103.194418 \nL 66.974937 103.025801 \nL 67.020596 103.37029 \nL 67.134744 103.883354 \nL 67.157574 103.715999 \nL 67.271721 103.221513 \nL 67.294551 103.389489 \nL 67.31738 103.556845 \nL 67.36304 103.228989 \nL 67.385869 103.395703 \nL 67.568506 102.758118 \nL 67.682653 103.254307 \nL 67.705483 103.095405 \nL 67.888119 102.161476 \nL 67.910949 102.323228 \nL 68.093585 103.596683 \nL 68.139244 103.28831 \nL 68.162074 103.134939 \nL 68.207733 103.447675 \nL 68.230563 103.294829 \nL 68.299051 103.759822 \nL 68.34471 103.455697 \nL 68.39037 103.153675 \nL 68.436029 103.460965 \nL 68.664324 104.369239 \nL 68.846961 103.483771 \nL 68.86979 103.632001 \nL 68.938279 103.781198 \nL 68.961108 103.635891 \nL 68.983938 103.491078 \nL 69.029597 103.784089 \nL 69.189404 104.22 \nL 69.257893 104.362952 \nL 69.303552 104.077511 \nL 69.326381 104.22 \nL 69.37204 103.936399 \nL 69.554677 103.1 \nL 69.577506 103.241554 \nL 69.645995 103.385304 \nL 69.668825 103.247715 \nL 69.691654 103.110567 \nL 69.737313 103.39053 \nL 69.89712 103.809765 \nL 70.056927 103.137788 \nL 70.079757 103.274516 \nL 70.262393 103.819701 \nL 70.467859 103.166871 \nL 70.513518 103.432503 \nL 70.559177 103.173116 \nL 70.718984 102.795328 \nL 70.833132 102.934285 \nL 70.878791 102.681627 \nL 70.92445 102.941739 \nL 71.015768 103.203282 \nL 71.038598 103.077847 \nL 71.107087 103.209107 \nL 71.152746 102.960003 \nL 71.403871 104.095953 \nL 71.4267 103.972242 \nL 71.44953 103.848889 \nL 71.495189 104.096647 \nL 71.518019 103.97363 \nL 71.540848 104.096983 \nL 71.586507 103.85199 \nL 71.609337 103.974997 \nL 71.632166 103.85301 \nL 71.677826 104.098003 \nL 71.883292 104.701968 \nL 71.997439 104.339673 \nL 72.020269 104.459026 \nL 72.225735 105.046505 \nL 72.385542 104.687906 \nL 72.522519 105.14842 \nL 72.545349 105.031301 \nL 72.613837 105.143562 \nL 72.682326 104.794967 \nL 72.773644 105.020777 \nL 72.796474 104.905493 \nL 72.819303 104.790504 \nL 72.864962 105.016644 \nL 72.887792 104.90196 \nL 73.024769 105.122302 \nL 73.161747 104.891572 \nL 73.321554 105.662262 \nL 73.367213 105.437315 \nL 73.50419 105.208545 \nL 73.549849 105.425218 \nL 73.595508 105.203641 \nL 73.663997 105.091112 \nL 73.686826 105.198793 \nL 73.732486 105.413358 \nL 73.778145 105.193988 \nL 73.869463 104.973846 \nL 73.892292 105.080487 \nL 74.211906 106.123596 \nL 74.440202 105.47403 \nL 74.463031 105.576923 \nL 74.50869 105.365455 \nL 74.53152 105.468111 \nL 74.577179 105.257647 \nL 74.645668 105.357396 \nL 74.668497 105.459342 \nL 74.714156 105.250376 \nL 74.851134 105.038561 \nL 74.942452 105.238474 \nL 74.965282 105.135571 \nL 75.010941 104.93047 \nL 75.079429 105.029172 \nL 75.193577 105.125131 \nL 75.239236 104.922389 \nL 75.284895 105.121019 \nL 75.353384 105.21774 \nL 75.376214 105.11695 \nL 75.490361 104.814608 \nL 75.513191 104.912932 \nL 75.55885 105.108912 \nL 75.604509 104.90983 \nL 75.695827 104.514326 \nL 75.764316 104.61113 \nL 75.832805 104.512375 \nL 75.878464 104.706216 \nL 75.901293 104.608544 \nL 75.946952 104.801538 \nL 75.969782 104.897718 \nL 76.015441 104.703025 \nL 76.038271 104.605994 \nL 76.08393 104.797727 \nL 76.152418 104.699871 \nL 76.266566 105.174548 \nL 76.426373 104.504211 \nL 76.472032 104.692669 \nL 76.494862 104.786598 \nL 76.540521 104.596926 \nL 76.60901 104.501787 \nL 76.631839 104.595317 \nL 76.905794 105.331767 \nL 76.974282 105.420629 \nL 77.019942 105.233795 \nL 77.042771 105.324802 \nL 77.08843 105.138752 \nL 77.11126 105.229574 \nL 77.271067 104.766694 \nL 77.293896 104.857154 \nL 77.362385 104.945925 \nL 77.385214 104.85453 \nL 77.453703 104.94295 \nL 77.522192 104.670457 \nL 77.545021 104.760002 \nL 77.59068 104.579267 \nL 77.61351 104.489176 \nL 77.659169 104.667718 \nL 77.955953 105.457276 \nL 78.092931 105.274181 \nL 78.207078 105.531202 \nL 78.229908 105.442572 \nL 78.252738 105.354126 \nL 78.298397 105.526023 \nL 78.389715 105.694336 \nL 78.412544 105.606247 \nL 78.481033 105.515788 \nL 78.503863 105.600822 \nL 78.595181 105.767368 \nL 78.61801 105.679979 \nL 78.800647 105.327824 \nL 78.823476 105.411891 \nL 78.869136 105.239652 \nL 78.891965 105.323561 \nL 78.914795 105.237691 \nL 78.960454 105.405031 \nL 78.983283 105.319329 \nL 79.211579 105.811454 \nL 79.325727 105.553839 \nL 79.348556 105.635866 \nL 79.371386 105.717735 \nL 79.417045 105.548813 \nL 79.485534 105.462254 \nL 79.508363 105.543829 \nL 79.531193 105.625247 \nL 79.576852 105.457607 \nL 79.599681 105.538877 \nL 79.64534 105.371863 \nL 79.713829 105.450699 \nL 79.827977 105.853332 \nL 79.873636 105.687281 \nL 79.964954 105.519447 \nL 79.987784 105.599392 \nL 80.124761 105.75321 \nL 80.147591 105.671189 \nL 80.19325 105.829491 \nL 80.353057 106.059165 \nL 80.375886 105.977611 \nL 80.421545 106.133926 \nL 80.467205 106.289674 \nL 80.512864 106.127029 \nL 80.581352 106.042642 \nL 80.604182 106.120179 \nL 80.741159 106.424998 \nL 80.763989 106.344354 \nL 80.832477 106.260213 \nL 80.855307 106.336799 \nL 80.946625 106.485544 \nL 80.969455 106.405489 \nL 81.037943 106.166163 \nL 81.083603 106.318152 \nL 81.22058 106.461719 \nL 81.448875 105.982468 \nL 81.654341 106.501037 \nL 81.677171 106.4231 \nL 81.700001 106.3453 \nL 81.74566 106.493193 \nL 81.768489 106.415535 \nL 81.791319 106.489298 \nL 81.836978 106.334386 \nL 82.065273 105.867201 \nL 82.202251 106.156826 \nL 82.22508 106.080761 \nL 82.316399 105.777782 \nL 82.384887 105.847854 \nL 82.704501 106.560301 \nL 82.864308 106.328656 \nL 82.978456 106.537243 \nL 83.001285 106.462986 \nL 83.024115 106.38885 \nL 83.069774 106.529657 \nL 83.25241 106.801461 \nL 83.27524 106.727719 \nL 83.320899 106.866718 \nL 83.389388 106.931649 \nL 83.412217 106.858159 \nL 83.435047 106.78478 \nL 83.480706 106.922901 \nL 83.503535 106.849653 \nL 83.526365 106.918553 \nL 83.572024 106.77241 \nL 83.640513 106.69554 \nL 83.663342 106.76423 \nL 83.686172 106.832809 \nL 83.731831 106.687628 \nL 83.754661 106.756102 \nL 83.823149 106.539204 \nL 83.868808 106.675847 \nL 83.937297 106.600002 \nL 83.982956 106.736004 \nL 84.051445 106.799763 \nL 84.142763 106.513617 \nL 84.188422 106.648794 \nL 84.256911 106.573848 \nL 84.27974 106.502824 \nL 84.325399 106.637386 \nL 84.462377 107.038552 \nL 84.508036 106.896888 \nL 84.576525 106.822174 \nL 84.599354 106.888582 \nL 84.622184 106.954885 \nL 84.667843 106.81412 \nL 84.690672 106.743895 \nL 84.736331 106.876217 \nL 84.941797 107.196072 \nL 85.033116 107.05211 \nL 85.055945 107.117325 \nL 85.078775 107.182445 \nL 85.124434 107.043478 \nL 85.147263 107.108498 \nL 85.352729 106.755251 \nL 85.421218 106.816076 \nL 85.444048 106.747602 \nL 85.740832 106.130231 \nL 85.85498 106.188748 \nL 85.991957 105.918668 \nL 86.014787 105.982695 \nL 86.220253 106.295294 \nL 86.243082 106.228965 \nL 86.288741 106.355436 \nL 86.425719 106.732635 \nL 86.471378 106.600307 \nL 86.745332 106.069458 \nL 86.88231 106.188469 \nL 86.905139 106.123596 \nL 86.950798 106.247586 \nL 86.996457 106.371217 \nL 87.042117 106.241776 \nL 87.110605 106.048304 \nL 87.156264 106.171604 \nL 87.338901 106.536113 \nL 87.36173 106.471913 \nL 87.430219 106.279875 \nL 87.475878 106.401625 \nL 87.498708 106.462371 \nL 87.544367 106.334812 \nL 87.567196 106.395474 \nL 87.590026 106.331831 \nL 87.635685 106.452913 \nL 87.658515 106.38936 \nL 87.704174 106.510106 \nL 87.772662 106.443529 \nL 87.90964 106.311213 \nL 88.069447 106.484677 \nL 88.206424 106.35338 \nL 88.274913 106.53145 \nL 88.320572 106.406777 \nL 88.38906 106.341651 \nL 88.41189 106.400768 \nL 88.617356 106.688396 \nL 88.799992 106.435481 \nL 88.91414 106.487658 \nL 88.982629 106.42351 \nL 89.005458 106.481538 \nL 89.188095 106.706175 \nL 89.370731 106.457382 \nL 89.41639 106.571998 \nL 89.462049 106.451425 \nL 89.713175 106.027138 \nL 89.987129 106.475608 \nL 90.055618 106.298012 \nL 90.101277 106.410588 \nL 90.124107 106.466766 \nL 90.169766 106.34877 \nL 90.192595 106.404874 \nL 90.238254 106.287188 \nL 90.283913 106.399196 \nL 90.557868 106.837546 \nL 90.626357 106.889287 \nL 90.672016 106.77241 \nL 90.740505 106.824109 \nL 90.763334 106.765865 \nL 90.831823 106.817437 \nL 90.877482 106.701328 \nL 90.945971 106.752864 \nL 90.9688 106.694999 \nL 91.128607 106.515999 \nL 91.242755 106.676205 \nL 91.265584 106.61886 \nL 91.448221 106.38476 \nL 91.585198 106.487211 \nL 91.699346 106.424998 \nL 91.813494 106.473085 \nL 91.973301 106.299158 \nL 92.201596 106.612604 \nL 92.338573 106.495428 \nL 92.475551 106.810945 \nL 92.544039 106.752317 \nL 92.589699 106.641597 \nL 92.658187 106.690888 \nL 92.863653 106.944528 \nL 92.932142 106.886264 \nL 92.954971 106.937947 \nL 92.977801 106.989566 \nL 93.02346 106.879829 \nL 93.206097 106.655292 \nL 93.365903 106.803264 \nL 93.411563 106.694867 \nL 93.480051 106.743002 \nL 93.662688 106.940286 \nL 93.913813 106.558834 \nL 93.959472 106.659907 \nL 94.027961 106.603782 \nL 94.096449 106.547857 \nL 94.119279 106.598194 \nL 94.187767 106.645629 \nL 94.210597 106.592632 \nL 94.256256 106.486822 \nL 94.324745 106.534283 \nL 94.393233 106.581582 \nL 94.416063 106.5289 \nL 94.57587 106.366234 \nL 94.598699 106.416061 \nL 94.644359 106.311496 \nL 94.667188 106.361271 \nL 94.690018 106.309083 \nL 94.735677 106.408459 \nL 94.781336 106.507608 \nL 94.826995 106.403417 \nL 94.895484 106.247586 \nL 94.941143 106.346519 \nL 94.963972 106.395905 \nL 95.009631 106.292318 \nL 95.07812 106.238307 \nL 95.10095 106.287582 \nL 95.215097 106.432542 \nL 95.237927 106.381027 \nL 95.420563 106.171105 \nL 95.626029 106.410068 \nL 95.740177 106.354274 \nL 95.763007 106.402676 \nL 95.808666 106.301124 \nL 95.877155 106.347087 \nL 95.922814 106.245882 \nL 96.173939 106.675458 \nL 96.196768 106.625011 \nL 96.425064 106.221216 \nL 96.447893 106.268893 \nL 96.539212 106.361723 \nL 96.562041 106.311891 \nL 96.676189 106.063565 \nL 96.721848 106.158461 \nL 96.790337 106.203652 \nL 96.813166 106.154208 \nL 97.018632 105.904127 \nL 97.041462 105.951299 \nL 97.087121 105.853332 \nL 97.109951 105.900457 \nL 97.13278 105.851555 \nL 97.178439 105.945653 \nL 97.201269 105.896804 \nL 97.224098 105.943776 \nL 97.269757 105.846246 \nL 97.315417 105.748926 \nL 97.383905 105.794148 \nL 97.406735 105.840973 \nL 97.452394 105.743974 \nL 97.475223 105.790751 \nL 97.543712 105.645644 \nL 97.612201 105.690792 \nL 97.794837 105.873457 \nL 97.840496 105.777304 \nL 97.886155 105.869919 \nL 98.068792 106.050653 \nL 98.114451 105.954926 \nL 98.16011 106.046763 \nL 98.205769 106.138411 \nL 98.274258 106.088641 \nL 98.479724 105.847308 \nL 98.548213 105.983998 \nL 98.593872 105.889402 \nL 98.639531 105.794999 \nL 98.68519 105.885895 \nL 98.776508 106.067119 \nL 98.844997 106.018117 \nL 98.913485 105.877205 \nL 98.981974 105.920571 \nL 99.004804 105.965625 \nL 99.050463 105.872027 \nL 99.141781 105.777009 \nL 99.164611 105.821974 \nL 99.370077 106.042316 \nL 99.415736 105.949412 \nL 99.484224 105.992178 \nL 99.552713 106.034814 \nL 99.575543 105.988535 \nL 99.598372 105.942304 \nL 99.644031 106.031086 \nL 99.73535 106.208115 \nL 99.803838 106.159949 \nL 99.986475 105.972317 \nL 100.351748 106.492967 \nL 100.374577 106.447272 \nL 100.420236 106.534026 \nL 100.534384 106.572592 \nL 100.671361 106.388487 \nL 100.694191 106.431632 \nL 100.785509 106.515494 \nL 100.808339 106.470225 \nL 100.922486 106.332572 \nL 100.945316 106.375513 \nL 101.150782 106.584846 \nL 101.173612 106.539903 \nL 101.219271 106.625058 \nL 101.310589 106.794863 \nL 101.379078 106.74747 \nL 101.516055 106.653105 \nL 101.607373 106.735042 \nL 101.630203 106.690467 \nL 101.858498 106.419609 \nL 102.041135 106.582885 \nL 102.063964 106.538794 \nL 102.109623 106.622335 \nL 102.178112 106.747344 \nL 102.223771 106.659303 \nL 102.29226 106.69849 \nL 102.383578 106.523096 \nL 102.406408 106.564611 \nL 102.452067 106.47717 \nL 102.748851 106.082186 \nL 102.908658 106.286856 \nL 102.931487 106.243711 \nL 102.999976 106.282913 \nL 103.068465 106.15384 \nL 103.182612 106.358942 \nL 103.228272 106.273114 \nL 103.502226 105.928173 \nL 103.707692 106.128375 \nL 103.753351 106.043683 \nL 103.82184 106.082506 \nL 103.890329 106.12122 \nL 103.913158 106.079016 \nL 103.935988 106.036853 \nL 103.981647 106.117661 \nL 104.004476 106.075541 \nL 104.072965 106.19647 \nL 104.118624 106.112351 \nL 104.187113 106.150786 \nL 104.232772 106.066903 \nL 104.255602 106.107068 \nL 104.301261 106.023348 \nL 104.32409 105.981543 \nL 104.369749 106.061762 \nL 104.461068 106.140056 \nL 104.483897 106.098336 \nL 104.689363 105.88722 \nL 104.872 106.043152 \nL 104.963318 105.878046 \nL 105.031806 105.916155 \nL 105.191613 106.031502 \nL 105.214443 105.990438 \nL 105.260102 106.069227 \nL 105.282932 106.108566 \nL 105.328591 106.026555 \nL 105.465568 105.861491 \nL 105.488398 105.900762 \nL 105.671034 106.054176 \nL 105.830841 105.849652 \nL 105.85367 105.88865 \nL 105.8765 105.92761 \nL 105.922159 105.846719 \nL 105.990648 105.804904 \nL 106.013477 105.843801 \nL 106.218943 106.034488 \nL 106.355921 105.950952 \nL 106.424409 105.909352 \nL 106.470068 105.986354 \nL 106.492898 105.946336 \nL 106.538557 106.023201 \nL 106.652705 106.214765 \nL 106.698364 106.134841 \nL 106.881 105.972317 \nL 106.972319 106.124716 \nL 107.040807 106.083379 \nL 107.200614 105.961556 \nL 107.269103 105.997914 \nL 107.291932 105.958501 \nL 107.474569 105.798437 \nL 107.588717 105.910246 \nL 107.611546 105.871112 \nL 107.771353 105.751249 \nL 107.93116 105.861108 \nL 108.113796 105.70331 \nL 108.227944 105.73807 \nL 108.319262 105.735462 \nL 108.387751 105.847029 \nL 108.43341 105.770023 \nL 108.547558 105.72898 \nL 108.707365 105.987762 \nL 108.753024 105.911092 \nL 109.072638 105.527495 \nL 109.118297 105.601043 \nL 109.163956 105.525287 \nL 109.323763 105.409883 \nL 109.346592 105.446546 \nL 109.392252 105.371243 \nL 109.529229 105.294252 \nL 109.643377 105.32897 \nL 109.666206 105.291555 \nL 109.711865 105.364498 \nL 109.826013 105.472632 \nL 109.848843 105.43528 \nL 109.96299 105.322499 \nL 109.98582 105.358773 \nL 110.077138 105.430228 \nL 110.099968 105.393065 \nL 110.191286 105.24473 \nL 110.236945 105.317016 \nL 110.373922 105.387246 \nL 110.5109 105.238811 \nL 110.533729 105.274764 \nL 110.556559 105.310686 \nL 110.602218 105.237134 \nL 110.784854 105.088967 \nL 110.899002 105.195581 \nL 110.921832 105.159065 \nL 111.172957 104.903164 \nL 111.264275 104.973846 \nL 111.287105 104.937656 \nL 111.355593 104.829265 \nL 111.401252 104.900388 \nL 111.53823 105.041626 \nL 111.561059 105.005589 \nL 111.698037 104.932441 \nL 111.789355 105.002419 \nL 111.812184 104.966554 \nL 112.200287 104.502466 \nL 112.360094 104.607303 \nL 112.382923 104.571954 \nL 112.428582 104.642011 \nL 112.451412 104.676997 \nL 112.497071 104.606378 \nL 112.611219 104.570556 \nL 112.656878 104.640334 \nL 112.702537 104.569999 \nL 112.953662 104.324543 \nL 113.273276 104.670531 \nL 113.296106 104.635713 \nL 113.341765 104.704618 \nL 113.433083 104.703856 \nL 113.455912 104.669117 \nL 113.501572 104.737806 \nL 113.57006 104.771683 \nL 113.59289 104.736997 \nL 113.821185 104.528993 \nL 113.980992 104.69935 \nL 114.003822 104.664937 \nL 114.186458 104.527079 \nL 114.323436 104.59445 \nL 114.528902 104.42354 \nL 114.62022 104.558712 \nL 114.665879 104.490758 \nL 114.780027 104.45646 \nL 114.917004 104.523326 \nL 115.008322 104.455561 \nL 115.031152 104.489107 \nL 115.09964 104.522401 \nL 115.12247 104.488697 \nL 115.305106 104.286971 \nL 115.327936 104.320416 \nL 115.442084 104.353636 \nL 115.579061 104.286666 \nL 115.601891 104.319964 \nL 115.64755 104.253293 \nL 115.693209 104.186733 \nL 115.738868 104.253246 \nL 115.830186 104.319586 \nL 115.853016 104.286366 \nL 116.035652 104.153834 \nL 116.104141 104.186954 \nL 116.12697 104.153928 \nL 116.218289 104.154034 \nL 116.332436 104.18708 \nL 116.469414 104.12145 \nL 116.560732 104.252799 \nL 116.606391 104.187227 \nL 116.629221 104.154475 \nL 116.67488 104.22 \nL 116.766198 104.22 \nL 116.926005 104.056966 \nL 116.948834 104.089623 \nL 117.108641 104.18749 \nL 117.17713 104.155053 \nL 117.19996 104.187542 \nL 117.314107 104.22 \nL 117.542403 103.961728 \nL 117.565232 103.994091 \nL 117.588062 104.026432 \nL 117.633721 103.962106 \nL 117.747869 103.866037 \nL 117.770698 103.898337 \nL 118.021824 104.123889 \nL 118.158801 104.060162 \nL 118.295778 104.124299 \nL 118.318608 104.092452 \nL 118.364267 104.156273 \nL 118.387096 104.188152 \nL 118.432756 104.124509 \nL 118.524074 103.997518 \nL 118.569733 104.061193 \nL 118.683881 104.093188 \nL 118.70671 104.06154 \nL 118.889347 104.125193 \nL 119.003494 104.030733 \nL 119.026324 104.062328 \nL 119.117642 104.125529 \nL 119.163301 104.062665 \nL 119.368767 103.906349 \nL 119.551404 104.032341 \nL 119.574233 104.001146 \nL 119.688381 103.907895 \nL 119.73404 103.970497 \nL 119.939506 104.064557 \nL 120.007995 104.033666 \nL 120.053654 104.095858 \nL 120.190631 104.15806 \nL 120.213461 104.127128 \nL 120.350438 104.127317 \nL 120.487416 104.189172 \nL 120.510245 104.158365 \nL 120.601563 104.15845 \nL 120.624393 104.189235 \nL 120.692882 104.22 \nL 120.738541 104.158576 \nL 120.898348 104.128095 \nL 120.966836 104.158796 \nL 121.012495 104.097667 \nL 121.172302 104.067459 \nL 121.354939 104.128726 \nL 121.537575 104.0683 \nL 121.834359 104.28041 \nL 122.039825 104.069436 \nL 122.085485 104.129725 \nL 122.245291 104.159964 \nL 122.382269 104.160079 \nL 122.519246 104.160205 \nL 122.701883 104.100726 \nL 122.724712 104.130576 \nL 122.793201 104.041342 \nL 122.930178 104.041699 \nL 122.953008 104.071465 \nL 123.021496 103.982588 \nL 123.386769 103.688672 \nL 123.546576 103.719364 \nL 123.660724 103.690796 \nL 123.683553 103.720362 \nL 123.820531 103.83869 \nL 123.86619 103.780315 \nL 123.980338 103.75179 \nL 124.003167 103.781198 \nL 124.208633 103.928334 \nL 124.231463 103.899273 \nL 124.36844 103.783511 \nL 124.414099 103.84196 \nL 124.551077 103.900745 \nL 124.573906 103.871841 \nL 124.710883 103.814612 \nL 124.733713 103.843695 \nL 124.87069 103.902217 \nL 124.89352 103.873439 \nL 125.007668 103.845167 \nL 125.030497 103.874112 \nL 125.144645 103.903458 \nL 125.167475 103.874796 \nL 125.281622 103.904078 \nL 125.39577 103.99061 \nL 125.441429 103.933455 \nL 125.578407 103.876815 \nL 125.601236 103.905508 \nL 125.806702 104.048954 \nL 125.829532 104.020513 \nL 125.966509 103.96402 \nL 125.989339 103.992535 \nL 126.263293 104.163349 \nL 126.308952 104.106782 \nL 126.377441 104.191717 \nL 126.400271 104.22 \nL 126.468759 104.135276 \nL 126.491589 104.107066 \nL 126.560077 104.19179 \nL 126.651396 104.191832 \nL 126.674225 104.163675 \nL 126.788373 104.135655 \nL 126.811203 104.163791 \nL 126.92535 104.248063 \nL 126.971009 104.191958 \nL 127.153646 104.080129 \nL 127.176475 104.108139 \nL 127.678726 104.553249 \nL 127.929851 104.358373 \nL 127.95268 104.385999 \nL 128.089658 104.496141 \nL 128.135317 104.440778 \nL 128.226635 104.4405 \nL 128.249465 104.467984 \nL 128.386442 104.467521 \nL 128.409271 104.439948 \nL 128.47776 104.522148 \nL 128.660397 104.630997 \nL 128.683226 104.603476 \nL 128.865863 104.547882 \nL 128.98001 104.629215 \nL 129.025669 104.574436 \nL 129.048499 104.547072 \nL 129.116988 104.628458 \nL 129.322454 104.708792 \nL 129.50509 104.653414 \nL 129.687727 104.814487 \nL 129.733386 104.760112 \nL 129.984511 104.623721 \nL 130.121488 104.676718 \nL 130.144318 104.649718 \nL 130.167147 104.622738 \nL 130.235636 104.702846 \nL 130.395443 104.72859 \nL 130.418272 104.701674 \nL 130.486761 104.78144 \nL 130.669397 104.833428 \nL 130.829204 104.75229 \nL 130.852034 104.778733 \nL 131.011841 104.804104 \nL 131.171648 104.723396 \nL 131.194477 104.749729 \nL 131.399943 104.827545 \nL 131.58258 104.773392 \nL 131.742387 104.798531 \nL 131.947853 104.665852 \nL 131.970682 104.691938 \nL 132.107659 104.691097 \nL 132.244637 104.690261 \nL 132.495762 104.81894 \nL 132.60991 104.792049 \nL 132.632739 104.817878 \nL 132.883864 104.9455 \nL 133.043671 104.918152 \nL 133.180649 104.968552 \nL 133.203478 104.942529 \nL 133.386115 104.889354 \nL 133.61441 104.99008 \nL 133.842706 104.783086 \nL 133.888365 104.833922 \nL 133.911194 104.859314 \nL 133.979683 104.782108 \nL 134.185149 104.65323 \nL 134.207979 104.678579 \nL 134.550422 104.904899 \nL 134.961354 104.548071 \nL 134.984183 104.573206 \nL 135.16682 104.622738 \nL 135.303797 104.62205 \nL 135.463604 104.646327 \nL 135.646241 104.595317 \nL 135.737559 104.594891 \nL 135.760388 104.569804 \nL 135.874536 104.544354 \nL 135.897366 104.56921 \nL 136.057173 104.593414 \nL 136.262639 104.368987 \nL 136.331127 104.443291 \nL 136.445275 104.517301 \nL 136.490934 104.467616 \nL 136.673571 104.318824 \nL 136.71923 104.368151 \nL 136.810548 104.417316 \nL 136.856207 104.367904 \nL 137.267139 104.072832 \nL 137.381287 104.048544 \nL 137.495435 104.073242 \nL 137.632412 104.073484 \nL 137.792219 104.000662 \nL 137.815048 104.025087 \nL 137.952026 104.074052 \nL 137.974855 104.049774 \nL 138.317299 103.832518 \nL 138.477105 103.905781 \nL 138.499935 103.881704 \nL 138.728231 103.786234 \nL 138.956526 103.883544 \nL 139.116333 103.764249 \nL 139.161992 103.812446 \nL 139.298969 103.86098 \nL 139.321799 103.837144 \nL 139.458776 103.837765 \nL 139.687072 103.981747 \nL 139.709902 103.957995 \nL 139.892538 103.863493 \nL 139.915368 103.88735 \nL 140.052345 103.887886 \nL 140.075174 103.864261 \nL 140.143663 103.935642 \nL 140.28064 103.983408 \nL 140.30347 103.959825 \nL 140.531766 103.866164 \nL 140.760061 103.961213 \nL 141.034016 103.774427 \nL 141.056845 103.797989 \nL 141.216652 103.868981 \nL 141.239482 103.845671 \nL 141.444948 103.729867 \nL 141.467777 103.753335 \nL 141.696073 103.847648 \nL 141.810221 103.824906 \nL 141.83305 103.848237 \nL 141.924368 103.895057 \nL 141.970028 103.848826 \nL 142.129834 103.733726 \nL 142.175494 103.780273 \nL 142.3353 103.804182 \nL 142.449448 103.827797 \nL 142.609255 103.851538 \nL 142.814721 103.783479 \nL 142.906039 103.783932 \nL 142.928869 103.761105 \nL 143.134335 103.60194 \nL 143.179994 103.648014 \nL 143.38546 103.717828 \nL 143.636585 103.605442 \nL 143.750733 103.583509 \nL 143.864881 103.607029 \nL 144.275813 103.881031 \nL 144.458449 103.836619 \nL 144.549767 103.791954 \nL 144.663915 103.815001 \nL 144.869381 103.88327 \nL 144.983529 103.906118 \nL 145.280313 104.063579 \nL 145.371631 104.108381 \nL 145.531438 104.13086 \nL 145.828222 103.97568 \nL 145.988029 103.998275 \nL 146.170666 103.954473 \nL 146.284814 103.932708 \nL 146.421791 103.933139 \nL 146.581598 103.955661 \nL 146.764234 103.912227 \nL 146.946871 103.956712 \nL 147.152337 103.891629 \nL 147.334973 103.935978 \nL 147.540439 103.87121 \nL 147.677416 103.871726 \nL 147.928542 103.764133 \nL 148.088348 103.786581 \nL 148.202496 103.808756 \nL 148.316644 103.787643 \nL 148.567769 103.681008 \nL 148.681917 103.660137 \nL 148.887383 103.596904 \nL 149.115678 103.684151 \nL 149.252656 103.68494 \nL 149.480951 103.77163 \nL 149.57227 103.814728 \nL 149.754906 103.858089 \nL 149.914713 103.837449 \nL 150.097349 103.88061 \nL 150.257156 103.860002 \nL 150.394134 103.860517 \nL 150.5996 103.797989 \nL 150.782236 103.840919 \nL 151.010532 103.757783 \nL 151.215998 103.821667 \nL 151.307316 103.863935 \nL 151.672589 104.073936 \nL 151.832396 104.053349 \nL 152.12918 104.199234 \nL 152.243328 104.22 \nL 152.357475 104.199276 \nL 152.517282 104.178626 \nL 152.699919 104.22 \nL 152.859726 104.199392 \nL 153.019532 104.22 \nL 153.270658 104.11737 \nL 153.453294 104.158534 \nL 153.63593 104.117749 \nL 153.887056 104.22 \nL 153.978374 104.260759 \nL 154.115351 104.260701 \nL 154.275158 104.240319 \nL 154.480624 104.301102 \nL 154.617601 104.300991 \nL 154.777408 104.321079 \nL 154.937215 104.300734 \nL 155.21117 104.42128 \nL 155.348147 104.421001 \nL 155.485124 104.420728 \nL 155.622102 104.420454 \nL 155.964545 104.59957 \nL 156.170011 104.538987 \nL 156.352648 104.578216 \nL 156.489625 104.577727 \nL 156.580943 104.537694 \nL 156.786409 104.477605 \nL 156.946216 104.496988 \nL 157.128852 104.456991 \nL 157.311489 104.495994 \nL 157.402807 104.535139 \nL 157.562614 104.554317 \nL 157.79091 104.475081 \nL 157.927887 104.474745 \nL 158.042035 104.494033 \nL 158.179012 104.49367 \nL 158.29316 104.512896 \nL 158.704092 104.745114 \nL 158.886728 104.705364 \nL 159.092194 104.762531 \nL 159.32049 104.684005 \nL 159.480297 104.702599 \nL 159.617274 104.701968 \nL 159.731422 104.682186 \nL 159.914058 104.642931 \nL 160.051036 104.642379 \nL 160.233672 104.603313 \nL 160.370649 104.602814 \nL 160.667434 104.468126 \nL 160.804411 104.467805 \nL 160.941388 104.467485 \nL 161.124025 104.505068 \nL 161.329491 104.447613 \nL 161.489298 104.466212 \nL 161.580616 104.503843 \nL 161.740423 104.522311 \nL 161.831741 104.55981 \nL 161.991548 104.578152 \nL 162.265503 104.464425 \nL 162.448139 104.501551 \nL 162.676435 104.426032 \nL 162.859071 104.463079 \nL 162.996048 104.462769 \nL 163.247173 104.555363 \nL 163.40698 104.536264 \nL 163.658105 104.628332 \nL 163.749424 104.665079 \nL 163.93206 104.701359 \nL 164.206015 104.589345 \nL 164.388651 104.625603 \nL 164.502799 104.643593 \nL 164.753924 104.7345 \nL 164.95939 104.678516 \nL 165.233345 104.787144 \nL 165.415981 104.749677 \nL 165.530129 104.730883 \nL 165.735595 104.675294 \nL 165.895402 104.692826 \nL 166.192186 104.5646 \nL 166.351993 104.582216 \nL 166.5118 104.563611 \nL 166.648777 104.563191 \nL 166.854243 104.508469 \nL 167.173857 104.651474 \nL 167.402153 104.578831 \nL 167.53913 104.578389 \nL 167.721766 104.542031 \nL 167.835914 104.523831 \nL 167.972891 104.523463 \nL 168.109869 104.523095 \nL 168.292505 104.487004 \nL 168.56646 104.592899 \nL 168.726267 104.574646 \nL 168.840415 104.556572 \nL 168.954562 104.573931 \nL 169.045881 104.538282 \nL 169.319835 104.431678 \nL 169.548131 104.501677 \nL 169.707938 104.483703 \nL 169.890574 104.518389 \nL 170.027551 104.518032 \nL 170.187358 104.535123 \nL 170.324336 104.56972 \nL 170.438483 104.551909 \nL 170.689609 104.498891 \nL 170.940734 104.550468 \nL 171.237518 104.462879 \nL 171.465813 104.49703 \nL 171.579961 104.548655 \nL 171.762598 104.54814 \nL 171.968064 104.530323 \nL 172.196359 104.564126 \nL 172.356166 104.580844 \nL 172.607291 104.631507 \nL 172.835587 104.596484 \nL 173.17803 104.714833 \nL 173.497644 104.611393 \nL 173.68028 104.610794 \nL 173.885746 104.593156 \nL 174.068383 104.592578 \nL 174.296678 104.55806 \nL 174.661951 104.691838 \nL 174.913076 104.640402 \nL 175.209861 104.723233 \nL 175.438156 104.688794 \nL 175.689281 104.737938 \nL 175.917577 104.703609 \nL 176.123043 104.719433 \nL 176.237191 104.76886 \nL 176.442657 104.784532 \nL 176.648123 104.767004 \nL 176.899248 104.815501 \nL 177.036225 104.847875 \nL 177.333009 104.928766 \nL 177.7896 104.729069 \nL 178.017896 104.760901 \nL 178.223362 104.743636 \nL 178.33751 104.694104 \nL 178.542976 104.676997 \nL 178.702783 104.660106 \nL 178.999567 104.577742 \nL 179.250692 104.625703 \nL 179.501817 104.576297 \nL 179.684454 104.575776 \nL 179.86709 104.575256 \nL 179.981238 104.526533 \nL 180.163874 104.526081 \nL 180.300852 104.493565 \nL 180.506318 104.477047 \nL 180.825931 104.572543 \nL 181.122716 104.491778 \nL 181.305352 104.491384 \nL 181.533648 104.459026 \nL 181.853261 104.553791 \nL 182.127216 104.489628 \nL 182.332682 104.505026 \nL 182.538148 104.48876 \nL 182.720784 104.488371 \nL 182.903421 104.487987 \nL 183.063228 104.471911 \nL 183.337182 104.408531 \nL 183.542648 104.423914 \nL 183.816603 104.36087 \nL 184.090558 104.423046 \nL 184.341683 104.375884 \nL 184.569978 104.406733 \nL 184.752615 104.40647 \nL 184.958081 104.42169 \nL 185.140717 104.421406 \nL 185.483161 104.52904 \nL 185.734286 104.482178 \nL 186.03107 104.558522 \nL 186.168047 104.588909 \nL 186.327854 104.573106 \nL 186.464832 104.603413 \nL 186.647468 104.602877 \nL 186.830104 104.602346 \nL 187.149718 104.692963 \nL 187.469332 104.600501 \nL 187.720457 104.645354 \nL 187.857434 104.675267 \nL 188.131389 104.734904 \nL 188.268366 104.764633 \nL 188.496662 104.793905 \nL 188.656469 104.808299 \nL 188.861935 104.822457 \nL 189.09023 104.791361 \nL 189.364185 104.850214 \nL 189.523992 104.86445 \nL 189.752288 104.893281 \nL 189.934924 104.892371 \nL 190.071901 104.861832 \nL 190.277367 104.845956 \nL 190.414345 104.815543 \nL 190.688299 104.754908 \nL 190.825277 104.724679 \nL 191.007913 104.724001 \nL 191.16772 104.708603 \nL 191.327527 104.722818 \nL 191.647141 104.810165 \nL 191.829777 104.809377 \nL 192.17222 104.910787 \nL 192.400516 104.88029 \nL 192.628812 104.908489 \nL 192.902766 104.848642 \nL 193.016914 104.804298 \nL 193.176721 104.818214 \nL 193.382187 104.831893 \nL 193.587653 104.816437 \nL 193.793119 104.830079 \nL 193.907267 104.87312 \nL 194.021414 104.829075 \nL 194.341028 104.740866 \nL 194.706301 104.854946 \nL 194.980256 104.796092 \nL 195.25421 104.852464 \nL 195.391188 104.880569 \nL 195.665142 104.936605 \nL 195.939097 104.877993 \nL 196.076074 104.848774 \nL 196.28154 104.833591 \nL 196.509836 104.861091 \nL 196.760961 104.817289 \nL 197.034916 104.872914 \nL 197.240382 104.857795 \nL 197.445848 104.871027 \nL 197.605655 104.884433 \nL 197.811121 104.897591 \nL 198.016587 104.882525 \nL 198.267712 104.923572 \nL 198.427519 104.936846 \nL 198.655814 104.963773 \nL 198.906939 104.920444 \nL 199.226553 105.002755 \nL 199.500508 104.945468 \nL 199.865781 105.054964 \nL 200.025588 105.067943 \nL 200.162565 105.039366 \nL 200.390861 105.010346 \nL 200.641986 105.050511 \nL 200.893111 105.007623 \nL 201.144236 105.047651 \nL 201.372531 105.018816 \nL 201.577997 105.031443 \nL 201.760634 105.030434 \nL 201.9661 105.043019 \nL 202.125907 105.055826 \nL 202.422691 105.122512 \nL 202.742305 105.038688 \nL 203.016259 105.091648 \nL 203.267385 105.049381 \nL 203.609828 105.142426 \nL 203.952271 105.045564 \nL 204.157737 105.057939 \nL 204.317544 105.070546 \nL 204.52301 105.082858 \nL 204.705647 105.081801 \nL 204.911113 105.094066 \nL 205.162238 105.05233 \nL 205.367704 105.064595 \nL 205.57317 105.050053 \nL 205.824295 105.088767 \nL 206.05259 105.060762 \nL 206.258056 105.072948 \nL 206.57767 104.991352 \nL 206.851625 105.043056 \nL 206.988602 105.068842 \nL 207.148409 105.054701 \nL 207.399534 105.013643 \nL 207.62783 105.038866 \nL 207.833296 105.024577 \nL 208.13008 105.088835 \nL 208.267057 105.114364 \nL 208.65516 105.230173 \nL 208.951944 105.162756 \nL 209.15741 105.174574 \nL 209.499853 105.081123 \nL 209.728149 105.10591 \nL 209.887956 105.118007 \nL 210.139081 105.155533 \nL 210.321717 105.154434 \nL 210.481524 105.140508 \nL 210.823967 105.047924 \nL 211.143581 105.12369 \nL 211.349047 105.109606 \nL 211.531684 105.108565 \nL 211.782809 105.068569 \nL 211.896957 105.029409 \nL 212.170911 104.976695 \nL 212.330718 104.963116 \nL 212.467695 104.988072 \nL 212.627502 104.974503 \nL 212.832968 104.960745 \nL 213.152582 105.035723 \nL 213.426537 105.008864 \nL 213.76898 105.096032 \nL 214.020105 105.081969 \nL 214.225571 105.068201 \nL 214.431037 105.079746 \nL 214.682162 105.091017 \nL 214.956117 105.064321 \nL 215.161583 105.050653 \nL 215.618174 104.922789 \nL 215.892129 104.946646 \nL 216.234572 104.882593 \nL 216.554186 104.93119 \nL 216.828141 104.905072 \nL 217.102095 104.928782 \nL 217.330391 104.927783 \nL 217.672834 104.988251 \nL 217.992448 104.937272 \nL 218.175084 104.911765 \nL 218.517528 104.848679 \nL 218.791482 104.872242 \nL 218.951289 104.908484 \nL 219.133926 104.883155 \nL 219.476369 104.820501 \nL 219.795983 104.868262 \nL 220.047108 104.855061 \nL 220.252574 104.842071 \nL 220.48087 104.841214 \nL 220.709165 104.840357 \nL 221.074438 104.766168 \nL 221.325563 104.777461 \nL 221.576688 104.764518 \nL 221.941961 104.835768 \nL 222.170257 104.834926 \nL 222.421382 104.846045 \nL 222.786655 104.772603 \nL 223.01495 104.771851 \nL 223.197587 104.795219 \nL 223.494371 104.83009 \nL 223.745496 104.817236 \nL 223.950962 104.804582 \nL 224.293405 104.743868 \nL 224.56736 104.766794 \nL 224.955462 104.682528 \nL 225.206588 104.693683 \nL 225.480542 104.669274 \nL 225.686008 104.656925 \nL 226.09694 104.561635 \nL 226.325236 104.561177 \nL 226.462213 104.607924 \nL 226.667679 104.595717 \nL 227.055782 104.512869 \nL 227.421054 104.582384 \nL 227.67218 104.570183 \nL 227.900475 104.56972 \nL 228.128771 104.569263 \nL 228.379896 104.580381 \nL 228.69951 104.533299 \nL 228.859316 104.498234 \nL 229.110442 104.486258 \nL 229.338737 104.485906 \nL 229.70401 104.416128 \nL 229.909476 104.404373 \nL 230.092112 104.427205 \nL 230.251919 104.392513 \nL 230.525874 104.369276 \nL 230.73134 104.357632 \nL 231.005295 104.334516 \nL 231.210761 104.322945 \nL 231.393397 104.345687 \nL 231.690181 104.379701 \nL 232.009795 104.333864 \nL 232.306579 104.367778 \nL 232.580534 104.344846 \nL 232.877318 104.378634 \nL 233.31108 104.276514 \nL 233.653523 104.332818 \nL 233.904648 104.321394 \nL 234.201432 104.354966 \nL 234.406898 104.366043 \nL 234.589535 104.343453 \nL 234.749342 104.376978 \nL 235.183103 104.477273 \nL 235.548376 104.409772 \nL 235.753842 104.398406 \nL 235.959308 104.409341 \nL 236.37024 104.497813 \nL 236.689854 104.452954 \nL 236.849661 104.419498 \nL 237.123616 104.397065 \nL 237.374741 104.407869 \nL 237.717184 104.352364 \nL 237.991139 104.374197 \nL 238.150946 104.407075 \nL 238.379241 104.406844 \nL 238.630366 104.395609 \nL 238.92715 104.428198 \nL 239.109787 104.449889 \nL 239.360912 104.460503 \nL 239.680526 104.416438 \nL 239.908821 104.416196 \nL 240.228435 104.372331 \nL 240.685026 104.480496 \nL 240.958981 104.458437 \nL 241.187277 104.458143 \nL 241.552549 104.392854 \nL 241.780845 104.392644 \nL 242.123288 104.338475 \nL 242.397243 104.359813 \nL 242.739686 104.30588 \nL 242.967982 104.305775 \nL 243.196277 104.30567 \nL 243.470232 104.326935 \nL 243.698528 104.326804 \nL 243.995312 104.358631 \nL 244.200778 104.369129 \nL 244.58888 104.443239 \nL 244.976983 104.368519 \nL 245.159619 104.34718 \nL 245.433574 104.325831 \nL 245.707529 104.346818 \nL 245.981483 104.325526 \nL 246.301097 104.367494 \nL 246.552222 104.356775 \nL 246.734859 104.335625 \nL 246.917495 104.356517 \nL 247.077302 104.324927 \nL 247.237109 104.356291 \nL 247.442575 104.345672 \nL 247.762189 104.303641 \nL 247.990484 104.303541 \nL 248.264439 104.28257 \nL 248.538393 104.303304 \nL 248.789519 104.292795 \nL 249.086303 104.323839 \nL 249.383087 104.292575 \nL 249.542894 104.261437 \nL 249.74836 104.271741 \nL 249.885337 104.230341 \nL 250.159292 104.209675 \nL 250.75286 104.374428 \nL 250.958326 104.384553 \nL 251.323599 104.445836 \nL 251.551895 104.445573 \nL 251.78019 104.445315 \nL 252.008486 104.445052 \nL 252.259611 104.454983 \nL 252.647713 104.383145 \nL 252.898839 104.393123 \nL 253.149964 104.382729 \nL 253.401089 104.392681 \nL 253.743532 104.341682 \nL 253.948998 104.33143 \nL 254.200123 104.341403 \nL 254.793692 104.502434 \nL 255.021987 104.522264 \nL 255.41009 104.571954 \nL 255.615556 104.601731 \nL 255.957999 104.631165 \nL 256.368931 104.570277 \nL 256.551567 104.529966 \nL 256.802693 104.539565 \nL 257.008159 104.50931 \nL 257.556068 104.389138 \nL 258.058318 104.487966 \nL 258.332273 104.487604 \nL 258.788864 104.566114 \nL 259.131307 104.535917 \nL 259.405262 104.535491 \nL 259.702046 104.525192 \nL 259.930342 104.505184 \nL 260.341274 104.445725 \nL 260.683717 104.474745 \nL 261.003331 104.454783 \nL 261.414263 104.512896 \nL 261.642558 104.532074 \nL 262.09915 104.609233 \nL 262.327445 104.628242 \nL 262.669888 104.656683 \nL 262.943843 104.656105 \nL 263.217798 104.655532 \nL 263.446093 104.674389 \nL 263.697218 104.664185 \nL 263.971173 104.663602 \nL 264.313616 104.691759 \nL 264.541912 104.71048 \nL 264.884355 104.738484 \nL 265.203969 104.718523 \nL 265.500753 104.727392 \nL 265.729049 104.745965 \nL 265.957344 104.726293 \nL 266.20847 104.73523 \nL 266.505254 104.744036 \nL 266.779208 104.743358 \nL 267.098822 104.761568 \nL 267.418436 104.741781 \nL 267.783709 104.77877 \nL 267.989175 104.806612 \nL 268.354448 104.84339 \nL 268.674062 104.823592 \nL 269.130653 104.897591 \nL 269.450266 104.877788 \nL 269.724221 104.876947 \nL 269.975346 104.885553 \nL 270.27213 104.893996 \nL 270.546085 104.893139 \nL 270.957017 104.947844 \nL 271.185313 104.965719 \nL 271.641904 105.038561 \nL 271.938688 105.028152 \nL 272.235472 105.036322 \nL 272.555086 105.016597 \nL 272.943188 105.061409 \nL 273.37695 104.995143 \nL 273.696564 105.012439 \nL 274.130325 104.946493 \nL 274.312962 104.909136 \nL 274.632576 104.889779 \nL 274.952189 104.907128 \nL 275.203314 104.915497 \nL 275.40878 104.887419 \nL 275.95669 104.776325 \nL 276.253474 104.784684 \nL 276.732895 104.701669 \nL 277.006849 104.701075 \nL 277.394952 104.654928 \nL 277.760225 104.690397 \nL 278.057009 104.68074 \nL 278.376623 104.69812 \nL 278.604918 104.715658 \nL 278.810384 104.688189 \nL 279.03868 104.705701 \nL 279.312635 104.705107 \nL 279.586589 104.704518 \nL 280.020351 104.766268 \nL 280.408453 104.720626 \nL 280.728067 104.737775 \nL 280.933533 104.76406 \nL 281.275976 104.789952 \nL 281.572761 104.78031 \nL 281.755397 104.744315 \nL 282.075011 104.725825 \nL 282.348965 104.725215 \nL 282.531602 104.760233 \nL 282.759897 104.741996 \nL 283.12517 104.705827 \nL 283.330636 104.678916 \nL 283.467614 104.643362 \nL 283.467614 104.643362 \n\" clip-path=\"url(#p533b02d9f9)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 43.78125 104.22 \nL 294.88125 104.22 \n\" clip-path=\"url(#p533b02d9f9)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #000000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 201.24 \nL 43.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 294.88125 201.24 \nL 294.88125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 201.24 \nL 294.88125 201.24 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 294.88125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 182.759375 44.55625 \nL 287.88125 44.55625 \nQ 289.88125 44.55625 289.88125 42.55625 \nL 289.88125 14.2 \nQ 289.88125 12.2 287.88125 12.2 \nL 182.759375 12.2 \nQ 180.759375 12.2 180.759375 14.2 \nL 180.759375 42.55625 \nQ 180.759375 44.55625 182.759375 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 184.759375 20.298438 \nL 194.759375 20.298438 \nL 204.759375 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- P(coin=heads) -->\n     <g transform=\"translate(212.759375 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.296875\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"215.478516\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"243.261719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"306.640625\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"390.429688\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"453.808594\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"515.332031\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"576.611328\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"640.087891\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"692.1875\"/>\n     </g>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 184.759375 34.976562 \nL 194.759375 34.976562 \nL 204.759375 34.976562 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- P(coin=tails) -->\n     <g transform=\"translate(212.759375 38.476562) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.296875\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"215.478516\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"243.261719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"306.640625\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"390.429688\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"429.638672\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"490.917969\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"518.701172\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"546.484375\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"598.583984\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p533b02d9f9\">\n   <rect x=\"43.78125\" y=\"7.2\" width=\"251.1\" height=\"194.04\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = Multinomial(1, fair_probs).sample((10000,))\n",
    "cum_counts = counts.cumsum(dim=0)\n",
    "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
    "estimates = estimates.numpy()\n",
    "\n",
    "d2l.set_figsize((4.5, 3.5))\n",
    "d2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\n",
    "d2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\n",
    "d2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Samples')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very diagram explains why we need large amounts of data with machine learning, this is becasue as we get more sample data of a real world event we can better predict the probability distribution as it converges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the PyTorch framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones in module torch:\n",
      "\n",
      "ones(...)\n",
      "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Dataset has samples and their labels, while data loaders wrap an iterable around a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and Test data\n",
    "\n",
    "train_data = datasets.FashionMNIST(root=\"data\", train=True, transform=ToTensor(), download=True)\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, transform=ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data_dataloader = DataLoader(train_data, batch_size)\n",
    "test_data_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "for X,  y in train_data_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here MPS is the metal performance shaders which is the library form gpu operation sin apple processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define our neural network\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out\n",
    "    \n",
    "model = NeuralNetwork().to(device=device)\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<generator object Module.parameters at 0x14f86a2d0>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309698  [   64/60000]\n",
      "loss: 2.293108  [ 6464/60000]\n",
      "loss: 2.281984  [12864/60000]\n",
      "loss: 2.272993  [19264/60000]\n",
      "loss: 2.244553  [25664/60000]\n",
      "loss: 2.226620  [32064/60000]\n",
      "loss: 2.230687  [38464/60000]\n",
      "loss: 2.197608  [44864/60000]\n",
      "loss: 2.201126  [51264/60000]\n",
      "loss: 2.171486  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 2.159489 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.177954  [   64/60000]\n",
      "loss: 2.163384  [ 6464/60000]\n",
      "loss: 2.107312  [12864/60000]\n",
      "loss: 2.118319  [19264/60000]\n",
      "loss: 2.060704  [25664/60000]\n",
      "loss: 2.010913  [32064/60000]\n",
      "loss: 2.044134  [38464/60000]\n",
      "loss: 1.959837  [44864/60000]\n",
      "loss: 1.980040  [51264/60000]\n",
      "loss: 1.909664  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.896440 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.939248  [   64/60000]\n",
      "loss: 1.903836  [ 6464/60000]\n",
      "loss: 1.784762  [12864/60000]\n",
      "loss: 1.823933  [19264/60000]\n",
      "loss: 1.704647  [25664/60000]\n",
      "loss: 1.668531  [32064/60000]\n",
      "loss: 1.700881  [38464/60000]\n",
      "loss: 1.589481  [44864/60000]\n",
      "loss: 1.631651  [51264/60000]\n",
      "loss: 1.528840  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.534213 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.608172  [   64/60000]\n",
      "loss: 1.568799  [ 6464/60000]\n",
      "loss: 1.415451  [12864/60000]\n",
      "loss: 1.483494  [19264/60000]\n",
      "loss: 1.360767  [25664/60000]\n",
      "loss: 1.364230  [32064/60000]\n",
      "loss: 1.384173  [38464/60000]\n",
      "loss: 1.296845  [44864/60000]\n",
      "loss: 1.340101  [51264/60000]\n",
      "loss: 1.241718  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.263470 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.345785  [   64/60000]\n",
      "loss: 1.324532  [ 6464/60000]\n",
      "loss: 1.156653  [12864/60000]\n",
      "loss: 1.254304  [19264/60000]\n",
      "loss: 1.134900  [25664/60000]\n",
      "loss: 1.161867  [32064/60000]\n",
      "loss: 1.186075  [38464/60000]\n",
      "loss: 1.112768  [44864/60000]\n",
      "loss: 1.157100  [51264/60000]\n",
      "loss: 1.073043  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.094271 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_data_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_data_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pytorch model state\n"
     ]
    }
   ],
   "source": [
    "# save the model \n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved pytorch model state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
