{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to reinforcement learning by david silver\n",
    "# What is reinforcement learning?\n",
    "# Reinforcement Learning sits at the intersection of computer science, \n",
    "# engineering, nuerosicence, mathematics, pyscology and economics.\n",
    "\n",
    "# Engineering formulates this problem as optimal control.\n",
    "# Understanding of how the human brain tries to make decisions, which one of the newer breakthroughs of neurosciece. \n",
    "# Pyschology think about conditioning and understanding why animals exibit certain behaviour\n",
    "# Economics thinks about game theory\n",
    "\n",
    "# Machine learning is composed of supervised learning, unsupervised learning and reinforement learning.\n",
    "# In reinforcement learning there is no supervisor.\n",
    "# Feedback may be delayed by many steps (Markovian decision problem).\n",
    "# The concept of time matters in RL that where we think about temporal difference problem. \n",
    "# Agent gets to influence based on its actions in the environment. \n",
    "\n",
    "# Fly stunt manoeuvres in a helicopter\n",
    "# Defeat the world champion at backgammon based purely on reinforcement learning\n",
    "# managing investment portfolio is a reinforcement learning problem.\n",
    "# Control a power station is another reinforcement learning problem. \n",
    "# Make a humanoid robot walking is another problem that can be solved with reinforcement learning.\n",
    "# Get a single program to perform at human level at a bunch of atari games.\n",
    "\n",
    "# You can learn a model from real data. \n",
    "# The AI got the chicken cross the road\n",
    "# 15hz in game simulation\n",
    "# How long does it take to train these algorithms? and are they take 3 for days to train\n",
    "\n",
    "# Games are macrocosms of what happen in the real world and help us use as a test bed for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Rt <- how well is the agent doing at that time step. \n",
    "# Hypothesis: All goals can be defined by the maximization of returns\n",
    "\n",
    "\n",
    "# Fly stunt manoevres: +ve reward for following desired trajectory, -ve reward for crashing\n",
    "# Goal: select actions to maximize future reward (sequential decision making)\n",
    "# This involves thinking ahead, and thinking about long term consequences\n",
    "# You cant be greedy because you have to think ahead. \n",
    "\n",
    "# Observation, Action, Reward. The agent is the brain.\n",
    "# We also have an environment that provides the observation for the agent. \n",
    "# We can influence the environment with actions but we can not control the environment. \n",
    "\n",
    "# The time series is the data we use for training our agent, these time series is generated from the actions\n",
    "# taken by the agent on the environment over time\n",
    "\n",
    "# A scalar reward ultimately must be enough\n",
    "# History is everything the agent has seen so far. \n",
    "# The algorithms are only concerned with what the agent can see or observe.\n",
    "# What happens next depends on the history of the agent. \n",
    "\n",
    "# History determines what happens next in the agent. \n",
    "# History is usually enormous and may be too large to manage. \n",
    "# History is replaced by state.\n",
    "# State is a summerization of the information we need to make a decision in our algorithm (agent)\n",
    "# State is a function of the history. \n",
    "# State could be the last observation of the agent. \n",
    "\n",
    "# Environment state: Information used within the environment to determine what happes next.\n",
    "# The environment is not usually visible to the agent. \n",
    "# Agent state can be captured within the agent itself. \n",
    "# We pick actions from our agent state. \n",
    "\n",
    "# We also build the function that is used to determin the state from the history. \n",
    "\n",
    "# Information state: Contains all useful information from the history. \n",
    "# P[St+1 | St] = P[St+1 | S0 .....St]\n",
    "# The future is independent of the past given the present. \n",
    "\n",
    "# The state is a sufficient statistic of the future.\n",
    "# Doesn't need the full history of the past to make predictions.\n",
    "# If our current state is not equivalent in information for predicting the future as the \n",
    "# list of all previous states, then the state is not markovian.\n",
    "# The environment state by definition is markovian. \n",
    "# The entire history of everything is markovian. Becuase welll.... its the entire history. \n",
    "\n",
    "\n",
    "# Rat example. \n",
    "# What our state can be defined in different representations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully observable environment. \n",
    "# Ot = Sat = Set\n",
    "\n",
    "# Markov decision process\n",
    "\n",
    "\n",
    "# Partial observable environment. \n",
    "# - a robot within a room with camera vision\n",
    "# - trading agent\n",
    "# - poker playing robot, where we can only view publicly available cards\n",
    "# Partially observable Markov decision process (POMDP)\n",
    "# In this situation we have to build the agent state. \n",
    "# We could have beliefs, which finds a probability distribution of what we feel the \n",
    "# state of the environment is. \n",
    "# We could also use recurrent neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside and RL agent.\n",
    "# Taxonomy of how to discover reinforcement learning. \n",
    "# Policy: This is how the agent picks its action based on the state (Behaviour funciton)\n",
    "# Value function: Which tells us the amount of reward to expect for taking a specific action in a given state. \n",
    "# Model: This is the agent's internal representation of the environment. \n",
    "\n",
    "# Thesea are three pieces that may or may not be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy. this is the agents behaviour\n",
    "# a = f(s) maps state to action\n",
    "# Policies can also be stochastic functions. ie probability\n",
    "# f(a|s) = P[A=a|S=s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function is a prediciton of future reward. \n",
    "# Used to estimate goodness or badness of state\n",
    "# V(s) = E[Rt + vRt+1 + v^2Rt+2 + ... | St = s]\n",
    "# The value function is only considering future rewards. \n",
    "# We can be looking approximately a hundred steps into the future. \n",
    "# (Risk sensitive markov decision processes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: predicts what the environment might do\n",
    "# The model is the agents representaiton of the environment\n",
    "# The model can be divided into:\n",
    "# - Transitions: Predicts next state\n",
    "# - Reward: Predicts next reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy is a mapping of state to action. \n",
    "# Value fucntion tells us expected future reward at any given reward. \n",
    "# The model is not reality but our agents understanding of reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value based algorithm if it contains a value function (Policy function is the best value based on value function).\n",
    "# Policy based algorithm if it contains a policy( Policy agent may not need a value function).Stores the policy\n",
    "\n",
    "# Actor critic: Stores both the policy and the value function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model free and Model based environment\n",
    "# Model free: No need to understand how the environment works\n",
    "# Model based: First need to build an understanding of the environment works and then builidng your policy and/or value funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems within reinforcement learning. \n",
    "# Learning and Planning: Optimal decisions. \n",
    "# Difference between learning and plannng. Search trees come into play with planning in this situation the environment is known. \n",
    "# Exploration vs Exploitation: Our model has to balance exploration and exploitation. \n",
    "# Prediction vs control: evaluate the future given policy, vs optimize the future by finding the best policy. We need to solve prediction to solve the control problem. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
