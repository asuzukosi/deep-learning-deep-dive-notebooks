{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov Decision Process\n",
    "# Markov Decision process formally describes the the environment in reinforcement learning\n",
    "# All reinforcement learning problems can be formalized as markov decision processes\n",
    "# We can use Markov Decision Process to deal with continous actions\n",
    "# We can also use Markov Decision Process in partially observable environments to form POMDPs\n",
    "\n",
    "\n",
    "# MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central ideas to MDPs\n",
    "# The future is independent of the past given the present.\n",
    "# We dont need to retain all the information from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State transition matrix\n",
    "# For a markov state s and a successor state s', the state transition probability can be \n",
    "# defined\n",
    "# Transition probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov process is a random process we are sampling from successively. \n",
    "# Markov process is a sequence of states with the markov property.\n",
    "\n",
    "# Markov process == Markov Chain\n",
    "# State space and Transition probability matrix\n",
    "# Markov processes have a terminal state. \n",
    "\n",
    "# A markov chain sample is a sequence of states though the markov chain\n",
    "# State transition probability matrix.\n",
    "# Dealing with changing probability in the state transition probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Markov Reward Process\n",
    "# A markov reward process is a markov chain with values. \n",
    "# Reward function R\n",
    "# Discount factor gamma\n",
    "\n",
    "# Reward function tells us how much reward I get from a \n",
    "# particular state at a particular moment\n",
    "\n",
    "# Total reward across the whole chain\n",
    "# The Return (G) is the cumulative sum of rewards across a particular thread of \n",
    "# actions in the markov decision chain\n",
    "\n",
    "# Discount factor has to be between 0 and 1.\n",
    "# Discount factor of 0 is very short sighted\n",
    "# Discount factor of 1 is very long sighted\n",
    "\n",
    "# The value of the discount factor can determin if the agent is short sighted/myopic or long sighted\n",
    "# Becuase of the impact of the gamma (discount factor) on future rewards\n",
    "\n",
    "# Why do we even need a discount factor? Because there is more uncertainty in the future. \n",
    "# The reason is uncertainty.\n",
    "# Its mathematically convinient to discount rewards.\n",
    "# Behaviour shows preference for immediate rewards. \n",
    "\n",
    "# There are undiscounted markov reward processes. \n",
    "# Markov decision processes are finite and will definitely terminate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectations\n",
    "# Value function gives the long term value of state s\n",
    "\n",
    "# v(s) = E[Gt | St=s]\n",
    "# Expectation is required because the environment is stochastic\n",
    "# We prefer to be in states that maximize returns. \n",
    "\n",
    "# Value function is an expectation over random vallues\n",
    "# The return is from that timestep forward.\n",
    "\n",
    "\n",
    "# Bellman equation: Immediate reward plus the value of the next state. \n",
    "# v(s) = E[Rt+1 + vGt+1 | St=s]\n",
    "# Law of iterative expection states that expectations can be composed in each other\n",
    "# v(s) =  E[Rt+1 + gamma(v(St+1)) | St=s] <- Bellman equation for Markov Reward processes.\n",
    "\n",
    "# Bellman equation for MRP: Can be perceived aa a one step look ahead tree.\n",
    "# v(s) = R + gamaPv(St+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bellman equation is a linear equation\n",
    "# v = (1 - gamma*P)**-1 * R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov decision process\n",
    "# The markov reward process with decisions. \n",
    "# Its an environment in which all states are markov\n",
    "\n",
    "\n",
    "# A is a finite set of actions.\n",
    "# Where you end up is depending on the actions that you take. \n",
    "\n",
    "# Reward function may or may not depend on the actions. \n",
    "# Formalizing what it means to take decisions\n",
    "# Distributions over actions given states\n",
    "\n",
    "# Policy is a distribution over actions over state\n",
    "# p(a|s) = P[At = a| St = s]\n",
    "# Policy is the same no matter what the timestep is \n",
    "\n",
    "# Markov decision process the state characterizes the reward. \n",
    "# The rewards are considered in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function in a markov decision process.\n",
    "# Vpi(s) <- How much reward will i get for being in state s using policy pi\n",
    "# Action-Value function : How good is it to take a particular action from a given state\n",
    "# qpi(s, a) = Epi[Gt | St = s, At = a]\n",
    "# Bellman equation for the action-value function \n",
    "# Decompose to an immediate reward plus the action-value function for the next state. \n",
    "# qpi(s, a) = Epi[Rpit + gamma*qpi(St+1, At+1) | St = s, At = a]\n",
    "\n",
    "# It uses one step look ahead\n",
    "# We are trying to figure out the best way to behave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal value function. \n",
    "# V*(s) = maxVpi(s), * = best policy(pi)\n",
    "# Q*(s, a) = maxQpi(s, a), * = best policy(pi) <- If you have q star then you are done. \n",
    "\n",
    "# finding Q* is solving the markov decision process. \n",
    "\n",
    "\n",
    "# Optimal policy \n",
    "# What makes one policy better than another policy\n",
    "# pi >= pi' if Vpi(S) >= Vpi'(S) for a policy p to be better than policy p' the value function for all the states for value function vpi(S) must be greater than\n",
    "# that of the states for the value function vpi'(s)\n",
    "\n",
    "\n",
    "# There is always one policy that is better than all other policies. \n",
    "# It is possible to have more than one optimal policy \n",
    "\n",
    "\n",
    "# Fining the optimal policy \n",
    "# There is always a deterministic optimal policy\n",
    "# once we have Q* we are done \n",
    "# How do we find Q* in practice?\n",
    "# Bellman optimality equation <- Max of the q values which you can take on that state.\n",
    "# Bellman optimality equation for V*\n",
    "# Bellman optimality equation of Q*\n",
    "\n",
    "# Bellman optimality equation is non linear because of the addition of the max function which is a non linear operation\n",
    "# We can now use the following methods to calculate our Bellman optimality equation\n",
    "# - Value Iteration\n",
    "# - Policy Iteration\n",
    "# - Q - Learning\n",
    "# - SARSA\n",
    "\n",
    "# We build our model of the environment from data.\n",
    "# There are risk sensitive MDPs\n",
    "\n",
    "\n",
    "# Extenstions of MDPs\n",
    "# - Infinite and continous MDPs\n",
    "# - Partially obervable MPDs\n",
    "# - Undiscounted, average reward MDPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
