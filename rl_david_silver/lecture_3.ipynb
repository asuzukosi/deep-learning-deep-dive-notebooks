{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning and dynamic programming\n",
    "\n",
    "\n",
    "# How to solve markov decision processs\n",
    "# - Policy evaluation\n",
    "# - Policy Iteration\n",
    "# - Value Iteration\n",
    "# - Extensions to Dynamic programming\n",
    "# - Contraction mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is dynamic programming\n",
    "# Dynamic sequential or temporal component to the problem\n",
    "# Programming in the sense of optimizing a \"program\" ie. a policy\n",
    "\n",
    "# Optimizaton method for sequential problems. \n",
    "# Dynamic problems helps us solve complex problems: It does this by breaking a \n",
    "# program into smaller programs, solving them individually and then combining them together to find the solution for the whole problem.\n",
    "\n",
    "# Divide and conquer\n",
    "\n",
    "# Overlapping subproblems. \n",
    "# This requires that sub problem recurs many times (We can cache our solutions to these sub problems and reuse the solutions)\n",
    "# Bellman equation is the recursive decomposition needed to solve the problem. This is what helps us satisfify the requirement of dynamic programming.\n",
    "\n",
    "# The value function is a cache of good information we have figured about the MDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dynammic progrmaming to plan assuming full knowledge of the MDP\n",
    "# Prediction and Control\n",
    "\n",
    "# Prediction gives us the value of the policy\n",
    "# Control gives us the best policy available\n",
    "\n",
    "# Applications of dynammic programming, scheduling algorithms, string algorithms, graph alogrithms, graphical models, biofinformaatics\n",
    "# Evaluate a given policy (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy evaluation. \n",
    "\n",
    "# Using synchronous backups\n",
    "# For every iteration we would consider all states in our MDP\n",
    "# Convergence ? It does converge to the true value for each of the states\n",
    "\n",
    "# Define value function as an iterative process\n",
    "# The value function helps us figure out better policies\n",
    "\n",
    "# Fining the optimal policy\n",
    "# Acting greedy wiht respect to our value function. \n",
    "\n",
    "# There is always one determinstic policy for an MDP\n",
    "# Policy evaluation and policy improvements are done in sequence. \n",
    "# Acting greedily always leaves us at a deterministic policy\n",
    "\n",
    "# Greedily it improves the value over the next step, without considering future steps. \n",
    "# The value function improves over one step\n",
    "\n",
    "# We dont make things worse, only better\n",
    "# Anything that satisifies th bellman optimality equation is optimal.\n",
    "\n",
    "# Does policy evalution need to converge to Vpi\n",
    "\n",
    "# Modified policy iteration: Stop early \n",
    "# Principle of optimality: If the policy is optimal from that state onwards\n",
    "# Value function is caching all the solutions to all our subproblems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration\n",
    "# Value iteration moves from value function to value function. \n",
    "# Modified K-Iteration with K = 1\n",
    "# You can mix value iteration and policy iteration\n",
    "# Bellman optimality equation into an iterative update\n",
    "\n",
    "# Modified policy iteration algorithm\n",
    "\n",
    "# Problem               Bellman Equation           Algorithm\n",
    "# Prdiction            Bellman expectation           Policy evaluation\n",
    "#                      equation \n",
    "# Control              Bellman expectation equation\n",
    "#                      + Greedy policy improvemnt    Policy iteration\n",
    "# Control              Bellman optimality equation   Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensions ot dynamic programming\n",
    "# DP methods are used for synchronous backups\n",
    "# Asynchronous DP backs up states individually, in any order\n",
    "# This is done to reduce computation. \n",
    "\n",
    "# In place dynamic programming\n",
    "# Priorised sweeping \n",
    "# Real time dynamic programming\n",
    "# are all example of asynchronous dynamic programming\n",
    "\n",
    "# Contraction mapping"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
