{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Free Control\n",
    "# On-policy monte carlo control\n",
    "# On-policy temporal differnce learning\n",
    "\n",
    "# off-policy learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model free prediction is used to evaluate a given markov decision process\n",
    "# without knowing the whole markov decision process. \n",
    "\n",
    "# Multi step problems that have state are MDP problems\n",
    "# In some cases, the MDP might be known or unkwown\n",
    "# The first step is to break our problem down into an MDP, \n",
    "# if we can fit a problem into an MDP, then it can be solved with reinforcement learning\n",
    "\n",
    "# On policy learning, this learning on the job, this is evaluating a policy that the agent is currently following\n",
    "# Off policy learning, this is learning from a different policy than that you are currently using, in this case you would be learning from an observed policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On policy learning\n",
    "\n",
    "# We would use policy iteration to find the optimal policy\n",
    "# The iterative process is to evaluate a policy and improve on the evaluated policy\n",
    "# This is done by acting greedily to the evaluated value function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte carlo learning for policy evaluation\n",
    "# Greedy policy improvement\n",
    "\n",
    "\n",
    "# How do we get the MDP if we want to act greedily on policy improvement\n",
    "# Action-value function allow us to do greedy policy improvement without knowledge\n",
    "# of our MDP.\n",
    "\n",
    "# Issues with greedy action selection\n",
    "# Exploration vs exploitation\n",
    "\n",
    "# Simplest idea for ensuring continual exploration\n",
    "# all m actions are tried with non-zero probability\n",
    "\n",
    "# e-Greedy exploration allows us to explore other paths and enables us imporove our policy\n",
    "# it decides whether we should randomly pick an action or exploit our best action.Â \n",
    "\n",
    "# The max is greater than any weighted sum of your actions.\n",
    "\n",
    "# Monte carlo learning for policy evaluation\n",
    "# e-Greedy for policy improvement\n",
    "\n",
    "# Its not compulsory to fully evaluate a policy to improve it\n",
    "# Always greedily to your evaluation for the improvement of the value function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How csan we guarantee that we find the best possible policy\n",
    "# GLIE = Greedy in a limit with infinite exploration\n",
    "# GLIE Monte Carlo Control <- This is the fully learnable algorithm because it can be thrown into a markovian \n",
    "# environment and it will learn the best policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TD learning because it uses bootstraping, its low on variance\n",
    "# Genearally we do we use TD over MC in our control loop\n",
    "\n",
    "# We continue to use e-greedy policy improvement\n",
    "# The name of this idear is called SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa(lambda)\n",
    "# Averge over our n step returns \n",
    "# We can tweak the lambda parameter to make us more or less long term\n",
    "\n",
    "# Lambda defeats the terrony of the time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off policy learning - Learning based on the policy you are not currently following\n",
    "# Learning from observation or other agents\n",
    "# Reuse experience gathered from old policies\n",
    "# learn about optimal policy while following exploratory policy\n",
    "# learn about multiple policies while following one policy\n",
    "\n",
    "# Importance sampling for off-policy monte-carlo\n",
    "# Monte carlo learning is impractical off policy\n",
    "# We have to use TD learning for off-policy learning\n",
    "\n",
    "\n",
    "# Q-learing works best for off-policy learning\n",
    "# We chose next action using the bahaviour policy\n",
    "# We also consider the alternate action we would have taken using the target policy\n",
    "\n",
    "# The bellman equation for Q doest involve importance sampling\n",
    "# We bootstrap for the alternate action\n",
    "# Off policy control with Q-learning\n",
    "# The target policy is a greedy policy\n",
    "# updates in the direction of the maximum Q value you can take\n",
    "\n",
    "\n",
    "# Q-learning is the sampling implementation fo the value iteration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
