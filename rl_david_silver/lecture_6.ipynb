{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function approximation\n",
    "\n",
    "# Incremental/Online methods and Batch/Offline methods\n",
    "\n",
    "# Why are we interested in value function approximation\n",
    "# Backagammon 10**20 states\n",
    "# Computer Go 10**170 states\n",
    "# Robotics infinite number of states\n",
    "\n",
    "\n",
    "# Estimates the value of similar states, value functions that understand generalization\n",
    "# Build efficient methods for representing and building value functions\n",
    "\n",
    "# Scale up model free methods for prediction and control \n",
    "# So far we have represented value functions by a lookup table.\n",
    "# Every state has an equivalent value V(s)\n",
    "\n",
    "# We also represented or action value function Q(s, a) as a matrix (2D table)\n",
    "# There are too many states and actions to store in memory. \n",
    "\n",
    "# The solution is value function approximation\n",
    "# We build a function that approximates teh Vpi(s)\n",
    "# v_approx(s, W) == v_pi(s) : where W is our weights of our neural network\n",
    "# q_approx(s, a, W) == q_pi(s, a) : where W is our weights of our neural network\n",
    "\n",
    "# This allows us to both reduce the memory requirement and also allows us to generalize\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does it mean to do function approximation with value function\n",
    "# Neural networks allow us to build function approximators. This is \n",
    "# what makes neural networks so powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening up the neural network black box\n",
    "# Linear combination of features, neural netwowks, decision trees, nearest neighbor, forier bases etc are all function approximators\n",
    "# For this couse we will be focusing on linear combination of features and neural networks. \n",
    "\n",
    "# The backward pass of the neural network gives us the gradients, which is what made me run here lol\n",
    "# We have a non stationary sequence of value functions as we improve\n",
    "# Using this tools to let us use RL at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradint descent\n",
    "# Stochastic graeint descent\n",
    "# Gradients is a vector of partial derivatives of parameters in the function\n",
    "# Why is the half there?\n",
    "# The gradient tells you how to adjust the parameters to move in a particular direction\n",
    "# Represent state by a feature vector: Each feature tells you tells you something about the state. \n",
    "# Each feature is a value telling you some information about the state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear value fucntion approximation\n",
    "# Represents the value fucntion by a linear combination of features\n",
    "# Objective function is quadratic in parameters W\n",
    "# You will get the optimal answer if you follow your gradeints long enough\n",
    "# We adjust the feautures with the most active weights when evaluating the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Prediction algorithms\n",
    "# Monte Carlo and Temporal Difference prediction algorithms\n",
    "# In monte Carlo the Gt would be the target \n",
    "# Incrementally building training data\n",
    "# Agent is seeing the states and the estimates of the return. \n",
    "# The simplest case is using linear monte carlo value approximation\n",
    "# Fit our value function towards TD targets\n",
    "# Linear temporal differnce learning still converges close to the optimal solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foward view of value function approximation\n",
    "# What is the size of the eligibility trace? They are the size of your parameters\n",
    "# Eligibility trace decays over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control with value function approximation\n",
    "# Residual gradient methods\n",
    "\n",
    "# Approximate policy evaluation\n",
    "# Act greedy on our neural network with respect to our evaluation which will now give\n",
    "# us a new policy and new value function\n",
    "\n",
    "# Action-value function Q so that we can be off policy\n",
    "# Return is the noisy, unbiased estimate of the oracle\n",
    "\n",
    "# Gradient TD\n",
    "# TD approaches are not good for non-linear function approximation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch methods\n",
    "# Gradient descent is simple adn appealing\n",
    "# But they are not sample efficient\n",
    "# Batch methods seek to find the best fit value function with the batch of data\n",
    "\n",
    "\n",
    "# Least Squares Prediction\n",
    "# Stochastic Gradient Descent with experience replay\n",
    "# We get to the least square solution\n",
    "# Keep going over your data and learning from your data. \n",
    "\n",
    "# Q-Learning is off policy\n",
    "# Experience replay in Deep Q-Network (DQN) also called fitted Q iteration\n",
    "\n",
    "# Methods to jump to approximate solution with linear approximation functions\n",
    "# This is possible when you have history of experience\n",
    "\n",
    "# LSPI - Least squares policy iteration\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
